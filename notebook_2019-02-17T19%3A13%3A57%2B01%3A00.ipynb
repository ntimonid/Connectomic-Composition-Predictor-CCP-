{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":["import sys\n","import os\n","from IPython.core.debugger import set_trace\n","os.environ['KMP_DUPLICATE_LIB_OK']='True'\n","import numpy as np\n","import scipy as sci\n","import csv\n","import pickle as pk\n","import h5py\n","import math as mt\n","import matplotlib.pyplot as plt\n","import matplotlib.axis as axis\n","import matplotlib\n","import matplotlib.legend as lgd\n","import sklearn.cluster as cluster\n","import glob\n","import six\n","import time as time\n","import json\n","from sklearn import multioutput\n","import pandas as pd\n","from pandas.tools.plotting import table\n","from itertools import groupby\n","from sklearn.preprocessing import Imputer, normalize, StandardScaler, scale\n","import sklearn.decomposition as decomp\n","from scipy.stats import boxcox, mstats, normaltest, pearsonr, skew, iqr\n","from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier\n","from sklearn.model_selection import cross_val_predict,\\\n","                                    GridSearchCV, KFold,\\\n","                                    permutation_test_score, \\\n","                                    StratifiedKFold, cross_val_score, RandomizedSearchCV\n","from sklearn.dummy import DummyClassifier, DummyRegressor\n","import sklearn.feature_selection as fs\n","from sklearn.linear_model import RandomizedLogisticRegression,\\\n","                                 LogisticRegression,SGDClassifier, Ridge, Lars, ElasticNet\n","import sklearn.linear_model as lm\n","from sklearn.neural_network import MLPRegressor\n","from sets import Set\n","from sklearn import utils\n","from collections import OrderedDict\n","import sklearn.metrics as metrics\n","from joblib import Parallel, delayed\n","#import plotly.offline as off\n","#import plotly.graph_objs as go\n","\n","from scipy.ndimage.filters import laplace\n","from allensdk.core.mouse_connectivity_cache import MouseConnectivityCache \n","from allensdk.api.queries.grid_data_api import GridDataApi\n","from scipy.ndimage.filters import laplace\n","import zipfile\n","import cortical_map_10 as cm\n","import nrrd\n","from allensdk.api.queries.ontologies_api import OntologiesApi\n","import skimage as ski\n","import rpy2.robjects as ro\n","import rpy2\n","import nibabel as nib\n","import base64\n","from IPython.core.display import display, HTML\n","import nrrd"],"execution_count":1,"outputs":[{"output_type":"stream","text":"/opt/conda/envs/python2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\n/opt/conda/envs/python2/lib/python2.7/site-packages/ipykernel_launcher.py:34: DeprecationWarning: the sets module is deprecated\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"markdown","source":["#!pip install allensdk\n","#!pip install joblib\n","#! pip install --upgrade \"hbp-service-client==1.0.0\"\n","#!conda install -y -c r rpy2 \n","#!pip install --upgrade sklearn\n","!pip install --upgrade pip"]},{"metadata":{"trusted":true},"cell_type":"markdown","source":["#clients = get_hbp_service_client()\n","client      = get_bbp_client().document\n","collab_path = get_collab_storage_path()\n","ccp_path    = 'CCP v2'\n","itemlist = client.listdir(os.path.join(collab_path,ccp_path))\n","for element in itemlist:\n","    if os.path.isfile('./' + str(element)) == True: continue\n","    print element\n","    local_path = './' + element\n","    client.download_file(os.path.join(collab_path,ccp_path, element),local_path)\n","#client.download_file(os.path.join(collab_path, 'cortical_map_10.py'))\n","#local_path = './cortical_map_10.py'\n","#client.download_file(os.path.join(collab_path, 'cortical_map_10.py'), local_path)\n","#local_path = './acr2parent.json'\n","#client.download_file(os.path.join(collab_path,ccp_path, 'acr2parent.json'),local_path)"]},{"metadata":{},"cell_type":"markdown","source":["## make_data_url function \n","### Author: Rembrandt Bakker    \n","### Description:  \n","    This function takes as input the name of a nifti volume and converts it to a url   \n","### Input:  \n","      filename = the filename of the nifti volume"]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":["def make_data_url(filename):\n","    prefix = 'data:image/gznii;base64,'\n","    fin = open(filename, 'rb')\n","    contents = fin.read()\n","    fin.close()\n","    #data_url = 'https://scalablebrainatlas.incf.org/composer-dev/?scene={\"imports\":{\"'+ filename +'\":{url:\"'+ prefix + base64.b64encode(contents)+'\",orientation:\"PIR\"}}}' \n","    data_url = '{\"imports\":{\"'+ filename +'\":{\"url\":\"'+ prefix + base64.b64encode(contents)+'\",\"orientation\":\"PIR\"}}}' \n","    return data_url"],"execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["## readtable function\n","### Author: Nestor Timonidis   \n","### Description:   \n","Given the file pointer of a .csv file, this function stores the csv elements into a dictionary\n","*        #### input:    \n","fp = the file pointer corresponding to the .csv file\n","*        #### output:   \n","T = the output dictionary"]},{"metadata":{"collapsed":true,"trusted":true},"cell_type":"code","source":["def readtable(fp):\n","  r = csv.reader(fp)\n","  keys = r.next()\n","  T = [ [] for k in keys ]\n","  for row in r:\n","    for i,v in enumerate(row):T[i].append(v)\n","  T = {keys[i]:col for i,col in enumerate(T)}\n","  return T"],"execution_count":3,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["## RemoveOutliers class\n","### Author: Nestor Timonidis   \n","### Description:   \n","Removes outliers based on the input array's per column interquartile range. Elements exceeding the range of $(1.5*iqr - Q1, 1.5*iqr + Q3)$,  \n","where the interquartile range (iqr) and the two quartiles (1st and 3rd) are defined per column of the array, are detected as outliers, and their corresponding columns are being removed from the array.\n","*   ### functions:   \n","       *        #### init:    \n","initalizes the quartiles used for estimating outliers. By default are Q1 (1st) and Q3 (3rd)\n","       *        #### fit:   \n","scans the input array per column and checks for outliers based on the method provided in the description.\n","For each detected outlier, the column corresponding to it is removed from the array.\n","The function returns the updated array, as well as a list that stores the outlier indices.\n",""]},{"metadata":{"collapsed":true,"trusted":true},"cell_type":"code","source":["class RemoveOutliers():\n","     \n","    def __init__(self, q1 = 25, q2 = 75):\n","        self.q1 = q1\n","        self.q2 = q2\n","    def fit(self, A):\n","        ranges = iqr(A, axis = 0)\n","        Q1 = np.percentile(A, self.q1, axis = 0)\n","        Q3 = np.percentile(A, self.q2, axis = 0)\n","\n","        genes4deletion = Set()\n","        for row,cell in enumerate(A):\n","            for col,gene in enumerate(cell):\n","               if gene >= ranges[col]*3/2 + Q3[col] or gene <= ranges[col]*3/2 - Q1[col]:\n","                   genes4deletion.add(col)\n","                   break\n","\n","        outliers = list(genes4deletion)\n","        A = np.delete(A,outliers,1)\n","        return A,outliers"],"execution_count":4,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["## filter_cortex function\n","### Author: Nestor Timonidis   \n","### Description:   \n","Given a mask corresponding to specific stuctures of the mouse cortex and a tract tracing experiment defined in a volumetric space, this function filters out voxels of the experiment not corresponding to the mask.\n","*        #### input:    \n","         * struct_idx_dict = dictionary that maps identities of anatomically distinct structures, as provided by the Allen Institute, to their position in a unionized space, meaning a 2D space of anatomical structures and brain related experiments.\n","         * l_vals = the anatomical structures that will be used to filter the data, defined by their position in the unionized space.\n","         * Annotation = the Annotation identities of the structures in the volumetric space, as provided by the Allen Institute.\n","         * pd: the tract tracing experiment defined in the volumetric space, measured by projection density.\n","*        #### output:   \n","         * l_voxel_mask = the refined volumetric data based on the applied mask."]},{"metadata":{"collapsed":true,"trusted":true},"cell_type":"code","source":["def filter_cortex(struct_idx_dict,l_vals, Annotation, pd):\n","        scis = [sci for sci,x in struct_idx_dict.items() if x in l_vals]\n","        l_mask = []\n","        for sci in scis:\n","            tmp = Annotation == int(sci) \n","            if l_mask == []:\n","                l_mask = tmp\n","            else:\n","                l_mask = np.ma.mask_or(l_mask,tmp)   \n","\n","        l_voxel_mask = np.zeros(np.shape(pd))\n","        l_nzero      = np.nonzero(l_mask)\n","        l_voxel_mask[l_nzero] = pd[l_nzero]\n","        return l_voxel_mask"],"execution_count":5,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["## MakeArrayBorders function\n","### Author: Nestor Timonidis   \n","### Description:   \n","This function creates a structural border mask given the annotation volume of the mouse brain, based on an anatomical parcelation made by the Allen Institute. The voxels are masked based on the ones that differ in value with their neighboring voxels and the rest.\n","*        #### input:    \n","         * Annotation_copy = annotation volume of the mouse brain (defined in micrometers).\n","*        #### output:   \n","         * Annotation_copy2 = the structural border mask"]},{"metadata":{"collapsed":true,"trusted":true},"cell_type":"code","source":["def MakeArrayBorders(Annotation_copy):\n","    #description: given a matrix, comparison with neighbors so that different neighboring pixels get the value 1 \n","    #             and create borders, else 0\n","    \n","    Annotation_copy2 = np.zeros(np.shape(Annotation_copy))\n","    \n","    for x,row in enumerate(Annotation_copy):\n","       \n","        for y,col in enumerate(row):\n","            flag = 0\n","            if x > 0:\n","                if (Annotation_copy[x,y] - Annotation_copy[x-1,y]) != 0:\n","                    flag = 1\n","            if x < np.shape(Annotation_copy)[0] - 1:\n","                if (Annotation_copy[x,y] - Annotation_copy[x+1,y]) != 0:\n","                    flag = 1\n","            if y > 0:\n","                if (Annotation_copy[x,y] - Annotation_copy[x,y-1]) != 0:\n","                    flag = 1\n","            if y < np.shape(Annotation_copy)[1] - 1:\n","                if (Annotation_copy[x,y] - Annotation_copy[x,y+1]) != 0:\n","                    flag = 1    \n","                    \n","            if flag == 1:\n","                Annotation_copy2[x,y] = 1\n","            else:\n","                Annotation_copy2[x,y] = 0\n","                \n","    return Annotation_copy2          "],"execution_count":6,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["## BrainPlotter class\n","### Author: Nestor Timonidis   \n","### Description:   \n","This class contains functions that provide visualizations of the mouse brain, either in the form of brain slices or cortical flatmaps. The inputs of interest for visualization are identities of tract tracing or gene expression experiments, as defined by the Allen Institute.\n","*   ### functions:   \n","       *        #### init:  \n","                *  #### description:  \n","                creates a brain plotter object by defining a number of parameters for the visualization process.\n","                *  #### input:  \n","                      *  mcc = an instance of MouseConnectivityCache class\n","                      *  experiment_id = the identity of the experiment  \n","                      *  driver  =  the type of the experiment. It could be wild_type (tract tracing), or gene_expression for the equivalent experiment categories, or the name of a cre category (tract tracing).\n","                      * res = resolution of the experiment in micrometers. Acceptable options are: 100, 25 and 10 for tract tracing experiments, and 200 for gene expression.\n","                     \n","       *        ####   plot_slice:   \n","                * #### description:   \n","                provides a visualization in the form of a brain slice, given the parameters defined by the init function and a set of other parameters. This is done by merging the experimental data together with a registration nissl template in an rgb form.\n","                 *  #### input:  \n","                       * matrix = a 3-dimensional array corresponding to an experiment as a manual option. Default = 'None'.\n","                       * struct_idx_dict = dictionary that maps identities of anatomically distinct structures, as provided by the Allen Institute, to their position in a unionized space, meaning a 2D space of anatomical structures and brain related experiments.\n","                       * l_vals = the anatomical structures that will be used to filter the data, defined by their position in the unionized space, in case they are being provided.  \n","                       * Annotation = the Annotation identities of the structures in the volumetric space, as provided by the Allen Institute.\n","                       * slice_idx = the point in the first dimension of the data (anterior-posterior range), which will be stable for the brain slice to be visualized. Default = 264.\n","                       * savefile = the save file name. Default = 'slice.png'.\n","         *        #### plot_flatmap:  \n","                  * #### description:   \n","                 provides a visualization in the form of a cortical flatmap, given the parameters defined by the init function and a set of other parameters. This is done by converting the experimental data in a flatmap form (calling the cortical_map_10 function), and then merging them together with a registration nissl template and an annotation template in an rgb form.\n","                 *  #### input:  \n","                           * matrix = a 3-dimensional array corresponding to an experiment as a manual option. Default = 'None'\n","                           * l_vals = the anatomical structures that will be used to filter the data, defined by their position in the unionized space, in case they are being provided.  \n","                           * Annotation = the Annotation identities of the structures in the volumetric space, as provided by the Allen Institute.\n","                           * savefile = the save file name. Default = 'slice.png'.\n","                 *  ### output:  \n","                           * trs_pd = the cortical flatmap in a rgb form.\n","                           * mask_and_pd = the cortical flatmap merged with with a registration nissl template and an annotation template in an rgb form.\n","                 "]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":["class BrainPlotter():\n","    \n","    def __init__(self, mcc, experiment_id = 181599674, driver = 'wild_type', res = 25):\n","        \n","        self.id     = experiment_id\n","        self.driver = driver  \n","       \n","        if driver != 'gene_expression':\n","            flag = 0 \n","            # projection density: number of projecting pixels / voxel volume\n","            all_files = os.listdir('./')\n","            experiment_list = [val for val in all_files if 'experiment_' in val]\n","            file_to_look  = 'experiment_' + str(experiment_id)\n","            if file_to_look in experiment_list:\n","                pathway  = os.path.join('./',file_to_look)\n","                pd_files = os.listdir(pathway)\n","                if 'projection_density_' + str(res) + '.nrrd' in pd_files:\n","                    self.pd, pd_info = nrrd.read(pathway +  '/projection_density_' + str(res) + '.nrrd') \n","                else:\n","                    self.pd, pd_info = mcc.get_projection_density(experiment_id)\n","\n","            else:\n","               self.pd, pd_info = mcc.get_projection_density(experiment_id)\n","            \n","        else:\n","    \n","            GDA = GridDataApi(resolution = res)\n","            #GDA.download_expression_grid_data(experiment_id, include = ['energy','density','intensity'], path = 'local.zip')\n","            GDA.download_gene_expression_grid_data(experiment_id, GDA.INTENSITY, path = 'local.zip')\n","            zip_ref = zipfile.ZipFile('local.zip', 'r')\n","            zip_ref.extractall('expression_files')\n","            zip_ref.close()\n","            with open('expression_files/density.raw','rb') as fp:\n","                buffer = fp.read()\n","            \n","            dims = [67,41,58]\n","            A  = np.ndarray(shape = dims, dtype = np.float32, buffer = buffer, order = 'F')\n","            fp.close()\n","            self.xd = A\n","        \n","        # For reducing time complexity, the brain slice templates provided by the Allen Institute's \n","        # MouseConnectivityCache tool have already been uploaded and stored as files:\n","        # average_template_10.nrrd and average_template_25.nrrd. The code commented bellow\n","        # displays how the templates can be downloaded in a custom fashion.\n","        #self.template, template_info = mcc.get_template_volume()\n","        self.template, template_info = nrrd.read('average_template_'+ str(res) + '.nrrd')\n","  \n","\n","    def plot_slice(self,struct_idx_dict,l_vals, Annotation, matrix = 'None', slice_idx = 264, savefile = 'slice.png'):    \n","        \n","        #*** Laplacian intervention ****#\n","        if l_vals == 'None':\n","            l_voxel_mask = self.pd\n","        else:            \n","            l_voxel_mask = filter_cortex(struct_idx_dict,l_vals, Annotation, self.pd)\n","            \n","        #1:\n","        mask_lpc = np.zeros((np.shape(self.template)[1],np.shape(self.template)[2]))\n","        laplace(self.template[slice_idx,:,:], mask_lpc)\n","        \n","        norm_mask_lpc = mask_lpc/100\n","        norm_mask_lpc[norm_mask_lpc > 1] = 1\n","        \n","        cmap_mask = plt.get_cmap('gray') \n","        rgb_mask = cmap_mask(norm_mask_lpc, bytes = True)\n","      \n","        \n","        if self.driver != 'gene_expression':\n","            # compute the maximum intensity projection (along the anterior-posterior axis) of the projection data\n","            if matrix != 'None':\n","                pd_mip = matrix[slice_idx,:,:]\n","            else:                \n","                pd_mip  = l_voxel_mask[slice_idx,:,:]\n","            \n","            pd_mip  = pd_mip/(1.0*np.max(pd_mip))\n","            #3:\n","            cmap_pd  = plt.get_cmap('hot') \n","            rgb_pd  = cmap_pd(pd_mip, bytes = True)\n","            #4:\n","            new_mat  = np.maximum(rgb_pd, rgb_mask)\n","            \n","        else:\n","            xd_mip  = self.xd[slice_idx,:,:]\n","            xd_mip  = ski.transform.rescale(xd_mip, (320/41.0,456/58.0))     #(528,320,456)\n","            cmap_xd = plt.get_cmap('hot') \n","            rgb_xd  = cmap_xd(xd_mip,bytes = True)\n","            new_mat = np.maximum(rgb_xd, rgb_mask)\n","        \n","        # Look at a slice from the average template and annotation volume\n","        fig  = plt.figure(figsize=(15, 6))\n","        plt.imshow(new_mat, cmap = 'hot', aspect='equal', vmin = 0, vmax = 1)\n","        plt.title(\"registration template for: \" + self.driver, fontsize = 20)\n","        plt.savefig(savefile)\n","        plt.show()\n","    \n","    def plot_flatmap(self,l_vals, Annotation, matrix = 'None', savefile = 'flatmap.png'):  \n","        \n","        cm1=cm.CorticalMap(projection='top_view')\n","        cm2=cm.CorticalMap(projection='dorsal_flatmap')\n","        \n","        if l_vals == 'None': \n","            l_voxel_mask = self.pd\n","        else:       \n","            l_voxel_mask = filter_cortex(struct_idx_dict,l_vals, Annotation, self.pd)\n","\n","        \n","        #%% scale the nissl template\n","        #%% initialize colormaps for rgb conversion\n","        self.template = self.template/(1.0*np.max(self.template))\n","        cmap_mask     = plt.get_cmap('gray') \n","        cmap_pd       = plt.get_cmap('hot') \n","        #%% Process the annotation template\n","        #Annot_lpc     = np.zeros(np.shape(Annotation))\n","        #laplace(Annotation,Annot_lpc)\n","        #Annot_lpc = Annot_lpc/100\n","        #Annot_lpc[Annot_lpc > 1] = 1\n","        trs_annot_lpc = cm1.transform(Annotation, agg_func = np.mean)\n","        trs_annot_lpc_bordered = MakeArrayBorders(trs_annot_lpc)\n","        pk.dump(trs_annot_lpc_bordered,open('Annotation_copy2.pkl','wb'))\n","        #trs_annot_lpc_bordered = pk.load(open('Annotation_copy2.pkl','rb'))\n","        rgb_annot     = cmap_mask(trs_annot_lpc_bordered, bytes = True)\n","        #%% transform projection density and nissl template\n","                \n","        if matrix != 'None':\n","            trs_pd = cm1.transform(matrix, agg_func = np.mean)\n","        else:    \n","            trs_pd = cm1.transform(l_voxel_mask, agg_func = np.mean)\n","            \n","        trs_mask      = cm1.transform(self.template, agg_func = np.mean)\n","        #%% convert projection density and nissl template to rbg images\n","        rgb_mask      = cmap_mask(trs_mask, bytes = True)\n","        rgb_pd        = cmap_pd(trs_pd, bytes = True)\n","        #%% mix rgb images - projection density with nissl template and annotation\n","        mask_and_pd   = np.maximum(rgb_pd,rgb_mask)\n","        annot_and_pd  = np.maximum(rgb_annot,rgb_pd)\n","        annot_mask_pd = np.maximum(mask_and_pd,rgb_annot)\n","        pk.dump(annot_mask_pd, open(str(savefile) + '_annot_mask.pkl','wb'))\n","        pk.dump(annot_and_pd, open(str(savefile) + '_annot_and_pd.pkl','wb'))\n","        #%%\n","        fig = plt.figure\n","        plt.imshow(annot_and_pd)\n","        savepoint1 = savefile + '_nonnissl.jpg' \n","        plt.savefig(savepoint1, dpi = 2000, format = 'jpg')\n","        plt.show()\n","        \n","        savepoint2 = savefile + '_nissl.jpg'\n","        plt.figure\n","        plt.imshow(annot_mask_pd)\n","        plt.savefig(savepoint2, dpi = 2000, format = 'jpg')\n","        plt.show()\n","       \n","        return trs_pd, mask_and_pd\n","        "],"execution_count":7,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["## FromVoxel2Union function\n","### Author: Nestor Timonidis   \n","### Description:   \n","This function maps the projection patterns that are defined by a given tract tracing experiment across anatomically distinct brain areas (unionized space), to a volumetric 3D space as specified by an annotation volume provided by the Allen Institute.\n","*        #### Input:  \n","         * Annotation = the Annotation identities of the structures in the volumetric space, as provided by the Allen Institute.\n","         * struct_idx_dict = dictionary that maps identities of anatomically distinct structures, as provided by the Allen Institute, to their position in a unionized space, meaning a 2D space of anatomical structures and brain related experiments.\n","         * ConStr: the tract tracing experiment defined in the unionized space, measured by normalized projection density\n","         * Pred_ConStr: the equivalent tract tracing experiment as ConStr, but with values being predicted by the the gene expression based predictive model (MesoPred).\n","*        #### Output:   \n","         * UnionToVoxel      =  a 3D array containing the tracing data of the original experiment mapped to the volumetric space.\n","         * UnionToVoxel_pred =  a 3D array containing the tracing data of the predicted experiment mapped to the volumetric space."]},{"metadata":{"trusted":true},"cell_type":"code","source":["def FromUnion2Voxel(Annotation, struct_idx_dict, ConStr, Pred_ConStr = 'None'):\n","    \n","    original_shape       = Annotation.shape\n","    Annotation_flat      = np.ndarray.flatten(Annotation,order = 'C')\n","    UnionToVoxel         = np.zeros(np.shape(Annotation_flat))\n","    UnionToVoxel_pred    = np.zeros(np.shape(Annotation_flat))\n","    rem_ind              = np.asarray(params['remaining_indices'])\n","    i = 0\n","    for area_id,area_num in struct_idx_dict.items():\n","        i += 1\n","        if i%50 == 1:\n","            print i\n","        if int(area_id) in Annotation_flat:\n","            area_mask = np.nonzero(Annotation_flat == int(area_id))\n","            #area_num_reind = [idx for idx,val in enumerate(params['remaining_indices']) if val == area_num]\n","            if area_num in rem_ind:\n","                area_num_reind = rem_ind == area_num\n","                union_values                 = ConStr[area_num_reind]\n","                union_values2                = Pred_ConStr[area_num_reind]\n","                UnionToVoxel[area_mask]      = union_values\n","                UnionToVoxel_pred[area_mask] = union_values2\n","  \n","    UnionToVoxel      = UnionToVoxel.reshape(original_shape, order = 'C')\n","    UnionToVoxel_pred = UnionToVoxel_pred.reshape(original_shape, order = 'C')\n","    \n","    return UnionToVoxel, UnionToVoxel_pred"],"execution_count":8,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["## BinarizeTheVector function\n","### Author:   Nestor Timonidis    \n","### Description:   \n","Given an input vector with\n","continuous values, the function binarizes\n","its values based on a threshold value.     \n","* ### Input:     \n","     * input_vec = the vector to be binarized,  \n","     * binThreshold = the desired ratio between the positive and negative class    \n","* ### Output:   \n","    * y = the binarized vector."]},{"metadata":{"collapsed":true,"trusted":true},"cell_type":"code","source":["def BinarizeTheVector(input_vec,binThreshold):\n","    percentile = (1-binThreshold)*100\n","    Q = np.percentile(input_vec, percentile, axis = 0)\n","    y = np.zeros(np.shape(input_vec))\n","    for idx,val in enumerate(input_vec):\n","        if input_vec[idx] > Q:\n","            y[idx] = 1   \n","    return y "],"execution_count":9,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["## CustomCrossval function\n","### Author:   Nestor Timonidis    \n","### Description:   \n","A custom implementation of the cross-validation method, that validates the performance of a supervised learning model by partitioning into disjoint training and testing sets and testing each test set by the model trained by the train set. The difference of this implementation with the custom ones provided by sklearn is that this implementation stores a set of additional parameters at once, as described by the output section. the actual and the predicted values in corresponding lists, stores class , the model parameters and the model coefficients assigned to the trained features based on averaging over the training sets.    \n","* ### Input:    \n","     * mdl = the supervised learning model for utility.\n","     * X = the input dataset or set of independent variables.   \n","     * y = the dataset labels or dependent variables.\n","     * cv = the cross validation method as defined by sklearn. \n","* ### Output:   \n","    * probs = assignment probabilities in case of support by the model.\n","    * preds = list with all predicted data points, assigned to the list based on their corresponding index in the actual data.\n","    * trues = the list of the actual samples, on one to one correspondance with the preds list.\n","    * mdl_list = storage of all model parameters.\n","    * mean_coeffs = the coefficients attributed to the data features, averaged across all training sets."]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":["def CustomCrossval(mdl,X,y,cv):\n","    \n","    probs = np.zeros((np.shape(y)[0],2)); preds = np.zeros((np.shape(y)));\n","    trues = np.zeros((np.shape(y)))\n","    coeffs = []; mdl_list = []\n","    for train, test in cv.split(X,y):\n","        mdl.fit(X[train],y[train])\n","        mdl_list.append(mdl)\n","        if hasattr(mdl, 'best_estimator_'):\n","            if hasattr(mdl.best_estimator_, 'coef_'):\n","                coeffs.append(mdl.best_estimator_.coef_)\n","            elif hasattr(mdl.best_estimator_, 'feature_importances_'):\n","                coeffs.append(mdl.best_estimator_.feature_importances_)\n","        if hasattr(mdl, 'predict_proba'):        \n","            probs[test,:] = mdl.predict_proba(X[test])\n","        preds[test] = mdl.predict(X[test])\n","        trues[test] = y[test]\n","    mean_coeffs = np.mean(coeffs,0)        \n","    return probs, preds, trues, mdl_list, mean_coeffs"],"execution_count":10,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["## RemoveNanStructs class\n","### Author:   Nestor Timonidis    \n","### Description:   \n","Given a dataset as input, this function removes columns or rows containing NaN (not-a-number), based on whether the percentage of NaN exceeds a given threshold.\n","*   ### functions:   \n","       *        #### init:    \n","                *  #### description:  \n","                initializes a RemoveNanStructs class object by selecting the default dimension of the dataset for NaN scanning, and the threshold for exclusion.\n","                *  #### input:  \n","                      *  dim = dimension for selection.\n","                      *  nan_thr = the exclusion threshold.\n","       *        ####   fit:   \n","                * #### description:   \n","                Performs the exclusion of rows or columns as described above.\n","                 *  #### input:  \n","                       * X = the dataset to be processed.\n","\n","                 *  #### output:  \n","                       * X = the dataset after being processed.\n","                       * nan_list = list containing the rows or columns that were excluded."]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":["class RemoveNanStructs():\n","    def __init__(self, dim = 0, nan_thr = 0.1):\n","        self.nan_thr = nan_thr\n","        self.dim = dim\n","    def fit(self,X):\n","        nan_list = []\n","        if self.dim == 0:    #row\n","            for idx,row in enumerate(X):\n","                nancols = [col for col in row if mt.isnan(col) == True]\n","                if len(nancols) >= len(row)*self.nan_thr: # row for deletion spotted\n","                    nan_list.append(idx)\n","        elif self.dim == 1:   #column\n","            for idy in range(len(X[0])):\n","                nanrows = [X[idx,idy] for idx in range(len(X)) if mt.isnan(X[idx,idy]) == True]\n","                if len(nanrows) >= len(X[0])*self.nan_thr: # column for deletion spotted\n","                    nan_list.append(idy)\n","        X = np.delete(X,nan_list,self.dim)\n","        return X,nan_list"],"execution_count":11,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["## BimodalImputation class\n","### Author:   Nestor Timonidis    \n","### Description:   \n","Given a vector that is partitioned into two classes, imputes its missing values by sampling from the two classes according to their distribution.   \n","*   ### functions:   \n","       *        #### init:    \n","                *  #### description:  \n","                initializes a BimodalImputation class object by selecting the default class for selection as defined by mode.\n","                *  #### input:  \n","                      *  mode =  the default class for selection. Default = 0.\n","                     \n","       *        ####   fit:   \n","                * #### description:   \n","                Performs the bimodal imputation as described above.\n","                 *  #### input:  \n","                       * y = the vector for imputation.\n","\n","                 *  #### output:  \n","                       * y = the imputed vector"]},{"metadata":{"collapsed":true,"trusted":true},"cell_type":"code","source":["class BimodalImputation():\n","    def __init__(self, mode = 0):\n","         self.mode = mode\n","    def fit(self, y):\n","        modality = {}\n","        nan_poses = [idx for idx,val in enumerate(y) if mt.isnan(val) == True]\n","        modality[0] = [idx for idx,val in enumerate(y) if val == self.mode]\n","        modality[1] = [idx for idx,val in enumerate(y) if val != self.mode and mt.isnan(val) == False]\n","        freq    = np.zeros((2,1))\n","        freq[0] = len(modality[0])/float(len(y))\n","        freq[1] = len(modality[1])/float(len(y))\n","        max_pos = [idx for idx in range(2) if freq[idx] == max(freq)][0]\n","        min_pos = [idx for idx in range(2) if freq[idx] == min(freq)][0]\n","\n","        for nan in nan_poses:   # rinse and repeat for all nans\n","            # Draw a dice\n","            dice = np.random.random_sample()\n","            if dice <= min(freq):\n","               rnd_sl = np.random.randint(low = 0, high = len(modality[min_pos]))\n","               sample = modality[min_pos][rnd_sl]\n","            else:\n","               rnd_sl = np.random.randint(low = 0, high = len(modality[max_pos]))\n","               sample = y[modality[max_pos][rnd_sl]]\n","\n","            y[nan] = sample\n","\n","        return y"],"execution_count":12,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["## Save2Nifti function \n","### Author: Nestor Timonidis    \n","### Description:\n","This function takes a voxelized gridded matrix and creates nifti volumes out of each columns, as well as converts \n","    the volumes to urls in order to be visualized in the Scalable Brain Composer 3D brain visualization tool.   \n","* ### Input:  \n","    * brainData = the voxelized gridded matrix - contains predictions of the connection between each grid and a number of target brain areas. \n","* ### Output:    \n","    * nifti_urls = the urls of the produced nifti volumes."]},{"metadata":{"collapsed":true,"trusted":true},"cell_type":"code","source":["def Save2Nifti(brainData):\n","    \n","    nifti_urls = {}\n","    print 'Saving results to nifti form ............... \\n'\n","    Affine = np.eye(4)\n","    outfile = 'volume_density'\n","    for row in range(3):\n","        Affine[row][row] = 0.2\n","    for experiment,p_density in brainData.items():\n","        density_img = nib.Nifti1Image(p_density, Affine)\n","        outfile_revisited = outfile + '_'+ str(experiment) + '.nii.gz'\n","        nib.save(density_img, outfile_revisited)\n","        nifti_urls[experiment] = make_data_url(outfile_revisited)\n","       \n","    print 'Finished converting nifti files to url form ............... \\n'    \n","    return nifti_urls"],"execution_count":13,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["## render_mpl_table function\n","### author: Nestor Timonidis\n","### description:   \n","This function renders for visualizing data in a table form "]},{"metadata":{"collapsed":true,"trusted":true},"cell_type":"code","source":["def render_mpl_table(data, col_width = 13.0, row_height = 0.625, font_size=14,\n","                     header_color='#40466e', row_colors=['#f1f1f2', 'w'], edge_color='w',\n","                     bbox=[0, 0, 1, 1], header_columns=0,\n","                     ax=None, **kwargs):\n","    if ax is None: \n","        size = (np.array(data.shape[::-1]) + np.array([0, 1])) * np.array([col_width, row_height])\n","        fig, ax = plt.subplots(figsize=size)\n","        ax.axis('off')\n","\n","    mpl_table = ax.table(cellText=data.values, bbox=bbox, colLabels=data.columns, **kwargs)\n","\n","    mpl_table.auto_set_font_size(False)\n","    mpl_table.set_fontsize(font_size)\n","\n","    for k, cell in  six.iteritems(mpl_table._cells):\n","        cell.set_edgecolor(edge_color)\n","        if k[0] == 0 or k[1] < header_columns:\n","            cell.set_text_props(weight='bold', color='w')\n","            cell.set_facecolor(header_color)\n","        else:\n","            cell.set_facecolor(row_colors[k[0]%len(row_colors) ])\n","    return ax"],"execution_count":14,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["## MesoconnectomePredictor class\n","### Author: Nestor Timonidis\n","### Description:   \n","This class nests a set of function that are involved in the mesoconnectome prediction pipeline.\n","*   ### init function\n","    *   ### description:   \n","    initalizes a MesoconnectomePredictor object by specifying the gene expression (GeneExp) and connectivity strength (ConStr) matrices for analysis.\n","\n",""]},{"metadata":{"collapsed":true,"trusted":true},"cell_type":"code","source":["class MesoconnectomePredictor:\n","\n","    def __init__(self, GeneExp = [], ConStr = []):\n","        self.GeneExp = GeneExp\n","        self.ConStr = ConStr"],"execution_count":15,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"markdown","source":["class MesoconnectomePredictor(MesoconnectomePredictor):    \n","    \n","    def GO2Csv(self, BP, MF, CC, finalhits, ont_hits, gene_list, gene_stars):\n","        # Desciption: Given GOenrichment analysis results, appends them to a .csv file\n","        BP = list(np.asarray(BP))[0]; MF = list(np.asarray(MF))[0]; CC = list(np.asarray(CC))[0]\n","        write_to = os.path.join(params['prefix'],'hide_the_csvs', str(params['cre-line']) + '_GOenrichment_results')\n","         \n","        GODict = OrderedDict()\n","        GODict['BIOLOGICAL PROCESS'] = [element for element in BP]\n","        GODict['MOLECULAR FUNCTION'] = [element for element in MF]\n","        GODict['CELLULAR COMPONENT'] = [element for element in CC]\n","        GODict['CELL TYPE MARKERS']  = [key + ' : ' + str(val) for key,val in finalhits.items()]\n","        GODict['CELL TYPE ONTOLOGIES']       = [key + ' : ' + str(val) for key,val in ont_hits.items()]\n","        GODict['TOP ' + str(gene_stars) + ' GENES']  = [gene for gene in gene_list]\n","        \n","        GO_DF = pd.DataFrame(dict([ (k, pd.Series(v)) for k,v in GODict.items() ])) \n","        tbl = render_mpl_table(GO_DF)\n","         \n","        plt.plot(frame_on = False) # no visible frame\n","        ax = plt.gca()\n","        ax.xaxis.set_visible(False)  # hide the x axis\n","        ax.yaxis.set_visible(False)  # hide the y axis\n","        table(ax, GO_DF)\n","        plt.savefig(write_to + '.png')\n","        plt.show()\n","        write_to = write_to + '.csv'    \n","        '''PM.writetable(['BIOLOGICAL PROCESS'], open(write_to, 'wb'))\n","        PM.writetable(BP, open(write_to, 'ab'))\n","        PM.writetable([''], open(write_to, 'ab'))\n","        PM.writetable(['MOLECULAR FUNCTION'], open(write_to, 'ab'))\n","        PM.writetable(MF, open(write_to, 'ab'))\n","        PM.writetable([''], open(write_to, 'ab'))\n","        PM.writetable(['CELLULAR COMPONENT'], open(write_to,'ab'))\n","        PM.writetable(CC, open(write_to, 'ab'))\n","        PM.writetable([''], open(write_to, 'ab'))\n","        PM.writetable(['CELL-TYPE ONTOLOGIES'], open(write_to,'ab'))\n","        PM.writetable(finalhits, open(write_to, 'ab'))\n","        PM.writetable([''], open(write_to, 'ab'))\n","        PM.writetable(['CAHOY ONTOLOGIES HITS'], open(write_to,'ab'))\n","        PM.writetable(ont_hits, open(write_to, 'ab'))\n","        PM.writetable([''], open(write_to, 'ab'))\n","        PM.writetable(['TOP ' + str(gene_stars) + ' GENES'], open(write_to,'ab'))\n","        PM.writetable(gene_list, open(write_to, 'ab'))'''"]},{"metadata":{},"cell_type":"markdown","source":["## MesoconnectomePredictor class\n","### GOenrichment function\n","### author: Paul Tiesinga, Nestor Timonidis\n","### description:  \n","Given a set of genes, this function performs gene ontology enrichment analysis by comparing the set with the org.Mm.eg.db mouse gene database provided by https://bioconductor.org/packages/release/data/annotation/html/org.Mm.eg.db.html. The script utilizes a nested script written in the R programming languages, using the rpy2 library\n","* ### input:\n","    * GeneList = the list of genes\n","* ### output:\n","    * BP = biological processes related to the gene list based on their statistical significance as indicated by the analysis\n","    * MF = molecular functions related to the gene list based on their statistical significance as indicated by the analysis\n","    * CC = cellular components related to the gene list based on their statistical significance as indicated by the analysis"]},{"metadata":{"collapsed":true,"trusted":true},"cell_type":"code","source":["class MesoconnectomePredictor(MesoconnectomePredictor):\n","\n","    def GOenrichment(self, GeneList):\n","        # Description: given a set of selected genes, performs enrichment\n","        #              analysis comparing to a bioconductor database\n","        ro.numpy2ri.activate()\n","        MGP = ro.r('''\n","        GOenrichment <-function(GeneList){\n","            source(\"http://bioconductor.org/biocLite.R\")\n","            biocLite(\"ALL\")\n","            biocLite(\"GOstats\")\n","            biocLite(\"Category\")\n","            biocLite(\"genefilter\")\n","            biocLite(\"org.Mm.eg.db\")\n","            biocLite(\"gage\")\n","            biocLite(pkgs=c(\"Biobase\", \"IRanges\", \"AnnotationDbi\"),\n","                suppressUpdates=FALSE,\n","                suppressAutoUpdate=FALSE,\n","                siteRepos=character(),\n","                ask=TRUE)\n","            #install.packages(\"RSQLite\")\n","            #install.packages(\"devtools\")\n","            #require(devtools)\n","            library(\"RSQLite\")\n","            library(\"gage\")\n","            library(\"genefilter\")\n","            library(\"org.Mm.eg.db\")\n","            library(DBI)\n","            library(\"GO.db\")\n","            library(\"GOstats\")\n","            library(\"Category\")\n","            library(\"annotate\")\n","            data(ALL, package=\"ALL\")\n","\n","            hgCutoff <- 0.001\n","            params <- new(\"GOHyperGParams\",\n","                   geneIds = GeneList,\n","                   universeGeneIds = NULL,\n","                   annotation = \"org.Mm.eg.db\",\n","                   ontology = \"BP\",\n","                   pvalueCutoff = hgCutoff,\n","                   conditional = FALSE,\n","                   testDirection = \"over\")\n","            BP  <- hyperGTest(params)\n","            sumBP <- data.frame(summary(BP))\n","            sumBP <- subset(sumBP, select=c(\"Term\"))\n","\n","            params <- new(\"GOHyperGParams\",\n","                   geneIds = GeneList,\n","                   universeGeneIds = NULL,\n","                   annotation = \"org.Mm.eg.db\",\n","                   ontology = \"MF\",\n","                   pvalueCutoff = hgCutoff,\n","                   conditional = FALSE,\n","                   testDirection = \"over\")\n","            MF <- hyperGTest(params)\n","            sumMF <- data.frame(summary(MF))\n","            sumMF <- subset(sumMF, select=c(\"Term\"))\n","\n","            params <- new(\"GOHyperGParams\",\n","                   geneIds = GeneList,\n","                   universeGeneIds = NULL,\n","                   annotation = \"org.Mm.eg.db\",\n","                   ontology = \"CC\",\n","                   pvalueCutoff = hgCutoff,\n","                   conditional = FALSE,\n","                   testDirection = \"over\")\n","            CC <- hyperGTest(params)\n","            sumCC <- data.frame(summary(CC))\n","            sumCC <- subset(sumCC, select=c(\"Term\"))\n","\n","            foo <- vector(mode=\"list\", length = 3)\n","            foo[[1]] <- sumBP\n","            foo[[2]] <- sumMF\n","            foo[[3]] <- sumCC\n","\n","            return(foo)\n","        }''')\n","        r_getname = ro.globalenv['GOenrichment']\n","        verdict = r_getname(GeneList)\n","        BP = verdict[0]\n","        MF = verdict[1]\n","        CC = verdict[2]\n","\n","        return BP, MF, CC"],"execution_count":16,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["## MesoconnectomePredictor class\n","### PlotTheResults function\n","### author: Nestor Timonidis\n","### description:  \n","This function plots and saves the results of the predictive analysis.\n","* ### input:\n","    * Results  = the analysis results\n","    * MetaInfo = information related to the plots. Examples include the size of the plot ticks and the filename for save.\n",""]},{"metadata":{"collapsed":true,"trusted":true},"cell_type":"code","source":["class MesoconnectomePredictor(MesoconnectomePredictor):\n","\n","    def PlotTheResults(self,Results,MetaInfo):\n","\n","            k = len(MetaInfo['xtick'])\n","            fsz = 9\n","            fig = plt.figure(figsize=(8,6))\n","\n","            #plt.tight_layout()\n","            plt.rcParams['figure.figsize']\n","            plt.boxplot(Results, 0, 'gD', widths = 0.6, whis = [5,95])\n","\n","            ax = plt.gca()\n","            ax.xaxis.set_tick_params(labelsize = 14)\n","            ax.yaxis.set_tick_params(labelsize = 14)\n","            plt.xticks([i+1 for i in range(k)], MetaInfo['xtick'],rotation = 0)\n","            plt.text(0.55, MetaInfo['max1'][1], MetaInfo['max1'][0], fontsize = fsz)\n","            plt.text(0.55, MetaInfo['min1'][1], MetaInfo['min1'][0], fontsize = fsz)\n","            plt.text(0.55, MetaInfo['med1'][1], MetaInfo['med1'][0], fontsize = fsz)\n","            plt.text(1.45, MetaInfo['max2'][1], MetaInfo['max2'][0], fontsize = fsz)\n","            plt.text(1.45, MetaInfo['min2'][1], MetaInfo['min2'][0], fontsize = fsz)\n","            plt.text(1.45, MetaInfo['med2'][1], MetaInfo['med2'][0], fontsize = fsz)\n","            plt.text(2.45, MetaInfo['max3'][1], MetaInfo['max3'][0], fontsize = fsz)\n","            plt.text(2.45, MetaInfo['min3'][1], MetaInfo['min3'][0], fontsize = fsz)\n","            plt.text(2.45, MetaInfo['med3'][1], MetaInfo['med3'][0], fontsize = fsz)\n","            plt.title(MetaInfo['title'], fontsize = 14)\n","            # plt.xlabel(MetaInfo['xlabel'])\n","            plt.ylabel(MetaInfo['ylabel'], fontsize = 14)\n","            plt.yticks(np.arange(MetaInfo['lb'] - np.std(Results), MetaInfo['ub'] + np.std(Results), np.std(Results)))\n","            #plt.yticks(np.arange(0.4,1.0,0.05))\n","\n","            plt.savefig(MetaInfo['save_file'])\n","            plt.show(block = False)\n","            plt.pause(1)\n","            plt.close()"],"execution_count":17,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["## MesoconnectomePredictor class\n","### Evaluation function\n","### author: Nestor Timonidis\n","### description:  \n","This function plots and saves the results of the predictive analysis.\n","* ### input: \n","    * ClfResults_lr = results related to a linear model (classifier or regressor).\n","    * ClfResults_rf = results related to a random forest model (classifier or regressor).\n","    * ClfResults_bl = results related to a control (baseline) model (classifier or regressor).\n","    * params = parameters related to the evaluation of the analysis. Examples include the abbreviations of the brain structures."]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":[" class MesoconnectomePredictor(MesoconnectomePredictor):   \n","    \n","    def Evaluation(self,ClfResults_lr,ClfResults_rf, ClfResults_bl, params):\n"," \n","          MetaInfo = {}\n","          templ = 'values for predicted tracer - projection patterns\\nof driver line:'\n","          MetaInfo['xlabel'] = 'Classifier Models'\n","          exclusions = ['Model Storage', 'Gene Scoring', 'Mean Gene Scoring']\n","          for measure in ClfResults_lr.keys():\n","             if measure not in exclusions:\n","               tmp1 = [val[0] for val in ClfResults_lr[measure]] \n","               tmp2 = [val[0] for val in ClfResults_rf[measure]] \n","               tmp3 = [val[0] for val in ClfResults_bl[measure]] \n","               sort1 = np.argsort(tmp1)[::-1]; sort3 = np.sort(tmp1)[::-1]\n","               sort2 = np.argsort(tmp2)[::-1]; sort4 = np.sort(tmp2)[::-1]\n","               sort5 = np.argsort(tmp3)[::-1]; sort6 = np.sort(tmp3)[::-1]\n","               set_length = len(sort1)   \n","            \n","               MetaInfo['min1']  = [params['structure-abbrev'][sort1[set_length-1]], sort3[set_length-1]]\n","               MetaInfo['max1']  = [params['structure-abbrev'][sort1[0]], sort3[0]]\n","               MetaInfo['med1']  = [params['structure-abbrev'][sort1[set_length/2]], sort3[set_length/2]]\n","    \n","               MetaInfo['min2']  = [params['structure-abbrev'][sort2[set_length-1]], sort4[set_length-1]]\n","               MetaInfo['max2']  = [params['structure-abbrev'][sort2[0]], sort4[0]]\n","               MetaInfo['med2']  = [params['structure-abbrev'][sort2[set_length/2]], sort4[set_length/2]]\n","               MetaInfo['min3']  = [params['structure-abbrev'][sort5[set_length-1]], sort6[set_length-1]]\n","               MetaInfo['max3']  = [params['structure-abbrev'][sort5[0]], sort6[0]]\n","               MetaInfo['med3']  = [params['structure-abbrev'][sort5[set_length/2]], sort6[set_length/2]] \n","               \n","               MetaInfo['save_file'] = params['cre-line'] + '_' + measure +'.png'\n","               MetaInfo['title']     = params['cre-line'] #measure + ' '+ templ + ' ' + params['cre-line']\n","               MetaInfo['ylabel']    = measure\n","               MetaInfo['xtick']     = ['Ridge Regression','Random Forest', 'Control']\n","               if measure == 'AURoc': \n","                  MetaInfo['lb'] = 0.4\n","                  MetaInfo['ub'] = 1.0\n","               elif measure == 'RMSE' or measure == 'r2': \n","                  MetaInfo['lb'] = min([sort3[set_length-1],sort4[set_length-1],sort6[set_length-1]])\n","                  MetaInfo['ub'] = max([sort3[0],sort4[0],sort6[0]])\n","               else: \n","                  MetaInfo['lb'] = 0.1 \n","                  MetaInfo['ub'] = 1.0\n","                \n","               JoinResults = np.asarray([(a[0],b[0],c[0]) for a,b,c in zip(ClfResults_lr[measure],ClfResults_rf[measure],ClfResults_bl[measure])])\n","               MesoPred.PlotTheResults(JoinResults,MetaInfo)  \n","          \n","          '''gene_stars =  50\n","          BP, MF, CC = MesoPred.GOenrichment(ClfResults_lr['Mean Gene Scoring'][0][0:gene_stars])\n","          GODict = OrderedDict()\n","          GODict['BIOLOGICAL PROCESS'] = [element for element in BP]\n","          GODict['MOLECULAR FUNCTION'] = [element for element in MF]\n","          GODict['CELLULAR COMPONENT'] = [element for element in CC]\n","          GO_DF = pd.DataFrame(dict([ (k, pd.Series(v)) for k,v in GODict.items() ])) \n","          table(ax, GO_DF)'''\n","          "],"execution_count":18,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["## MesoconnectomePredictor class\n","### PreProcessing function\n","### author: Nestor Timonidis\n","### description:  \n","This function applies a set of pre-processing steps to the input gene expression and connectivity strength data, given a set of parameters. The steps are as follows:\n","*  1: Removal of structures dominated by NaN (not-a-number) values.\n","*  2: For gene expression data, imputation of the rest of NaN values by the median value of their corresponding column (feature) vector. For the connectivity data, sampling per column (feature) vector based on the frequency of zero elements and non-zero ones. A custom function called for the sampling is the BimodalImputation function that is described in the above cells.\n","*  3: Scaling both matrices by transforming them based on their cubic square root and then z-scoring them.\n","*  4: Removing outlier genes for the gene expression data based on their interquartile range. The function being called for that purpose is the RemoveOutliers function.\n","*  5: Binarizing the connectivity strength matrix based on the 66th percentile per column (source brain area).\n","\n","* ### input: \n","    * GeneExp = gene expression matrix.\n","    * ConStr = connectivity strength matrix.\n","    * params = information related to the pre-processing steps. Examples include removed structures (restCons, nanGenes) and gene acronyms. \n","* ### output:\n","    * GeneExp_hat_ctr = Processed Gene Expression matrix, with a cubic square root transformation and z-scoring.\n","    * ConStr_sqrt_ctr = Processed Connectivity Strength matrix, with a cubic square root transformation and z-scoring.\n","    * y               = Binarized Conenctivity Strength matrix, based on the 66th percentile of the data per column.\n","    * sc_sqrt         = the scaler used to z-score the Connectivity Strength matrix."]},{"metadata":{"collapsed":true,"trusted":true},"cell_type":"code","source":["class MesoconnectomePredictor(MesoconnectomePredictor):\n","\n","    def PreProcessing(self,GeneExp, ConStr, params):\n","        \n","        # ************ Step 1: Cleaning the dataset from rows containing complete nans *******************#\n","        print 'Data cleaning phase ...\\n'\n","        print 'Size before data cleaning ' + str(np.shape(ConStr))\n","        \n","        ConStr,nanCons = RemoveNanStructs(0,0.1).fit(ConStr)\n","        print 'Size after intermediate cleaning ' + str(np.shape(ConStr))\n","        params['restCons'] = [idx for idx in range(len(GeneExp)) if idx not in nanCons]\n","        GeneExp = np.delete(GeneExp,nanCons,0)\n","       \n","        GeneExp,nanGenes = RemoveNanStructs(0,0.03).fit(GeneExp)\n","        params['nanGenes'] = nanGenes; params['nanCons'] = nanCons\n","        params['str_cpy'] = np.delete(params['structure-abbrev'],params['nanCons'],0); params['str_cpy'] = np.delete(params['str_cpy'],params['nanGenes'],0)\n","        params['restGenes'] = [idx for idx in range(len(ConStr)) if idx not in nanGenes]\n","        ConStr = np.delete(ConStr,nanGenes,0)\n","        print 'Size after data cleaning ' + str(np.shape(ConStr))\n","        m = len(ConStr[0])\n","        tmp = [params['leaf_keys'][val] for val in params['restCons']]\n","        params['remaining_indices']   = [tmp[val] for val in params['restGenes']]\n"," \n","        # ************** Impute the value of the remaining nans with column averaging **************#\n","        print 'Data imputation phase ...\\n'\n","        for task in range(len(ConStr[0])):\n","            nans = [val for val in ConStr[:,task] if mt.isnan(val) == True]\n","            if len(nans) > 0:\n","                ConStr[:,task] = BimodalImputation().fit(ConStr[:,task])\n","\n","        GeneExp = Imputer(missing_values = 'NaN', strategy = 'median', axis = 0, verbose=0, copy=True).fit_transform(GeneExp)\n","\n","        print 'Data normalization phase ....\\n'\n","        \n","        GeneExp_hat = np.power(GeneExp, 1./3)\n","        GeneExp_hat_ctr = StandardScaler().fit_transform(GeneExp_hat)\n","        ConStr_sqrt = np.power(ConStr, 1./3)\n","        sc_sqrt = StandardScaler()\n","        sc_sqrt.fit(ConStr_sqrt)\n","        ConStr_sqrt_ctr = sc_sqrt.transform(ConStr_sqrt)\n","\n","        # Remove outliers\n","        GeneExp_hat_ctr,rem_genes = RemoveOutliers().fit(GeneExp_hat_ctr)\n","        params['Gene Acronyms'] = np.delete(params['Gene Acronyms Original'],rem_genes,0);\n","        params['rem_genes'] = rem_genes\n","        print 'Removing outliers ... Reduced dim: '+ str(np.shape(GeneExp)) + ' ' + str(np.shape(ConStr))\n","\n","\n","        # Binarize the connectivity matrix for classification\n","        binThr  = 0.33333\n","        y       = np.zeros((np.shape(ConStr)))\n","        for j in range(ConStr.shape[1]):\n","            y[:,j] = BinarizeTheVector(ConStr[:,j],binThr)\n","\n","        return GeneExp_hat_ctr, ConStr_sqrt_ctr, y, sc_sqrt"],"execution_count":19,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["## MesoconnectomePredictor class\n","### LaminarRegistration function\n","### author: Nestor Timonidis\n","### description:  \n","Given a set of brain structure acronyms, this function scans the keywords for laminar-specific information and registers structures based on it.\n","* ### input: \n","    * acronyms = brain structure acronyms.\n","* ### output:\n","    * laminar_profiles = layer specific profiles attributed to specific brain structures"]},{"metadata":{"collapsed":true,"trusted":true},"cell_type":"code","source":["def LaminarRegistration(acronyms):\n"," \n","    laminar_profiles = []\n","    for acro in acronyms:\n","        numbers = [idx for idx,val in enumerate(acro) if val.isdigit()]\n","        if numbers != []: \n","            split_letter =  acro[numbers[0]-1]\n","            split_choices =  acro.split(split_letter)\n","            potential_choice = split_choices[len(split_choices)-1]\n","            blobs = [val for val in potential_choice if val.isdigit() == False]\n","            if len(blobs) < 2 and potential_choice != '' and 'r' not in potential_choice: \n","                laminar_profiles.append('layer ' + potential_choice)\n","            elif '6a' in acro:\n","                laminar_profiles.append('layer 6a')\n","            else:\n","                laminar_profiles.append('layer inspecific')\n","        else:\n","            laminar_profiles.append('layer inspecific')\n","          \n","    return laminar_profiles   "],"execution_count":20,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["## MesoconnectomePredictor class\n","### UnravelResults function\n","### author: Nestor Timonidis\n","### description:  \n","This function receives a ground truth vector and predictive results from a set of analyses \n","and makes statistical evaluations by unraveling the results and comparing them with the ground truth.\n","* ### input:  \n","    * predictions = predictive results and ground truth from a set of analyses. \n","    * params = information related to the predictive evaluation. Examples include the method category (classification or regression) and acronyms of genes used in the analysis.\n","    * scaler = model used to z-score the data before the predictive process. During the function restores the actual and predicted results into their original scale.\n","* ### output:\n","    * ClfResults = evaluations of the predictive results stored in a dictionary."]},{"metadata":{"collapsed":true,"trusted":true},"cell_type":"code","source":["class MesoconnectomePredictor(MesoconnectomePredictor):\n","    \n","    def UnravelResults(self,predictions, params, scaler):\n","         \n","        num      = 1\n","        if len(predictions) == 1:\n","            predictions = predictions[0]\n","        if params['method'] == 'classification':\n","            y        = np.transpose([task[2] for task in predictions])\n","            y_preds  = np.transpose([task[1] for task in predictions])\n","            y_scores = np.asarray([task[0] for task in predictions])\n","            y_scores = y_scores.transpose(1,0,2)\n","        elif params['method'] == 'regression':\n","            if len(predictions) > 5:\n","                y        = np.transpose([task[2] for task in predictions])\n","                y_preds  = np.transpose([task[1] for task in predictions])\n","                y_scores = np.asarray([task[0] for task in predictions])\n","                if len(predictions[0]) > 4:\n","                    coeffs = [task[4] for task in predictions]\n","                mdls     = [task[3] for task in predictions]    \n","            else:\n","                y        = predictions[2]\n","                y_preds  = predictions[1]\n","                y_scores = predictions[0]\n","                mdls     = predictions[3]\n","                coeffs   = predictions[4]\n","                y_preds  = scaler.inverse_transform(y_preds)\n","                y        = scaler.inverse_transform(y)\n","                    \n","        if params['distance prior'] == True:\n","           y_preds  = np.asarray([MesoPred.DiscrimFunc(y_score, target, params) for target, y_score in enumerate(y_scores)]).transpose()\n","\n","        ClfResults = {}\n","        ClfResults['Model Storage'] = mdls\n","        if params['method'] == 'classification':\n","            ClfResults['Precision'] = np.zeros((np.shape(y)[num],1))\n","            ClfResults['F-score']   = np.zeros((np.shape(y)[num],1))\n","            ClfResults['Recall']    = np.zeros((np.shape(y)[num],1))\n","            ClfResults['AURoc']     = np.zeros((np.shape(y)[num],1))\n","        elif params['method'] == 'regression':\n","            ClfResults['RMSE']      = np.zeros((np.shape(y)[num],1))\n","            ClfResults['r2']        = np.zeros((np.shape(y)[num],1))\n","      \n","        if len(predictions) > 4 and np.shape(coeffs)!= ():\n","        #if params['method'] == 'classification' and len(predictions[0][0]) > 4 and np.shape(coeffs[0])!= ():\n","            mean_coeffs = np.mean(coeffs,0)\n","            tmp_mean = np.argsort(mean_coeffs)[::-1]\n","            ClfResults['Mean Gene Scoring'] = [params['Gene Acronyms'][tmp_mean], mean_coeffs[tmp_mean], tmp_mean]\n","            ClfResults['Gene Scoring'] = []\n","            for coeff in coeffs:\n","                tmp = np.argsort(coeff)[::-1]\n","                ClfResults['Gene Scoring'].append([params['Gene Acronyms'][tmp],coeff[tmp], tmp])    \n","                \n","        '''elif params['method'] == 'regression' and len(predictions) > 4 and np.shape(coeffs)!= ():\n","            tmp = np.argsort(coeffs)[::-1]\n","            ClfResults['Gene Scoring'] = [params['Gene Acronyms'][tmp], coeffs[tmp], tmp]'''\n","       \n","        for num in range(len(y[0])):\n","            injection_task = 'Task ' + str(num)\n","            if params['method'] == 'classification':\n","                ClfResults['F-score'][num]   = metrics.f1_score(y[:,num], y_preds[:,num], \\\n","                                                                average = 'binary')\n","                ClfResults['Precision'][num] = metrics.precision_score(y[:,num], \\\n","                                                               y_preds[:,num],\\\n","                                                               average = 'binary')\n","                ClfResults['Recall'][num]    = metrics.recall_score(y[:,num], y_preds[:,num], \\\n","                                                             average = 'binary')\n","                ClfResults['AURoc'][num]     = metrics.roc_auc_score(y[:,num],y_scores[:,num][:,1],\\\n","                                                             average = 'micro')\n","            elif params['method'] == 'regression':\n","                ClfResults['RMSE'][num]      = metrics.mean_squared_error(y[:,num], y_preds[:,num])\n","                ClfResults['r2'][num]        = metrics.r2_score(y[:,num], y_preds[:,num])\n","\n","        return ClfResults"],"execution_count":21,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["## Part I: Predictive Workflow"]},{"metadata":{},"cell_type":"markdown","source":["Initializing the parameters of the supervised learning models to be used.  \n","The models applied in this analysis are: \n","*   classification = Random Forest classifier, Logistic Regression classifier\n","*   regression     = Random Forest regressor,  Ridge regressor"]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":["tol_num = 0.001\n","\n","logreg_params = {'penalty': ['l2'], 'C' : np.logspace(-4, 1,base = 10), \\\n","            'solver': ['newton-cg','lbfgs','sag','liblinear','saga'], \\\n","            'class_weight' : ['balanced']}\n","\n","rfc_params = {'max_depth': [10, 60, 100, None],\n","             'max_features': ['auto', 'sqrt'],\n","             'min_samples_leaf': [1, 2, 4],\n","             'min_samples_split': [2, 5, 10],\n","             'n_estimators': [25,50,100,200],\\\n","             'criterion': ['gini'],\n","             'oob_score': [True], 'random_state' : [None],\\\n","             'max_features' : ['auto'], 'class_weight' : ['balanced']}\n","\n","ridge_params = {'alpha': [10e-4,10e-3,10e-2,0.5,1,2.5,5,7.5,10,100],\n","              'solver' : ['svd', 'cholesky', 'lsqr', 'sparse_cg', 'saga'],\n","              'tol': [tol_num], 'max_iter' : [1000]}\n","\n","\n","sv_models = [GridSearchCV(LogisticRegression(), param_grid = logreg_params, scoring = 'f1', cv = 3, n_jobs = -1),\n","             GridSearchCV(RandomForestClassifier(), param_grid = rfc_params, scoring = 'f1', cv = 3, n_jobs = -1),\n","             DummyClassifier(),\n","             GridSearchCV(Ridge(), param_grid = ridge_params, \n","                          scoring = 'neg_mean_squared_error', \n","                          cv = 3, n_jobs = -1),\n","             RandomForestRegressor(n_estimators = 100),\n","             DummyRegressor()\n","            ]"],"execution_count":22,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["Loading a series of data structures that will be used in the analysis.  \n","The data are being loaded for reasons of time efficiency.  \n","The Classes_and_Modules notebook provides comments and descriptions for the functions in which the data were created"]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":["leaves         = pk.load(open('leaf_nodes.pkl','rb'))\n","GeneExp        = h5py.File('G_Exp.hdf5', 'r')['dataset1']\n","GeneMeta       = pk.load(open('GeneMeta.pkl','r'))\n","GeneAcros      = np.asarray([val['genes'][0]['entrez_id'] for val in GeneMeta])\n","GeneNames      = np.asarray([val['genes'][0]['acronym'] for val in GeneMeta])\n","GeneMetaDict   = {val : GeneNames[idx] for idx,val in enumerate(GeneAcros)}\n","ConDict        = pk.load(open('CreLineDict.pkl','rb'))\n","folder_name    = 'Paper4/'\n","targetprofiles = OrderedDict()"],"execution_count":23,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["Storage of information regarding the mouse brain structures.\n","The targetprofiles dictionary specializes in target areas \n","with regards to connectivity patterns in the mouse mesoconnectome.\n","Its keys are: \n","   *           str_acronym = acronyms of target areas/structures\n","   *           alt laminar profiles = laminar profiles of cortical structures\n","   *           father               = the brain regions in which each target belongs to      \n","   \n","   \n","The leaf_keys contain information about target areas that are present on the finest possible level of description.\n","For instance areas called VISp (primary visual area) and VISp l1 (primary visual area, layer 1) could both be found \n","in the dataset, and therefore the leaf_keys are being used to filter out VISp as a non-leaf key in the structure hierarchy\n","since a finer description is present (VISp l1)."]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":["with open('acr2parent.json', 'r') as fp:\n","     acr2parent = json.load(fp)\n","with open('structures.csv','rb') as fp:\n","     structure = readtable(fp)\n","struct_idx_dict = {val:idx for idx,val in enumerate(structure['id'])}\n","targetprofiles['str_acronym'] = [val for val in structure['acronym']]\n","targetprofiles['alt laminar profiles'] = LaminarRegistration(targetprofiles['str_acronym'])\n","# storing layer 1 and layer 4 related areas for a latter analysis\n","l4_vals = [idx for idx,val in enumerate(targetprofiles['alt laminar profiles']) if val == 'layer 4']\n","l1_vals = [idx for idx,val in enumerate(targetprofiles['alt laminar profiles']) if val == 'layer 1']\n","lam_to_idx = {val:targetprofiles['alt laminar profiles'][idx] for idx,val in enumerate(structure['id'])}\n","\n","# storage of leaf-level structures\n","leaf_keys = [int(val[0]) for val in leaves]  \n","\n","for key in targetprofiles.keys():\n","    targetprofiles[key] = [val for idx,val in enumerate(targetprofiles[key]) if idx in leaf_keys]\n","targetprofiles['father'] = []\n","for strut in targetprofiles['str_acronym']:\n","    father = [val for key,val in acr2parent.items() if key == strut]\n","    if father == []:\n","       targetprofiles['father'].append(strut) \n","    else:\n","       targetprofiles['father'].append(father[0])  "],"execution_count":24,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["The params dictionary stores information which will be used throughout the analysis.\n","It will be passed as a parameter through the functions and updated at various steps. "]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":["params = {'alter' : None, 'fetch' : 'all', 'distance prior': False,\\\n","          'acronyms': targetprofiles['str_acronym'],\\\n","          'method': 'classification', 'leaf': True, \\\n","          'validation' : [StratifiedKFold(n_splits = 3, shuffle = True), KFold(n_splits = 3, shuffle = True)],\\\n","          'prefix': 'Paper4/', 'primary models': [i for i in sv_models], \\\n","          'Gene Acronyms Original': GeneAcros, 'Gene Metadata': GeneMetaDict,\n","          'leaf_keys' : leaf_keys}"],"execution_count":25,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["Conversion of the Gene Expression dataset as a numpy array for analysis,  \n","initialization of various dictionaries to store preprocessed data\n","and results per tracer category (driver line),  \n","and initialization the the MesoconnectomePredictor for its\n","functions to be utilized"]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":["GeneExp = np.asarray(GeneExp)\n","rf_pred = {}; bl_pred = {}; ridge_pred = {}\n","ClfResults_ridge = {}; ClfResults_rf = {}; ClfResults_bl = {}\n","ConStr_proc = {}; GeneExp_proc = {}; y = {}\n","\n","# matrix reduction to leaf-level structures\n","GeneExp = GeneExp[leaf_keys,:]\n","MesoPred = MesoconnectomePredictor(GeneExp, ConDict['wild_type']['ConMat'])"],"execution_count":26,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["### Predictive Pipeline  \n","In our examples I utilize the unionized anatomical tract tracing data taken from the Allen Institute for Brain Science, as shown in the Class_and_Modules notebook. The data come in the form of a dictionary with nested matrices, where each dictionary corresponds to a tracer category like wild type or Syt6-Cre_KI148 cre-line experiments for instance.   \n","In order to demonstrate the MesoPred tool, a number of categories are selected as shown bellow. This analysis can take more than a day if run for all possible tracer categories. Therefore, for reasons of time efficiency one could skip this cell and move to the next one where the results can be loaded by stored pickle files."]},{"metadata":{},"cell_type":"markdown","source":["\n","In this pipeline a number of steps is being followed:  \n","0.  Selection of the tracer category (driver) of interest, reduction of tracing data to the leaf-level structures and storage of a number of parameters to be used in the analysis, such as the layer profiles and abbreviations of the target areas of the dataset.\n","1.  Preprocessing of the gene expression and tract tracing data. The function called in that step is PreProcessing\n","2.  Prediction with a logistic or linear ridge regression model using the cross-validation method for performance evaluation. The function called in that step is CustomCrossval.\n","3.  Prediction with a random forest classifier or regressor model using the cross-validation method for performance evaluation. The function called in that step is CustomCrossval.\n","4.  Prediction with a control (baseline) model using the cross-validation method for performance evaluation. The function called in that step is CustomCrossval.\n","5.  Storage of the predicted connectivity patterns, probability scores and gene importance (coefficient) scores per tracer in dictionaries stratified by the tested driver categories. The function called in that step is UnravelResults.\n","6.  Based on the stored connectivity patterns and the actual (ground truth) ones, evaluation of the predictive process with series of statistical measures. Examples include r squared and root mean squared error for regression, and f1-score and area under the roc curve for classification. The function called in that step is Evaluation."]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"raw","source":["for key in ConDict.keys():\n","    \n","    ## Step 0:\n","    # measure used not to repeat predictions that have already been done\n","    if os.path.isfile('./' + key +'_ridge_pred.pkl') == True: continue \n","    ConStr  = np.asarray(ConDict[key]['ConMat'])\n","    ConStr = ConStr[leaf_keys,:]\n","    params['injection_number'] = np.shape(ConStr)[1]  \n","    params['cre-line'] = key\n","    params['structure-abbrev'] = ConDict[key]['structure-abbrev']\n","    params['layer'] = ConDict[key]['layer'][0]\n","    params['cell-type'] = ConDict[key]['cell-type'][0]\n","    params['method'] = 'regression'\n","    print 'Driver line: '+ str(key) + '\\nTracer number: ' + str(ConStr.shape[1]) \n","    \n","    ## Step 1:\n","    GeneExp_proc[key], ConStr_proc[key], y[key], sc_sqrt = MesoPred.PreProcessing(GeneExp, ConStr, params)\n","    str_copy = np.delete(targetprofiles['str_acronym'],params['nanCons'],0); str_copy = np.delete(str_copy,params['nanGenes'],0)\n","    \n","    start = time.time()\n","    \n","    ## Step 2:\n","    print 'Ridge regression predictive phase ..'\n","    ridge_pred[key]  = CustomCrossval(params['primary models'][3], GeneExp_proc[key], ConStr_proc[key], params['validation'][1])\n","    pk.dump(ridge_pred[key], open(params['cre-line'] + '_ridge_pred.pkl', 'wb'))\n","    print 'Ridge ' + params['method'] +  ' has been completed' \n","    \n","    ## Step 3:\n","    print 'Random forest ' + params['method'] + ' predictive phase ..'\n","    rf_pred[key]  = CustomCrossval(params['primary models'][4], GeneExp_proc[key], ConStr_proc[key], params['validation'][1])\n","    pk.dump(rf_pred[key], open(params['cre-line'] +'_rf_pred.pkl','wb'))\n","    print 'Random forest ' + params['method'] +  ' has been completed' \n","    \n","    ## Step 4:\n","    print 'Control model ' + params['method'] +  ' phase'\n","    bl_pred[key]  = CustomCrossval(params['primary models'][5], GeneExp_proc[key], ConStr_proc[key], params['validation'][1])\n","    pk.dump(bl_pred[key], open(params['cre-line'] + '_bl_pred.pkl','wb'))\n","    print 'Control ' + params['method'] +  ' has been completed'  \n","    \n","    end = time.time() \n","    print  'Elapsed time: ' + str((end - start)/60)   \n","    \n","    ## Step 5:\n","    outfile = key + '_all_results' \n","    print 'Unraveling of results phase ..'\n","    ClfResults_ridge[key] = MesoPred.UnravelResults(ridge_pred[key], params, sc_sqrt)\n","    pk.dump(ClfResults_ridge[key], open(outfile + '_ridge.pkl','wb'))\n","    ClfResults_rf[key] = MesoPred.UnravelResults(rf_pred[key], params, sc_sqrt)\n","    pk.dump(ClfResults_rf[key], open(outfile + '_rf.pkl','wb'))\n","    ClfResults_bl[key] = MesoPred.UnravelResults(bl_pred[key], params, sc_sqrt)\n","    pk.dump(ClfResults_bl[key], open(outfile + '_bl.pkl','wb')) \n","    \n","    ## Step 6:\n","    print 'Evaluation of results phase ..'\n","    MesoPred.Evaluation(ClfResults_ridge[key], ClfResults_rf[key], ClfResults_bl[key], params) "]},{"metadata":{},"cell_type":"markdown","source":["In case that a set of results from a number of tracer categories have been stored,  \n","they can be loaded as shown bellow in order to save time."]},{"metadata":{"trusted":true},"cell_type":"code","source":["ClfResults_rf = {}; ClfResults_bl = {}; rf_pred = {}; bl_pred = {}\n","for key in ConDict.keys():\n","    if os.path.isfile('./' + key +'_ridge_pred.pkl') == True and os.path.isfile('./' + key + '_all_results' + '_ridge.pkl') == True:\n","        tmp = pk.load(open('./' + key +'_ridge_pred.pkl','rb')) \n","        ridge_pred[key] = [tmp[1],tmp[2]]\n","        ClfResults_ridge[key] = []#pk.load(open('./' + key + '_all_results' + '_ridge.pkl','rb')) \n","    '''if os.path.isfile('./' + key +'_rf_pred.pkl') == True and os.path.isfile('./' + key + '_all_results' + '_rf.pkl') == True: \n","        tmp = pk.load(open('./' + key +'_rf_pred.pkl','rb'))\n","        rf_pred[key] = [tmp[1],tmp[2]]\n","        ClfResults_rf[key] = pk.load(open('./' + key + '_all_results' + '_rf.pkl','rb'))      \n","    if os.path.isfile('./' + key +'_bl_pred.pkl') == True and os.path.isfile('./' + key + '_all_results' + '_bl.pkl') == True: \n","        tmp = pk.load(open('./' + key +'_bl_pred.pkl','rb'))\n","        bl_pred[key] = [tmp[1],tmp[2]]\n","        ClfResults_bl[key] = pk.load(open('./' + key + '_all_results' + '_bl.pkl','rb'))  '''"],"execution_count":27,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["### VISUALIZATION CHAPTER  \n","In this section, a number of steps are being followed   \n","for visualizing brain slices containing gene expression   \n","or connectivity information\n",""]},{"metadata":{},"cell_type":"markdown","source":["Utilization of the MouseConnectivityCache (mcc) tool, provided by the Allen Institute for Brain Science Software Development Toolkit (SDK) at:  \n","https://alleninstitute.github.io/AllenSDK/_modules/allensdk/core/mouse_connectivity_cache.html  \n","With the mcc tool, the annotation volume of the mouse brain is being downloaded in both 25 and 10 micrometer resolution for visualization purposes."]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":["mcc_25 = MouseConnectivityCache(resolution = 25)\n","mcc_10 = MouseConnectivityCache(resolution = 10)\n","Annotation_25, annot_info = mcc_25.get_annotation_volume()\n","Annotation_10, annot_info = mcc_10.get_annotation_volume()"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["** Application of a set of experiment ids in the brain visualization tools (Brain Plotter) **"]},{"metadata":{"trusted":true},"cell_type":"code","source":["# grab the StructureTree instance\n","structure_tree = mcc_10.get_structure_tree()\n","oapi           = OntologiesApi()\n","# Get isocortex summary structures\n","kid_to_father = {}\n","summary_structures = structure_tree.get_structures_by_set_id([688152357])\n","for val in summary_structures:\n","    kids = structure_tree.descendant_ids([val['id']])\n","    for kid in kids[0]:\n","        kid_to_father[kid] = val['id']\n","\n","# For each distinct isocortical area, replace its children annotation with the area's id\n","father_acros_indices = {val['acronym']:val['id'] for val in summary_structures}\n","\n","#store the original annotation shape because it will be flatten for computational simplicity when updating the parent nodes\n","orig_annot_shape = np.shape(Annotation_10)\n","flat_annot = np.ndarray.flatten(Annotation_10, order = 'C')\n","\n","# Create a copy of the annotation structure that reduces areas to their parent nodes\n","Annotation_copy = np.zeros(np.shape(flat_annot))\n","\n","for kid_id,parent_id in kid_to_father.items():\n","    tmp  = flat_annot == int(kid_id)\n","    nzero_ids = np.nonzero(tmp)\n","    Annotation_copy[nzero_ids] = parent_id \n","     \n","Annotation_copy = np.ndarray.reshape(Annotation_copy, orig_annot_shape, order = 'C')    \n","    \n","pk.dump(Annotation_copy,open('Annotation_copy.pkl','wb'))"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["Selection of the 11th and 21st experiment for demonstration of the BrainPlotter tool"]},{"metadata":{"trusted":true},"cell_type":"code","source":["tracer_for_selection = 'Ntsr1-Cre_GN220'\n","experiment_to_vis = [10,20]\n","UnionToVoxel_pred_25 = {}; UnionToVoxel_pred_10 = {}; UnionToVoxel_25 = {}; UnionToVoxel_10 = {}\n","struct_idx_dict_cp = struct_idx_dict.copy()\n","for key,value in struct_idx_dict.items():\n","    if value not in params['remaining_indices']:\n","       del struct_idx_dict_cp[key]\n","struct_idx_dict = struct_idx_dict_cp    \n","Annotation_copy = pk.load(open('Annotation_copy.pkl','rb'))"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["for j in experiment_to_vis:\n","    \n","    key_to_save = tracer_for_selection + '_' + str(j)\n","    exp_id = ConDict[tracer_for_selection]['id'][j]\n","    ConStr = ConDict[tracer_for_selection]['ConMat']\n","    l4_vol  = np.nanmean(ConStr[l4_vals,j]); l1_vol  = np.nanmean(ConStr[l1_vals,j])\n","    Pred_ConStr = ridge_pred[tracer_for_selection]\n","    \n","    #UnionToVoxel_25[key_to_save], UnionToVoxel_pred_25[key_to_save] = \\\n","    #                   FromUnion2Voxel(Annotation_25, struct_idx_dict, Pred_ConStr[2][:,j], Pred_ConStr[1][:,j])\n","    # pk.dump(UnionToVoxel_25[key_to_save], open('UnionToVoxel_25_for_' + str(key_to_save),'wb'))\n","    #UnionToVoxel_10[key_to_save], UnionToVoxel_pred_10[key_to_save] = \\\n","    #                   FromUnion2Voxel(Annotation_10, struct_idx_dict, Pred_ConStr[2][:,j], Pred_ConStr[1][:,j])\n","    # pk.dump(UnionToVoxel_10[key_to_save], open('UnionToVoxel_10_for_' + str(key_to_save),'wb'))        \n","    \n","    #BP_10 = BrainPlotter(mcc_10, experiment_id = exp_id, res = 10, driver = tracer_for_selection + str(exp_id))\n","    '''BP_25 = BrainPlotter(mcc_25, experiment_id = exp_id, res = 25, driver = tracer_for_selection + str(exp_id))\n","    \n","    BP_25.plot_slice(struct_idx_dict, 'None', Annotation_25, matrix = UnionToVoxel_25[key_to_save],\\\n","                     savefile = str(tracer_for_selection) + '_union_' + str(j) + '.png')\n","    BP_25.plot_slice(struct_idx_dict, 'None', Annotation_25, matrix = UnionToVoxel_pred_25[key_to_save],\\\n","                     savefile = str(tracer_for_selection) + '_union_pred_' + str(j) + '.png')\n","    '''\n","    BP_10.plot_flatmap('None', Annotation_copy, matrix = UnionToVoxel_10[key_to_save],\\\n","                       savefile = str(tracer_for_selection) + '_union_' + str(j) + '_flatmap')\n","    BP_10.plot_flatmap('None', Annotation_copy, matrix = UnionToVoxel_pred_10[key_to_save],\\\n","                       savefile = str(tracer_for_selection) + '_union_pred_' + str(j) + '_flatmap')\n","    \n","    print 'layer 4 volume ' + str(l4_vol) + '\\nlayer 1 volume ' + str(l1_vol)\n","   "],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["### Part IV: Link to Scalable Brain Atlas Composer Section"]},{"metadata":{},"cell_type":"markdown","source":["Parsing results to json format and Calling the SBA API service to visually inspect them"]},{"metadata":{"collapsed":true,"trusted":true},"cell_type":"code","source":["JSonDict = {'bas': {'provider': \"sba\",\n","             'atlas': \"ABA_v3\",\n","             'orientation': \"RAS\",\n","             'unit': \"mm\"},\n","             'style': {}, 'markers':[]} \n",""],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":["for tracer_name in ridge_pred.keys():\n","    if tracer_name == 'wild_type':\n","       tracer_layer = ''\n","       tracer_cell  = ''\n","    else:\n","       tracer_layer = ConDict[tracer_name]['layer'][0]\n","       tracer_cell =  ConDict[tracer_name]['cell-type'][0]\n","    for idx, measure in enumerate(ClfResults_ridge[tracer_name]['r2']):\n","        JSonDict['markers'].append({})\n","        JSonDict['markers'][len(JSonDict['markers'])-1]['value']    = str(measure[0])\n","        JSonDict['markers'][len(JSonDict['markers'])-1]['position'] = [str(coo) for coo in ConDict[tracer_name]['Coordinates'][idx]]\n","        JSonDict['markers'][len(JSonDict['markers'])-1]['region']   = ConDict[tracer_name]['structure-abbrev'][idx]\n","        JSonDict['markers'][len(JSonDict['markers'])-1]['relsize']  = \"0.5\"\n","        JSonDict['markers'][len(JSonDict['markers'])-1]['cre-line'] = tracer_name\n","        if 'layer' in ConDict[tracer_name]:\n","            JSonDict['markers'][len(JSonDict['markers'])-1]['layer']  = ConDict[tracer_name]['layer'][idx] \n"," "],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":["from IPython.display import display, Javascript, clear_output\n","\n","# SBA Composer page to load: here it is the -dev version and atlas ABA_v3\n","sbaHost = 'https://scalablebrainatlas.incf.org/composer-dev';\n","sbaUrl = sbaHost+'/?template=ABA_v3';\n","\n","class sbaInterface_class:\n","    def __init__(this,sbaUrl):\n","        # In Javascript, load sbaInterface.js and create a new sbaInterface object\n","        display(Javascript(\"\"\"\n","            var script = document.createElement('script');\n","            script.src = '{}/../js/sba-interface.js';\n","            script.onload = function() {{\n","              window.global_sbaInterface = new sbaInterface_class('{}');\n","            }}\n","            document.head.appendChild(script)\n","        \"\"\".format(sbaHost,sbaUrl)))\n","        clear_output()\n","        \n","    def send(this,sbaCommand):\n","        display(Javascript(\"\"\"window.global_sbaInterface.send({})\"\"\".format(sbaCommand)))\n","        clear_output() # prevents clogging the Jupyter notebook\n","\n","sbaInterface = sbaInterface_class(sbaUrl)"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["Utilization of the sbaInterface_class object to <br>\n","visually inspect the MesoPred results with the <br>\n","use of the 3D brain visualization tool <br>\n","'Scalable Brain Atlas Composer' <br>"]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":["#sbaInterface.send(json.dumps({'method':'Composer.scatter3d' ,'params' : JSonDict}))\n","sbaCommand = {\n","    'method':'Composer.scatter3d' ,\n","    'params' : JSonDict\n","}\n","sbaCommand = json.dumps(sbaCommand)\n","sbaInterface.send(sbaCommand)"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["After the prediction of connectivity patterns with the training of unionized data has been evaluated at the plotting phase,\n","the next and final step is the visual inspection of the connectivity prediction for the voxelized gridded data for which\n","there is no ground truth information. The predicted results for the models constructed with the Logistic Regression classification\n","algorithms have been selected as an example, and are being given to the Save2Nifti function. In this function, \n","nifti volume files are being produced with the connectivity patterns for various target brain areas. The nifti volumes  converted as urls can be given to the SBA composer as input for visual inspection, after the user has clicking on the provided link/s."]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":["nifti_urls = Save2Nifti(UnionToVoxel_10)"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["Now we can select indicative target areas and visually inspect the predicted connectivity patterns between the area and all voxels in the 3D volume space.\n","6 indicative examples are selected ('VISpl', 'OT', 'MPO', 'IPN', 'CEA', 'BST'):\n",""]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":["for key in nifti_urls.keys():\n","    display(HTML('<form target=\"SBA COMPOSER\" action=\"https://scalablebrainatlas.incf.org/composer-dev/index.php?template=ABA_v3\" method=\"post\"><input type=\"hidden\" name=\"scene\" value=\"{}\">Predicted connectivity patterns between 3D brain volume and area {} <input type=\"submit\" value=\"Show connections to {} in 3D\"></form>'.format(nifti_urls[key].replace('\"','&quot;'),key,key)))"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["**As a side note**: The plane corresponding to the brain slice at the SBA composer is not fixed but can be rotated with the use of the left click or zoomed with the right click. Moreover, the toolbar at the left of the screen provides to the user a set of visualization configurations.\n","Furthermore in terms of predictive results, the black color at the SBA composer represents a lack of connection between the voxels and the corresponding target area, while the white color represents connection between them."]},{"metadata":{"collapsed":true,"trusted":true},"cell_type":"code","source":["Clearing storage space from the nifti volumes"],"execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true},"cell_type":"code","source":["call(['rm','*.nii.gz'])"],"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python2","display_name":"Python 2","language":"python"},"language_info":{"mimetype":"text/x-python","nbconvert_exporter":"python","name":"python","pygments_lexer":"ipython2","version":"2.7.14","file_extension":".py","codemirror_mode":{"version":2,"name":"ipython"}}},"nbformat":4,"nbformat_minor":1}