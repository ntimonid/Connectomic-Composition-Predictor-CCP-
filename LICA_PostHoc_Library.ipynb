{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntimonidis/anaconda2/lib/python2.7/site-packages/nilearn/__init__.py:73: DeprecationWarning: Python2 support is deprecated and will be removed in the next release. Consider switching to Python 3.6 or 3.7.\n",
      "  _python_deprecation_warnings()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from PrimaryLibrary.ipynb\n",
      "importing Jupyter notebook from PrimaryLibrary.ipynb\n"
     ]
    }
   ],
   "source": [
    "from cfg import *\n",
    "import PrimaryLibrary as PL\n",
    "imp.reload(PL)\n",
    "MesoPred = PL.MesoconnectomePredictor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ParseTheMarkers(infolder = 'disease_gene_markers', infile = 'alzheimer_genes.csv'):\n",
    "    startline = 0\n",
    "    markers = []\n",
    "    if infile == 'alzheimer_genes.csv':\n",
    "        startline = 4\n",
    "    with open(os.path.join(infolder,infile),'r') as fp:\n",
    "        tbl = csv.reader(fp)\n",
    "        for idx,val in enumerate(tbl):\n",
    "            if idx >= startline:\n",
    "                if type(val) is list and len(val) > 0:\n",
    "                    for val_sub in val[0].split(', '):\n",
    "                        markers.append(val_sub)\n",
    "                elif type(val) is str and len(val) > 0:        \n",
    "                    markers.append(val[0])\n",
    "    return markers                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New additions 25-05-2020: update Component summary by including \n",
    "#                          a) spectral clustering of H and then nmi with the allen annotation\n",
    "#                          b) pearson correlation with the average template volume\n",
    "#a)\n",
    "from sklearn.cluster import spectral_clustering, SpectralClustering\n",
    "from sklearn.metrics import normalized_mutual_info_score as nmi    \n",
    "\n",
    "def PostHocLICA(annotation200, Non_zero_voxels, infile = 'data/Outputs_O_param_source_vis_all/M.p',\n",
    "                maskfile = 'saves/masks_4_wild_type_vis_all.pkl',\n",
    "                tempfile = 'average_template_200.nii.gz',\n",
    "                comp_filter = None):\n",
    "\n",
    "    M = pk.load(open(infile,'rb'))\n",
    "    new_masks = pk.load(open(maskfile,'rb'))\n",
    "    avg_template = nib.load(tempfile).get_data()\n",
    "    lica2gray = np.zeros((len(M['H'])))\n",
    "    flat_avg_temp = np.ndarray.flatten(avg_template,order = 'F')\n",
    "    flat_annot = np.ndarray.flatten(annotation200[0],order = 'F')\n",
    "    \n",
    "    if comp_filter is None:\n",
    "        comp_filter = np.arange(len(M['H']))\n",
    "    \n",
    "    affinity_2d = np.corrcoef(M['H'][comp_filter,:].T)\n",
    "    spect_clus  = SpectralClustering(n_clusters=20, affinity = 'precomputed').fit(np.abs(affinity_2d))\n",
    "    labels      = spect_clus.labels_\n",
    "    nmi_score   = nmi(flat_annot[Non_zero_voxels][new_masks['vox_mask']],labels)\n",
    "    print np.shape(labels), nmi_score\n",
    "\n",
    "    flat_avg_temp_sub = flat_avg_temp[Non_zero_voxels][new_masks['vox_mask']]\n",
    "    for i in comp_filter:\n",
    "        lica2gray[i] = sci.stats.pearsonr(np.abs(M['H'][i,:]),flat_avg_temp_sub)[0]\n",
    "\n",
    "    plt.hist(lica2gray)    \n",
    "\n",
    "\n",
    "    return lica2gray, labels, nmi_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CheckMarkers(gene_list, comps_of_int, infile = 'data/Outputs_O_param_200_source_vispor/'):\n",
    "    \n",
    "    # Document the z-statistics of a list of genes for a list of flicas\n",
    "    M = pk.load(open(os.path.join(infile, 'M.p'),'rb'))\n",
    "    gene_coef =  M['X'][0]\n",
    "    gene_coef_z = StandardScaler().fit_transform(gene_coef)\n",
    "    Dev_dict = {}\n",
    "    for comp in comps_of_int:\n",
    "        Dev_dict[comp] = {}\n",
    "        for gene in gene_list:\n",
    "            gene_id = [idx for idx,val in enumerate(MesoPred.params['Gene Acronyms Original']) if val == gene]\n",
    "            try:\n",
    "                Dev_dict[comp][gene] = gene_coef_z[gene_id[0],comp]\n",
    "                if len(gene_id) > 0 and np.abs(gene_coef_z[gene_id[0],comp]) > 1:\n",
    "                    #print(comp, gene, gene_coef_z[gene_id[0],comp])\n",
    "                    a = 1\n",
    "            except:\n",
    "                Dev_dict[comp][gene] = 0\n",
    "                \n",
    "    return Dev_dict            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MarkNeighborhood(InputArray,n_zero_vox, thickness):\n",
    "    \n",
    "    input_shape = np.shape(InputArray)\n",
    "    x_ind = np.arange(n_zero_vox[0]-thickness,n_zero_vox[0]+thickness+1)\n",
    "    x_ind = x_ind[np.where(x_ind >= 0)[0]]\n",
    "    x_ind = x_ind[np.where(x_ind < input_shape[0])[0]]\n",
    "    y_ind = np.arange(n_zero_vox[1]-thickness,n_zero_vox[1]+thickness+1)\n",
    "    y_ind = y_ind[np.where(y_ind >= 0)[0]]\n",
    "    y_ind = y_ind[np.where(y_ind < input_shape[1])[0]]\n",
    "    z_ind = np.arange(n_zero_vox[2]-thickness,n_zero_vox[2]+thickness+1)\n",
    "    z_ind = z_ind[np.where(z_ind >= 0)[0]]\n",
    "    z_ind = z_ind[np.where(z_ind < input_shape[2])[0]]\n",
    "    neigh_mask = [x_ind,y_ind,z_ind]\n",
    "    res_mask = np.meshgrid(neigh_mask[0],neigh_mask[1],neigh_mask[2])\n",
    "    res_mask = np.reshape(res_mask,(3,np.shape(res_mask)[1]*np.shape(res_mask)[2]*np.shape(res_mask)[3]), order = 'F')\n",
    "    \n",
    "    return res_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VoxelThickening(InputArray, thickness = 1):\n",
    "    input_shape = InputArray.shape\n",
    "    n_zero_mask = np.nonzero(InputArray)\n",
    "    print(len(np.nonzero(InputArray)[0])/(1.0*input_shape[0]*input_shape[1]*input_shape[2]))\n",
    "    for vox_el in range(len(n_zero_mask[0])):\n",
    "        n_zero_vox = (n_zero_mask[0][vox_el],n_zero_mask[1][vox_el],n_zero_mask[2][vox_el])\n",
    "        neigh_mask = MarkNeighborhood(InputArray,n_zero_vox, thickness)\n",
    "        try:\n",
    "            InputArray[neigh_mask[0],neigh_mask[1],neigh_mask[2]] = 1\n",
    "        except:\n",
    "            print('Unexpected error! Lets debug')\n",
    "            set_trace()\n",
    "        \n",
    "    print(len(np.nonzero(InputArray)[0])/(1.0*input_shape[0]*input_shape[1]*input_shape[2]))\n",
    "    \n",
    "    return InputArray    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FindComponent(Gene_coeff, GeneSet, interval = 95, part_thr = 0.02):\n",
    "    # find the component with the highest co-expression\n",
    "    old_max_prc = -1; sel_component = -1\n",
    "    low_perc = (100-interval)/2.0; high_perc = interval + low_perc\n",
    "\n",
    "    for component in range(len(Gene_coeff[0])):\n",
    "        sel_gene_footprint    = Gene_coeff[:,component]\n",
    "        gene_high_thr = np.percentile(sel_gene_footprint, high_perc); gene_low_thr = np.percentile(sel_gene_footprint, low_perc)\n",
    "        gene_grp = [idx for idx,val in enumerate(sel_gene_footprint) if val > gene_high_thr or val < gene_low_thr]\n",
    "        gene_join = set(gene_grp).intersection(set(GeneSet))\n",
    "        participation_prc = len(gene_join)/(1.0*len(GeneSet))\n",
    "        print(component, len(GeneSet), len(gene_grp), len(gene_join))\n",
    "        if participation_prc > part_thr and participation_prc > old_max_prc:\n",
    "            sel_component =  component\n",
    "            old_max_prc   =  participation_prc \n",
    "        #print(component, participation_prc)\n",
    "\n",
    "    print('The component with the highest co-expression is: {}'.format(sel_component))\n",
    "    \n",
    "    return sel_component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Resample_Voxels(spatial_modules, resolution = 25, resolution2 = 100, modules = 'all', \n",
    "                    interpolation = 'continuous', savefile = 'saves/modules_res_200.nii.gz'):\n",
    "    \n",
    "    affine_res    = resolution*0.001*np.eye(4); affine_res[3,3] = 1.0\n",
    "    affine_res2   = resolution2*0.001*np.eye(4); affine_res2[3,3] = 1.0\n",
    "    annot_path    = '../25 3 2019/annotation/ccf_2017/annotation_{}.nrrd'.format(resolution)\n",
    "    \n",
    "    # first rearrange the array such that the feature space is in the last dimension\n",
    "    input_size = spatial_modules.shape\n",
    "    '''spatial_modules_res = np.zeros((input_size[1],input_size[2],input_size[3],input_size[0]))\n",
    "    for tracer in range(input_size[0]):\n",
    "        spatial_modules_res[:,:,:,tracer] = spatial_modules[tracer,:,:,:]'''\n",
    "    \n",
    "    annot_res     = nrrd.read(annot_path)\n",
    "    annot_res_nii = nib.Nifti1Image(annot_res[0], affine_res)\n",
    "    upsamp_shape  = annot_res[0].shape \n",
    "    modules_res   = np.zeros((upsamp_shape[0],upsamp_shape[1],upsamp_shape[2],np.shape(spatial_modules)[3]))\n",
    "    \n",
    "    \n",
    "    if modules == 'all': \n",
    "        mod_set = range(np.shape(modules_res)[3])\n",
    "    else:\n",
    "        mod_set = [modules]\n",
    "        \n",
    "    start_time = time.time()\n",
    "    for module in mod_set:    # iterate over each individual module\n",
    "        print('module no {}'.format(module))\n",
    "        modules200   = nib.Nifti1Image(spatial_modules[:,:,:,module], affine_res2)\n",
    "        tmp  = resample_to_img(modules200, annot_res_nii, interpolation = interpolation)\n",
    "        if len(mod_set) > 1:\n",
    "            modules_res[:,:,:,module] = tmp.get_data()\n",
    "        else:\n",
    "            modules_res[:,:,:,0] = tmp.get_data()\n",
    "        ratio_of_100 =  len(np.nonzero(spatial_modules[:,:,:,module])[0])/(1.0*input_size[0]*input_size[1]*input_size[2])\n",
    "        ratio_of_200 =  len(np.nonzero(tmp.get_data())[0])/(1.0*tmp.shape[0]*tmp.shape[1]*tmp.shape[2])  \n",
    "        print(ratio_of_100,ratio_of_200)\n",
    "            \n",
    "    end_time = time.time()    \n",
    "    print('upsampling has been completed!\\nTime elapsed: {}'.format((end_time - start_time)/60.0))\n",
    "    \n",
    "    modules_res_img = nib.Nifti1Image(modules_res, affine_res)\n",
    "    nib.save(modules_res_img, savefile)\n",
    "    \n",
    "    return modules_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Merge_GO_Meta(go_meta):\n",
    "    \n",
    "    tmp_list = go_meta.values()\n",
    "    for r1,item1 in enumerate(tmp_list):\n",
    "        s = item1\n",
    "        for r2,item2 in enumerate(tmp_list):\n",
    "            flag = 0\n",
    "            intersection = set(item1.split(' ')).intersection(set(item2.split(' ')))\n",
    "            union = set(list(set(item1.split(' '))) + list(set(item2.split(' '))))\n",
    "            jac_sim = len(intersection)/(1.0*len(union))\n",
    "            if jac_sim >= 0.7 and jac_sim < 1.0:\n",
    "                difference = union - intersection\n",
    "                for element in difference:\n",
    "                    if element not in item1 and len(item1) > len(item2):\n",
    "                        s+= ' and ' + str(element)\n",
    "                        flag = 1\n",
    "                if flag == 1: # new element has been placed:\n",
    "                    tmp_list.pop(r2)\n",
    "            if len(s) > len(item1):\n",
    "                tmp_list.pop(r1)\n",
    "                tmp_list.append(s)           \n",
    "\n",
    "    s = ''\n",
    "    cnt = 0\n",
    "    for value in tmp_list:\n",
    "        if cnt > 0:\n",
    "            value = value.lower()\n",
    "        s += str(value)\n",
    "        if cnt < len(tmp_list)-1:\n",
    "            s+= ', and '\n",
    "        if cnt == len(tmp_list)-1:\n",
    "            s+='.'\n",
    "        cnt+=1            \n",
    "\n",
    "    return s\n",
    "\n",
    "def GetGroupEnrichments(GeneIds, masks, infile = 'data/Outputs_O_param_400_mouselight_mo/M.p', hgCutoff = 0.001,\n",
    "                       comps_of_int = None, tail = 97.5, method = 'relative'):\n",
    "    \n",
    "    # Note: chosing the method to be relative performs enrichment analysis relative to the compared sets,\n",
    "    # meaning that only genes that are exclusively highly expressed within a group will be selected.\n",
    "    # The other option is 'absolute', leading to the classical enrichment analysis\n",
    "    \n",
    "    def ListDiff(List1,List2):\n",
    "        # Returns elements from two lists, in separate new lists, that are exclusive to each other.\n",
    "        DiffList = [val for val in List1 if val not in List2]\n",
    "        DiffList2 = [val for val in List2 if val not in List1]\n",
    "\n",
    "        print len(DiffList), len(List1)\n",
    "        print len(DiffList2), len(List2)\n",
    "\n",
    "        return DiffList, DiffList2\n",
    "\n",
    "    M = pk.load(open(infile,'rb'))\n",
    "    go_df = OrderedDict()\n",
    "    GeneListDict = OrderedDict()\n",
    "    inj_grp = infile.split('_')\n",
    "    ing_grp = inj_grp[len(inj_grp)-1].split('/')[0]\n",
    "    if comps_of_int == None: comps_of_int = np.arange(len(M['H']))\n",
    "    UniverseList = [val for val in GeneIds if val is not None]\n",
    "\n",
    "    for comp in comps_of_int:\n",
    "        Component_res, spatial_footprint = ComponentSummary(M, sel_component = comp, \\\n",
    "                                                               tail = tail, add_mask = masks['vox_mask'])\n",
    "        GeneListDict[comp]   = [GeneIds[val] for val in Component_res['gene indices'] \n",
    "                                 if GeneIds[val] is not None]\n",
    "\n",
    "    go_df = OrderedDict()\n",
    "    go_meta = OrderedDict()\n",
    "    for comp in comps_of_int:\n",
    "        new_key = str(ing_grp) + ' ICA ' + str(comp)\n",
    "        GeneList2 = []\n",
    "        for comp2 in comps_of_int:\n",
    "            if comp != comp2:\n",
    "                GeneList2.append(GeneListDict[comp2])\n",
    "        GeneList2 = np.unique([val2 for val in GeneList2 for val2 in val])        \n",
    "        DiffList, DiffList2 = ListDiff(GeneListDict[comp],GeneList2)\n",
    "        if method == 'relative':\n",
    "            go_df[new_key],go_meta[new_key] = MesoPred.GOenrichment(DiffList, UniverseList, hgCutoff)\n",
    "            print new_key, len(DiffList)\n",
    "        else:\n",
    "            go_df[new_key], go_meta[new_key] = MesoPred.GOenrichment(GeneListDict[comp], UniverseList, hgCutoff)\n",
    "            print new_key, len(GeneListDict[comp])\n",
    "\n",
    "    Ontology_dict = OrderedDict()  \n",
    "    unique_dict = OrderedDict()\n",
    "    for key in go_df.keys():\n",
    "        aggr_annots = np.unique([val for comp2 in go_df.keys()\\\n",
    "                                 for val in go_df[comp2]['Term'].values\\\n",
    "                                 if comp2 != key])\n",
    "        unique_dict[key] = np.unique([val for idx,val in enumerate(go_df[key]['Term'].values)\\\n",
    "                                 if val not in aggr_annots])\n",
    "        unique_ids = [idx for idx,val in enumerate(go_df[key]['Term'].values)\\\n",
    "                                 if val not in aggr_annots]\n",
    "        Ontology_dict[key] = go_df[key].iloc[unique_ids, :]\n",
    "        \n",
    "    unique_df = pd.DataFrame.from_dict(unique_dict, orient = 'index').T   \n",
    "    \n",
    "    return Ontology_dict, go_meta, unique_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemoveNaNs(InputArray, annot_path = '../25 3 2019/annotation/ccf_2017/',resolution = 200):\n",
    "    \n",
    "    annot_path    = os.path.join(annot_path,'annotation_{}.nrrd'.format(resolution))\n",
    "    \n",
    "    annot_res     = nrrd.read(annot_path)\n",
    "    unique_annots = np.unique(annot_res[0])\n",
    "    \n",
    "    feature_size = np.shape(InputArray)[3] # last dimension of nifti-formated volumetric arrays\n",
    "    \n",
    "    for feature in range(feature_size):    # iterate over each individual module\n",
    "        for annot in unique_annots:\n",
    "            annotated_elements = InputArray[:,:,:,feature][annot_res[0] == annot]\n",
    "            possible_nans = np.isnan(annotated_elements)\n",
    "            InputArray[:,:,:,feature][annot_res[0] == annot][possible_nans] = np.nanmean(annotated_elements)\n",
    "            \n",
    "    return InputArray\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerativePriors(InputMat, M, modality = 'voxels'):\n",
    "    # Generating the coefficient matrices ...\n",
    "    OutputMat = np.zeros(np.shape(InputMat))\n",
    "    # options = {'voxels','weights','genes','tracers'}\n",
    "    if modality == 'voxels':\n",
    "        OutputMat  = np.random.normal(loc = 0, scale = 1, size = np.shape(InputMat))\n",
    "    elif modality == 'weights':\n",
    "        for comp in range(np.shape(InputMat)[1]):\n",
    "            for mod in range(2):\n",
    "                OutputMat[:,comp] = np.random.normal(loc = 0, scale = np.transpose(M['ARD'][mod,comp]), size = 1)\n",
    "    else:\n",
    "        if modality == 'genes':\n",
    "            mod = 0 \n",
    "        else:\n",
    "            mod = 1\n",
    "\n",
    "        for comp in range(np.shape(InputMat)[1]):\n",
    "            for mix in range(3):\n",
    "                OutputMat[:,comp] += M['pi'][mod][mix,comp]*np.random.normal\\\n",
    "                (loc = M['mu'][mod][mix,comp], scale = M['beta'][mod][mix,comp], size = np.shape(InputMat)[0])       \n",
    "\n",
    "    return OutputMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VisualizeVolume(InputMat, sel_element, savefile = None): \n",
    "    \n",
    "    InputMat_25 =  Resample_Voxels(InputMat, resolution = 25, modules = sel_element)\n",
    "    InputMat_10 =  Resample_Voxels(InputMat, resolution = 10, modules = sel_element)\n",
    "    BP_10 = PL.BrainPlotter(resolution = 10)\n",
    "    BP_10.fit(projection = InputMat_10[:,:,:,0])\n",
    "    BP_10.plot_flatmap(savefile = savefile + '_flatmap')\n",
    "    BP_25 = PL.BrainPlotter(resolution = 25)\n",
    "    BP_25.fit(projection = InputMat_25[:,:,:,0])\n",
    "    BP_25.plot_slice(savefile = savefile + '_slice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerativePosteriors(InputMat, M, modality = 'voxels'):\n",
    "    # Generating the coefficient matrices ...\n",
    "    OutputMat = np.zeros(np.shape(InputMat))\n",
    "    # options = {'voxels','weights','genes','tracers'}\n",
    "    if modality == 'voxels':\n",
    "        OutputMat  = np.random.normal(loc = 0, scale = 1, size = np.shape(InputMat))\n",
    "    elif modality == 'weights':\n",
    "        for comp in range(np.shape(InputMat)[1]):\n",
    "            for mod in range(2):\n",
    "                OutputMat[:,comp] = np.random.normal(loc = 0, scale = np.transpose(M['ARD'][mod,comp]), size = 1)\n",
    "    else:\n",
    "        if modality == 'genes':\n",
    "            mod = 0 \n",
    "        else:\n",
    "            mod = 1\n",
    "\n",
    "        for comp in range(np.shape(InputMat)[1]):\n",
    "            for mix in range(3):\n",
    "                OutputMat[:,comp] += M['pi'][mod][mix,comp]*np.random.normal\\\n",
    "                (loc = M['mu'][mod][mix,comp], scale = M['beta'][mod][mix,comp], size = np.shape(InputMat)[0])       \n",
    "\n",
    "    return OutputMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Step-wise component ranking routine\n",
    "def Step_Wise_Reconstruction(M,InputMat, mod_num = 0, thr = 0.9):\n",
    "    comp_list = np.arange(0,len(M['H']))\n",
    "    opt_subset = []\n",
    "    flag = 0           # condition indicating when a suboptimal solution has been reached (1) or not (0)\n",
    "    top_solution = -10\n",
    "    ReconMat     = np.matmul(np.multiply(M['X'][mod_num],M['W'][mod_num]),M['H'])\n",
    "    Error        = InputMat - ReconMat\n",
    "    while comp_list.size > 0 and flag == 0:\n",
    "        local_perf_list = []\n",
    "        for indeks,element in enumerate(comp_list):\n",
    "            temp_merge    = list(opt_subset); temp_merge.append(element)\n",
    "            ReconMat      = np.matmul(np.multiply(M['X'][mod_num][:,temp_merge],\n",
    "                                                  M['W'][mod_num][0,temp_merge]),M['H'][temp_merge,:])\n",
    "            r2_tracer_err = r2_score(InputMat,ReconMat + Error)\n",
    "            local_perf_list.append(r2_tracer_err)\n",
    "        top_idx    = np.argmax(local_perf_list)    \n",
    "        top_scorer = comp_list[top_idx]\n",
    "        if np.max(local_perf_list) < top_solution:\n",
    "            flag = 1 # termination condition no 2\n",
    "        else:\n",
    "            top_solution = np.max(local_perf_list) \n",
    "            opt_subset.append(top_scorer)\n",
    "            comp_list = np.delete(comp_list,top_idx)\n",
    "            if top_solution >= thr:\n",
    "                print('optimal r2 has been reached')\n",
    "                flag = 1\n",
    "\n",
    "    return opt_subset,top_solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ValidateTheResults(Data_Modality,ReconModality,ModalityError):\n",
    "    r2s = []; MSEs = []; err_r2s = []; err_MSEs = []\n",
    "    for mod in Data_Modality.keys():\n",
    "        if mod >= len(ReconModality): continue\n",
    "        Mod_Rec_Orig = (np.square(Data_Modality[mod].T - ReconModality[mod].T)).mean(axis=None)\n",
    "        MSEs.append(Mod_Rec_Orig)\n",
    "        r2_mod = r2_score(Data_Modality[mod].T, ReconModality[mod].T)\n",
    "        r2s.append(r2_mod)\n",
    "        Mod_Err_Orig = (np.square(Data_Modality[mod].T - ModalityError[mod].T)).mean(axis=None)\n",
    "        err_MSEs.append(Mod_Err_Orig)\n",
    "        r2_mod_err = r2_score(Data_Modality[mod].T, ModalityError[mod].T)\n",
    "        err_r2s.append(r2_mod_err)\n",
    "        \n",
    "        print('Reconstruction to Error comparison modality {} (MSE): {} - {}'.format(mod, Mod_Rec_Orig, Mod_Err_Orig))\n",
    "        print('Reconstruction to Error comparison modality {} (r2): {} - {}'.format(mod, r2_mod, r2_mod_err))\n",
    "        \n",
    "    return r2s, MSEs, err_r2s, err_MSEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReconstructingData(M,Spatial_map,mod_num):\n",
    "  \n",
    "    VoxelSize = len(Spatial_map)\n",
    "    ReconMat  = np.matmul(np.multiply(M['X'][mod_num],M['W'][mod_num]),Spatial_map)\n",
    "   \n",
    "    return ReconMat\n",
    "\n",
    "clear_output()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Predictingdata(M, Data_Modality, GeneError = None, TracerError = None):\n",
    "    gene_coef   = np.multiply(M['X'][0],M['W'][0])\n",
    "    tracer_coef = np.multiply(M['X'][1],M['W'][1])\n",
    "    if GeneError is None:\n",
    "        GeneError = np.zeros(np.shape(Data_Modality[0]))\n",
    "    if TracerError is None:\n",
    "        TracerError = np.zeros(np.shape(Data_Modality[1]))    \n",
    "    H_hat       = (np.linalg.inv((gene_coef).T.dot(gene_coef)).dot(gene_coef.T)).dot(Data_Modality[0] - GeneError)\n",
    "    Y_con_pred  = tracer_coef.dot(H_hat) + TracerError\n",
    "    \n",
    "    return Y_con_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetAncestor(structure_abbrev, level = None, mode = 'id'):\n",
    "    mcc = MouseConnectivityCache(manifest_file='connectivity/mouse_connectivity_manifest.json')\n",
    "    # getting the structure tree of the CCF v3\n",
    "    structure_tree = mcc.get_structure_tree()\n",
    "\n",
    "    with open('../CCP_Collab_Backup_2/structures.csv','rb') as fp:\n",
    "        structure   = PL.readtable(fp) \n",
    "\n",
    "    # getting the ids of the source areas    \n",
    "    if mode == 'acro':\n",
    "        src_ids = [structure['id'][id2] for idx,val in enumerate(structure_abbrev) \n",
    "                       for id2,val2 in enumerate(structure['acronym']) if val == val2]\n",
    "    elif mode == 'id':    \n",
    "        src_ids = structure_abbrev\n",
    "        \n",
    "    # creating an ancestry dictionary\n",
    "    try:\n",
    "        ancestry_trg_dict = structure_tree.ancestors(map(int, src_ids))\n",
    "    except:\n",
    "        print('Id not included in the structure')\n",
    "        \n",
    "    primary_list = ['CTX', 'OLF', 'HIP', 'RHP', 'STR', 'PAL', 'TH', 'HY', 'MB', 'P', 'MY', 'CB']\n",
    "    if level is not None:\n",
    "        primary_src_acros = OrderedDict([(structure_abbrev[i],ancestry_trg_dict[i][level]['name']) for i in range(len(ancestry_trg_dict)) if len(ancestry_trg_dict[i]) > level])\n",
    "    else:\n",
    "        primary_src_acros = OrderedDict() #[(structure_abbrev[i],ancestry_trg_dict[i][level]['name']) for i in range(len(ancestry_trg_dict)) for level in range(len(ancestry_trg_dict[i])) if ancestry_trg_dict[i][level]['acronym'] in primary_list])\n",
    "        for i in range(len(ancestry_trg_dict)):\n",
    "            for level in range(len(ancestry_trg_dict[i])):\n",
    "                if ancestry_trg_dict[i][level]['acronym'] in primary_list:\n",
    "                    primary_src_acros[structure_abbrev[i]] = ancestry_trg_dict[i][level]['name']\n",
    "                    #print(i, ancestry_trg_dict[i][level]['acronym'])      \n",
    "                    break\n",
    "                    \n",
    "    return primary_src_acros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters for picking tailed samples\n",
    "def ComponentSummary(M, Data_Modality = None, mask_path = 'data/annotation200_mask.nii.gz', sel_component = 0, \n",
    "                     tail = 99, resolution = 200, nifti = 'no', add_mask = None):\n",
    "    \n",
    "    # Initialization of important variables \n",
    "    high_perc = 100 - (100 - tail)/2; low_perc = (100 - tail)/2\n",
    "    with open('../CCP_Collab_Backup_2/structures.csv','rb') as fp:\n",
    "        structure       = PL.readtable(fp) \n",
    "        struct_idx_dict = {val:idx for idx,val in enumerate(structure['id'])}\n",
    "    \n",
    "    annot_path = '../25 3 2019/annotation/ccf_2017/annotation_{}.nrrd'.format(resolution)\n",
    "    n_mask = nib.load(os.path.join(mask_path)).get_data() \n",
    "    mask2d = np.reshape(n_mask,[n_mask.shape[0]*n_mask.shape[1]*n_mask.shape[2],1], order='F')\n",
    "    Non_zero_voxels = ~np.all(mask2d == 0, axis = 1)\n",
    "    if add_mask is not None:\n",
    "        new_nzero = np.nonzero(Non_zero_voxels == True)[0][add_mask]\n",
    "        Non_zero_voxels = [True if idx in new_nzero else False for idx,val in enumerate(Non_zero_voxels)]\n",
    "    annotation = nrrd.read(annot_path)\n",
    "    annot_shape = n_mask.shape\n",
    "    BP = PL.BrainPlotter(resolution = resolution)\n",
    "    mcc = MouseConnectivityCache(resolution = resolution)\n",
    "    #annotation_parents = BP.ReduceToParent(annotation[0], mcc)\n",
    "    #annot_parent_res  = np.reshape(annotation[0],(np.shape(annotation[0])[0]*np.shape(annotation[0])[1]*np.shape(annotation[0])[2]),order='F')\n",
    "    \n",
    "    # select participations of the modality of interest and find the proper cutoffs\n",
    "    annot_res         = np.reshape(annotation[0],\n",
    "                                       (annot_shape[0]*annot_shape[1]*annot_shape[2]), order='F')\n",
    "    annot_nzero       = annot_res[Non_zero_voxels]\n",
    "    sel_spatial_footprint = M['H'][sel_component,:]\n",
    "    sel_gene_footprint    = M['X'][0][:,sel_component]\n",
    "    sel_tracer_footprint  = M['X'][1][:,sel_component] \n",
    "    \n",
    "    # Scale Arrays by multiplying them with their components\n",
    "    spatial_footprint = {} \n",
    "    for i in range(len(M['X'])+1):\n",
    "        if Data_Modality is None: break\n",
    "        if nifti == 'no': break\n",
    "        if i < 2:\n",
    "            tmp     = np.dot(M['X'][i][:,sel_component].T,Data_Modality[i]).T\n",
    "            if add_mask is not None:\n",
    "                tmp = tmp[add_mask]\n",
    "            outfile = 'saves/modality_{}_component_{}'.format(i, sel_component)\n",
    "        else:\n",
    "            tmp     = np.reshape(M['H'][sel_component,:], (len(M['H'][0]),1))\n",
    "            outfile = 'saves/H_of_component_{}'.format(sel_component)\n",
    "        \n",
    "        spatial_footprint[i] = np.zeros(mask2d.shape)\n",
    "        spatial_footprint[i][Non_zero_voxels] = tmp\n",
    "        spatial_footprint[i] = np.reshape(spatial_footprint[i],\n",
    "                                          (annot_shape[0],annot_shape[1],annot_shape[2],1), \n",
    "                                          order = 'F')\n",
    "        PL.Save2Nifti(spatial_footprint[i], outfile = outfile)     \n",
    "        VisualizeVolume(abs(spatial_footprint[i]), 0, \n",
    "                        savefile = 'component_{}_map'.format(sel_component))\n",
    "     \n",
    "    gene_high_thr = np.percentile(sel_gene_footprint, high_perc); \n",
    "    gene_low_thr = np.percentile(sel_gene_footprint, low_perc)\n",
    "    tracer_high_thr = np.percentile(sel_tracer_footprint, high_perc); \n",
    "    tracer_low_thr = np.percentile(sel_tracer_footprint, low_perc)\n",
    "    spatial_high_thr = np.percentile(sel_spatial_footprint, high_perc); \n",
    "    spatial_low_thr = np.percentile(sel_spatial_footprint, low_perc)\n",
    "    \n",
    "    # stratify the groups belonging to two tails\n",
    "    spatial_grp = [idx for idx,val in enumerate(sel_spatial_footprint) \n",
    "                   if val >= spatial_high_thr or val <= spatial_low_thr]\n",
    "    gene_grp = [idx for idx,val in enumerate(sel_gene_footprint) \n",
    "                if val >= gene_high_thr or val <= gene_low_thr]\n",
    "    tracer_grp = [idx for idx,val in enumerate(sel_tracer_footprint) \n",
    "                  if val >= tracer_high_thr or val <= tracer_low_thr]    \n",
    "\n",
    "    \n",
    "    # Link the activated voxels to coarse-grain target brain areas\n",
    "    target = {}\n",
    "    if np.max(spatial_grp) > 428:\n",
    "        strong_vxls = np.argsort(np.abs(M['H'][sel_component,:]))[::-1][0:20]\n",
    "        strong_vxls_annot = [str(annot_nzero[val]) for val in strong_vxls]\n",
    "        target['strong areas'] = np.unique([structure['acronym'][idx] for idx,val in enumerate(structure['id']) for val2 in strong_vxls_annot if val==val2])\n",
    "        target['annot'] = np.unique([str(annot_nzero[val]) for val in spatial_grp])\n",
    "    else:\n",
    "        target['annot'] = [structure['id'][MesoPred.params['remaining_indices'][val]] \n",
    "                           for val in spatial_grp]\n",
    "     \n",
    "    target['acro'] = np.unique(GetAncestor(target['annot']))\n",
    "    target['leaf acro'] = np.unique([structure['acronym'][idx] for idx,val in enumerate(structure['id']) for val2 in target['annot'] if val==val2])\n",
    "    unique_targets = np.unique(target['acro'][0].values(), return_counts = True)\n",
    "    target['acro'] = {v1:v2 for v1,v2 in zip(unique_targets[0],unique_targets[1])}\n",
    "    target_only_ancestor = target['acro'].keys()\n",
    "    target['idx']   = [struct_idx_dict[str(val)] for val in target['annot'] \n",
    "                       if str(val) in struct_idx_dict.keys()]\n",
    "\n",
    "    # Link the selected injections to their source brain areas\n",
    "    source = np.unique([MesoPred.ConDict['wild_type']['structure-abbrev'][val] for val in tracer_grp])\n",
    "    #source = [structure['name'][id2] for val in source for id2,val2 in enumerate(structure['acronym']) if val == val2]\n",
    "    source_only_ancestor = [GetAncestor([val],0, mode = 'acro') for val in source]\n",
    "    source_only_ancestor = np.unique([val.values()[0] for val in source_only_ancestor if len(val.values()) > 0])\n",
    "    \n",
    "    #Link the gene groups to significant ontologies\n",
    "    gene_id   = [MesoPred.params['Gene Ids Original'][val] for val in gene_grp \n",
    "                 if MesoPred.params['Gene Ids Original'][val] is not None]\n",
    "    gene_acro = [MesoPred.params['Gene Acronyms Original'][val] for val in gene_grp \n",
    "                 if MesoPred.params['Gene Acronyms Original'][val] is not None]\n",
    "    #ontology  = MesoPred.GOenrichment(gene_id)\n",
    "\n",
    "    Component_res = {'source areas': source_only_ancestor, 'target areas': target['acro'],\n",
    "                    'target indices': spatial_grp, 'source indices': tracer_grp, 'gene acronyms': \n",
    "                     gene_acro, 'gene indices': gene_grp, 'target annots': target['annot'],\n",
    "                    'strongest areas': target['strong areas'], 'leaf acronyms': target['leaf acro']}\n",
    "\n",
    "    GO_DF = pd.DataFrame(dict([(k, pd.Series(v)) for k,v in Component_res.items() \n",
    "                               if 'indices' not in k and 'source areas' not in k]))\n",
    "    GO_DF.to_csv('figures/component_{}_new.csv'.format(sel_component))\n",
    "\n",
    "    return Component_res, spatial_footprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HeatmapVisualization(inputMat, index = None, columns = None, savefig = None):\n",
    "    \n",
    "    if index is not None and columns is not None:\n",
    "        #NeuronDf = pd.DataFrame(np.power(inputMat, 1/3.0),index = index, columns = columns) \n",
    "        NeuronDf = pd.DataFrame(inputMat,index = index, columns = columns) \n",
    "    else:    \n",
    "        NeuronDf = pd.DataFrame(np.power(inputMat, 1/3.0))\n",
    "        \n",
    "    plt.figure(figsize=(20,12))\n",
    "    sns.heatmap(NeuronDf)\n",
    "    ax = plt.gca()\n",
    "    ax.xaxis.set_tick_params(labelsize = 12, rotation = 280)\n",
    "    ax.yaxis.set_tick_params(labelsize = 12)\n",
    "    if savefig is not None:\n",
    "        plt.savefig(savefig)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VolumeBuilder(Data_Modality, Non_zero_voxels, comp_set, shape_of_200, M = None, \n",
    "                  outfile = 'saves', filename = 'modality_', mod_list = None, masks = None):\n",
    "    \n",
    "    Volume = OrderedDict()\n",
    "    affine_res = np.eye(4)*0.2; affine_res[3,3] = 1\n",
    "    if mod_list == None: mod_list = list(Data_Modality.keys())\n",
    "    for mod in mod_list: # Pick a data modality\n",
    "        spatial_map_res = np.zeros((shape_of_200[0],shape_of_200[1],shape_of_200[2], len(comp_set)))\n",
    "        for i,dicti in enumerate(comp_set):\n",
    "            spatial_map_tmp = np.zeros((shape_of_200[0]*shape_of_200[1]*shape_of_200[2],1))\n",
    "            if M is not None:\n",
    "                if masks is not None and mod == 1:\n",
    "                    A = Data_Modality[mod][masks['source_mask'],:]\n",
    "                    spatial_map_tmp[np.nonzero(Non_zero_voxels)[0][masks['vox_mask']]] = \\\n",
    "                    np.matmul(M['X'][mod][:,dicti]*M['W'][mod][0][dicti], A[:,masks['vox_mask']]).T\n",
    "                elif masks is not None and mod == 2:    \n",
    "                        B = Data_Modality[mod][masks['source_mask_II'],:]\n",
    "                        spatial_map_tmp[np.nonzero(Non_zero_voxels)[0][masks['vox_mask']]] = \\\n",
    "                    np.matmul(M['X'][mod][:,dicti]*M['W'][mod][0][dicti], B[:,masks['vox_mask']]).T\n",
    "                else:    \n",
    "                    spatial_map_tmp[np.nonzero(Non_zero_voxels)[0]] = np.matmul(M['X'][mod][:,dicti]*M['W'][mod][0][dicti],\\\n",
    "                                                                            Data_Modality[mod]).T\n",
    "\n",
    "            else:\n",
    "                spatial_map_tmp[np.nonzero(Non_zero_voxels)[0],0] = Data_Modality[mod][dicti,:].T\n",
    "            spatial_map_res[:,:,:,i] = np.reshape(spatial_map_tmp,\n",
    "                                         (shape_of_200[0],shape_of_200[1],shape_of_200[2]), order = 'F')\n",
    "\n",
    "        spatial_map_nii = nib.Nifti1Image(spatial_map_res, affine_res)\n",
    "        print(mod, spatial_map_nii.shape)\n",
    "        nib.save(spatial_map_nii,os.path.join(outfile, filename + '_{}.nii.gz'.format(mod))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PreProcessing(InputData, mask_path = 'data/annotation200_mask.nii.gz', analysis = 'volumetric',\n",
    "                  add_mask = None):\n",
    "    \n",
    "    import flica_various as alb_various\n",
    "    \n",
    "    n_mask = nib.load(os.path.join(mask_path)).get_data() \n",
    "    mask2d = np.reshape(n_mask,[n_mask.shape[0]* n_mask.shape[1]* n_mask.shape[2],1], order='F')\n",
    "    Non_zero_voxels = ~np.all(mask2d == 0, axis=1)\n",
    "    \n",
    "    Data_Modality = {}; cnt = 0\n",
    "    Data_Modality_sc = {}\n",
    "    #Data_Modality[0] = ExpPerInj; Data_Modality[1] = ProjPerInj\n",
    "    for modality in InputData:\n",
    "        Data_Modality[cnt] = modality\n",
    "        cnt += 1\n",
    "    \n",
    "    for k in Data_Modality.keys():\n",
    "        shape = Data_Modality[k].shape\n",
    "        if analysis == 'volumetric' and len(shape) > 2:\n",
    "            Data_Modality[k] = np.reshape(Data_Modality[k] ,[shape[0]*shape[1]*shape[2],shape[3]], order='F')\n",
    "            Data_Modality[k] = Data_Modality[k][Non_zero_voxels,:].T\n",
    "            # Mew step for reduced masks\n",
    "            if add_mask is not None:\n",
    "                Data_Modality[k] = Data_Modality[k][:,add_mask['vox_mask']]\n",
    "                if len(Data_Modality[k]) < 1000: # injection matrix\n",
    "                    Data_Modality[k]  = Data_Modality[k][add_mask['source_mask'],:]\n",
    "        Data_Modality_sc[k] = Data_Modality[k]- (np.matrix(np.mean(Data_Modality[k],1)).T)   \n",
    "        scaling_data_transform = alb_various.rms(Data_Modality_sc[k],[],[])       \n",
    "        Data_Modality_sc[k] = np.divide(Data_Modality_sc[k],scaling_data_transform)\n",
    "        if analysis == 'unionized':\n",
    "            Data_Modality[k] = Data_Modality[k].T\n",
    "            \n",
    "    return Data_Modality, Data_Modality_sc, Non_zero_voxels        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BuildWhiteMatterMask(annotation200):\n",
    "    \n",
    "    unique_annot = np.unique(annotation200[0])\n",
    "    unique_annot = np.delete(unique_annot,0)\n",
    "    ancestry_results = GetAncestor(unique_annot, level = 3)\n",
    "    fiber_ids =  [key for key,value in ancestry_results.items() if value == 'fiber tracts']\n",
    "    fiber_mask = np.zeros(np.shape(annotation200[0])) + 1\n",
    "    for i,x in enumerate(annotation200[0]):\n",
    "        for j,y in enumerate(x):\n",
    "            for k,z in enumerate(y):\n",
    "                if z in fiber_ids:\n",
    "                    fiber_mask[i,j,k] = 0\n",
    "    affine_res = np.eye(4)*0.2; affine_res[3,3] = 1.0\n",
    "    fiber_mask_nii = nib.Nifti1Image(fiber_mask, affine_res)    \n",
    "    nib.save(fiber_mask_nii, 'fiber_mask.nii.gz')\n",
    "    \n",
    "    return fiber_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConnectivitySim(Data_Modality, M, lamda, gene_list, mask_path = None, savefile = None, sel_component = None):\n",
    "    \n",
    "    # Given multiple data modalities and a dictionary of mixing components, this function predicts changes in \n",
    "    # modality 2 given changes in a modality 1, finds the features with the strongest changes and compares \n",
    "    # them to their mixing coefficients\n",
    "    \n",
    "    # Initialize the Delta G matrix \n",
    "    Delta_G = np.zeros(np.shape(Data_Modality[0]))\n",
    "    \n",
    "    # Reconstruct Gene expression and connectivity arrays based on the factorized modules\n",
    "    ReconGene   = ReconstructingData(M,M['H'],0)\n",
    "    ReconTracer = ReconstructingData(M,M['H'],1)\n",
    "\n",
    "    # Estimate the residuals based on difference between the original and reconstructed arrays  \n",
    "    GeneError   = Data_Modality[0] - ReconGene\n",
    "    TracerError = Data_Modality[1] - ReconTracer\n",
    "    \n",
    "    # Alter the gene expression patterns \n",
    "    Delta_G[gene_list,:] = 1 # Modifying the genes-columns of interest\n",
    "    Data_Modality_new = copy.deepcopy(Data_Modality) \n",
    "    #Data_Modality_new[0] = (1-lamda)*Data_Modality[0] + lamda*Delta_G\n",
    "    Data_Modality_new[0] = np.multiply(Data_Modality[0].T,1-lamda).T + np.multiply(Delta_G.T,lamda).T\n",
    "    \n",
    "    # Predict structural connectivity based on altered gene expression patterns\n",
    "    Data_Modality_new[1]  = Predictingdata(M, Data_Modality_new, GeneError, TracerError)\n",
    "    \n",
    "    # Evaluate differences between the actual and the predicted connectivity patterns\n",
    "    con_effects = np.linalg.norm((Data_Modality_new[1]  - Data_Modality[1]), axis = 1)\n",
    "    \n",
    "    # Reshape the predicted connectivity array into a 4D array for NifTI visualization\n",
    "    new_mat = []\n",
    "    spatial_footprint_pert = {}\n",
    "    if mask_path is not None and savefile is not None:\n",
    "        n_mask = nib.load(os.path.join(mask_path)).get_data() \n",
    "        mask2d = np.reshape(n_mask,[n_mask.shape[0]* n_mask.shape[1]* n_mask.shape[2],1], order='F')\n",
    "        Non_zero_voxels = ~np.all(mask2d == 0, axis=1)\n",
    "        new_con = np.zeros((n_mask.shape[0],n_mask.shape[1],n_mask.shape[2],len(Data_Modality[1])))\n",
    "\n",
    "        for tracer in range(len(Data_Modality_new[1])):\n",
    "            tmp = np.zeros(mask2d.shape)\n",
    "            tmp[Non_zero_voxels] = Data_Modality_new[1] [tracer,:].T\n",
    "            new_con[:,:,:,tracer] =  np.reshape(tmp,(n_mask.shape[0],n_mask.shape[1],n_mask.shape[2],1), order = 'F')[:,:,:,0]\n",
    "        if savefile is not None: PL.Save2Nifti(new_con, outfile = savefile)   \n",
    "        \n",
    "        for i in range(len(M['X'])):\n",
    "            if sel_component is None: continue\n",
    "            tmp     = np.dot(M['X'][i][:,sel_component].T, Data_Modality_new[i]).T\n",
    "            outfile = 'saves/modality_{}_pertubed_{}_comp_{}'.format(i, lamda, sel_component)\n",
    "            spatial_footprint_pert[i] = np.zeros(mask2d.shape)\n",
    "            spatial_footprint_pert[i][Non_zero_voxels] = tmp\n",
    "            spatial_footprint_pert[i] = np.reshape(spatial_footprint_pert[i],\n",
    "                                          (n_mask.shape[0],n_mask.shape[1],n_mask.shape[2],1), order = 'F')\n",
    "            \n",
    "            PL.Save2Nifti(spatial_footprint_pert[i], outfile = outfile) \n",
    "               \n",
    "\n",
    "    return Data_Modality_new,con_effects, spatial_footprint_pert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Genes_To_Brain(keyword):\n",
    "    \n",
    "    filenames = os.listdir('saves/')\n",
    "    filenames = [filename for filename in filenames if '.xlsx' in filename]\n",
    "    # Searching or .xlsx files for parsing cell-type and anatomical genetic markers \n",
    "    \n",
    "    OutDict = OrderedDict() # initialization of the arrays\n",
    "    \n",
    "    if 'Genes_To_Brain_Link.xlsx' in filenames and 'allen_difexp' in keyword:\n",
    "        wb = load_workbook('saves/Genes_To_Brain_Link.xlsx')\n",
    "        OutDict = OrderedDict()\n",
    "        str_list = []\n",
    "        for key in wb.get_sheet_names():\n",
    "            ws = wb[key]\n",
    "            new_key = key.split('_')[0]\n",
    "            #OutDict[new_key] = OrderedDict()\n",
    "            for i,row in enumerate(ws.rows): \n",
    "                if i == 0: \n",
    "                    for j in range(4,len(row)):\n",
    "                        if row[j].value not in str_list:\n",
    "                            str_list.append(row[j].value)       \n",
    "                if i > 0: \n",
    "                    if row[0].value not in OutDict.keys():\n",
    "                        OutDict[row[0].value] = OrderedDict()\n",
    "                    for j in range(3,len(row)):\n",
    "                        OutDict[row[0].value][str_list[j-4]] = row[j].value\n",
    "                        \n",
    "    \n",
    "    if 'Tasic_2015_Supplementary_Markers.xlsx' in filenames and 'tasic_15' in keyword:\n",
    "        wb = xlrd.open_workbook('saves/Tasic_2015_Supplementary_Markers.xlsx')\n",
    "        ws = wb.sheet_by_index(0)\n",
    "        OutDict = {}\n",
    "        for row_id in range(ws.nrows):\n",
    "            row =  ws.row(row_id) # get row elements\n",
    "            if row_id == 0: # do the keys\n",
    "                OutDict = OrderedDict((r.value, []) if r.value != '' else ('Transcriptomic subtype',[]) for r in row)\n",
    "                key_list = OutDict.keys()\n",
    "            else:  # put items in the corresponding keys\n",
    "                for i,r in enumerate(row): OutDict[key_list[i]].append(r.value)\n",
    "                    \n",
    "    if 'Supplementary_Table_9_Cell_types_markers.xlsx' in filenames and 'tasic_19' in keyword:\n",
    "        wb = xlrd.open_workbook('saves/Supplementary_Table_9_Cell_types_markers.xlsx')\n",
    "        ws = wb.sheet_by_index(0)\n",
    "        OutDict = {}\n",
    "        for row_id in range(ws.nrows):\n",
    "            row =  ws.row(row_id) # get row elements\n",
    "            if row_id == 0: # do the keys\n",
    "                OutDict = OrderedDict((r.value, []) for r in row)\n",
    "                key_list = OutDict.keys()\n",
    "            else:  # put items in the corresponding keys\n",
    "                for i,r in enumerate(row): OutDict[key_list[i]].append(r.value)\n",
    "                    \n",
    "        # This part is a bit tricky .. I need to parse the injection coordinates and find one-to-one correspondance\n",
    "        # between cell-types and coordinates\n",
    "        wb = xlrd.open_workbook('saves/Supplementary_Table_6_Injection_Coordinates.xlsx')\n",
    "        ws = wb.sheet_by_index(0)\n",
    "        OutDict2 = {}\n",
    "        for row_id in range(ws.nrows):\n",
    "            row =  ws.row(row_id) # get row elements\n",
    "            if row_id == 0: # do the keys\n",
    "                OutDict2 = OrderedDict((r.value, []) for r in row)\n",
    "                key_list = OutDict2.keys()\n",
    "            else:  # put items in the corresponding keys\n",
    "                for i,r in enumerate(row): OutDict2[key_list[i]].append(r.value)\n",
    "                 \n",
    "            \n",
    "    return OutDict    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "def Genes_To_Brain_v2(structure, basic_areas):\n",
    "    \n",
    "    prim_ids = {structure['id'][idx]:val for idx,val in enumerate(structure['acronym']) for val2 in basic_areas if val == val2}\n",
    "    Gene_Diff = {}\n",
    "    page_limit = 12000\n",
    "    grey = [structure['id'][idx]  for idx,val in enumerate(structure['acronym']) if val == 'grey'][0]\n",
    "    for prim_id,prim_name in prim_ids.items():\n",
    "        print(prim_id,prim_name)\n",
    "        diff_exp_url = 'http://api.brain-map.org/api/v2/data/query.xml?criteria=service::mouse_differential[set$eq{}][structures1$eq{}][structures2$eq{}]'.format('mouse_coronal',prim_id,grey)\n",
    "        start_page = 0\n",
    "        end_page = 2000\n",
    "        while (start_page <= page_limit):\n",
    "            pagedUrl = diff_exp_url + '[start_row$eq{}][num_rows$eq{}]'.format(start_page,end_page)\n",
    "            source = urllib.urlopen(pagedUrl).read()\n",
    "            tree = ET.fromstring(source)\n",
    "            for tmp1,tmp2 in zip(tree.iter('gene-symbol'),tree.iter('fold-change')):\n",
    "                if tmp1.text not in Gene_Diff.keys():\n",
    "                    Gene_Diff[tmp1.text] = OrderedDict((x, 0) for x in prim_ids.values())\n",
    "                Gene_Diff[tmp1.text][prim_name] = tmp2.text   \n",
    "            start_page += end_page\n",
    "            end_page += end_page\n",
    "            \n",
    "    Gene_Diff2 = pd.DataFrame(Gene_Diff,index = Gene_Diff[Gene_Diff.keys()[0]].keys(), \n",
    "                             columns = Gene_Diff.keys(), dtype = np.float32)            \n",
    "    return Gene_Diff2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CGE(GeneExp, area1,area2):  # given the names of two brain areas, this function estimates the average correlated\n",
    "                       # gene expression (CGE) between them.\n",
    "    annotation = nib.load('annotation_200.nii.gz').get_data()\n",
    "    with open('../CCP_Collab_Backup_2/structures.csv','rb') as fp:\n",
    "        structure       = PL.readtable(fp) \n",
    "    \n",
    "    mask_path = 'data/annotation200_mask.nii.gz'\n",
    "    n_mask          = nib.load(os.path.join(mask_path)).get_data() \n",
    "    mask2d          = np.reshape(n_mask,[n_mask.shape[0]*n_mask.shape[1]*n_mask.shape[2],1], order='F')\n",
    "    Non_zero_voxels = ~np.all(mask2d == 0, axis=1)\n",
    "    flat_annot = np.reshape(annotation,(n_mask.shape[0]*n_mask.shape[1]*n_mask.shape[2]), order = 'F')\n",
    "    flat_annot = flat_annot[Non_zero_voxels]\n",
    "    all_ids = np.unique(flat_annot)\n",
    "    all_ids = np.delete(all_ids,0)\n",
    "    \n",
    "    i = 0\n",
    "    struct_idx_dict = OrderedDict()\n",
    "    while True:\n",
    "        all_parent_nodes = GetAncestor(all_ids, level = i, mode = 'id') \n",
    "        if area1 not in all_parent_nodes.values(): \n",
    "            i +=1\n",
    "        else:\n",
    "            break\n",
    "    for key,val in all_parent_nodes.items():\n",
    "        if val not in struct_idx_dict.keys():\n",
    "            struct_idx_dict[val] = []\n",
    "        struct_idx_dict[val].append(key)   \n",
    "     \n",
    "    area1_mask = []\n",
    "    for val in struct_idx_dict[area1]:\n",
    "        area1_mask.extend(np.nonzero(flat_annot == val)[0])\n",
    "    \n",
    "    i = 0\n",
    "    struct_idx_dict = OrderedDict()\n",
    "    while True:\n",
    "        print(i)\n",
    "        all_parent_nodes = GetAncestor(all_ids, level = i, mode = 'id') \n",
    "        if area2 not in all_parent_nodes.values(): \n",
    "            i +=1\n",
    "        else:\n",
    "            break\n",
    "    for key,val in all_parent_nodes.items():\n",
    "        if val not in struct_idx_dict.keys():\n",
    "            struct_idx_dict[val] = []\n",
    "        struct_idx_dict[val].append(key) \n",
    "     \n",
    "    area2_mask = []\n",
    "    for val in struct_idx_dict[area2]:\n",
    "        area2_mask.extend(np.nonzero(flat_annot == val)[0])\n",
    "    \n",
    "    area1_avg_exp = np.mean(GeneExp[:,area1_mask], axis = 1)\n",
    "    area2_avg_exp = np.mean(GeneExp[:,area2_mask], axis = 1)\n",
    "    cge_rho,cge_pval = sci.stats.pearsonr(area1_avg_exp,area2_avg_exp)\n",
    "    \n",
    "    return cge_rho,cge_pval\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RunFLICA( num_components = 200, brain_data_main_folder = \"/home/ntimonidis//knoppers/Nestor/FOR FLICA/\",\n",
    "             output_dir = '/home/ntimonidis/Brain-Connectivity Project/VolumetricAnalysis/data/Outputs_O_param_200/'):\n",
    "    toolbox_path = \"/home/ntimonidis/Brain-Connectivity Project/VolumetricAnalysis/\"\n",
    "    maxits = 3000  # set to 3000 or somethng high, you can check convergence looking at Convergence rate.txt file saved in results.\n",
    "    lambda_dims = 'o' # 'R' or 'o'  # 'o' encodes modality-wise noise, 'R' encodes modality and subject wise (standard 'o')\n",
    "    fsl_path ='/usr/local/fsl'\n",
    "    fs_path ='/Applications/freesurfer'\n",
    "    tol = 0.000001 #tolerance for convergence. \n",
    "    M = flica(brain_data_main_folder, output_dir , num_components, maxits, tol, \\\n",
    "              lambda_dims , fs_path, fsl_path, \"PCA\")\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VolumeLoader(analysis = 'volumetric', infolder = 'data/Outputs_O_param_200/', \n",
    "                 indir = '/home/ntimonidis/knoppers/Nestor/venv/FOR FLICA/',\n",
    "                 twohundred_path = '../25 3 2019/annotation/ccf_2017'\n",
    "                 ):\n",
    "    InputData = []\n",
    "    if analysis == 'unionized':\n",
    "        ExpPerInj    = h5py.File('G_Exp.hdf5', 'r')['dataset1']\n",
    "        ExpPerInj    = ExpPerInj[MesoPred.params['leaf_keys'],:] \n",
    "        ProjPerInj   = MesoPred.ConDict['wild_type']['ConMat'][MesoPred.params['leaf_keys'],:]\n",
    "        ExpPerInj,b,ProjPerInj,d = MesoPred.PreProcessing(ExpPerInj, MesoPred.params, ProjPerInj)\n",
    "        InputData = [ExpPerInj,ProjPerInj]\n",
    "    elif analysis == 'volumetric':\n",
    "        for sub_dir in sorted(os.listdir(indir)):\n",
    "            sub_path = os.path.join(indir,sub_dir)\n",
    "            for subfiles in os.listdir(sub_path):\n",
    "                mod_file = os.path.join(sub_path,subfiles)\n",
    "                InputData.append(nib.load(mod_file).get_data())\n",
    "                print(InputData[len(InputData)-1].shape)\n",
    "        \n",
    "    M = pk.load(open(os.path.join(infolder,'M.p'),'rb')) \n",
    "    M['W'] = np.asarray(M['W'])  \n",
    "    annotation200   = nrrd.read(os.path.join(twohundred_path,'annotation_200.nrrd'))\n",
    "    average_template_200 = nrrd.read(os.path.join(twohundred_path,'average_template_200.nrrd'))\n",
    "    shape_of_200       = annotation200[0].shape\n",
    "    annot_200_res      = np.reshape(annotation200[0],\n",
    "                                       (shape_of_200[0]*shape_of_200[1]*shape_of_200[2]),order='F')\n",
    "    template_200_res   = np.reshape(average_template_200[0],\n",
    "                                       (shape_of_200[0]*shape_of_200[1]*shape_of_200[2]),order='F')\n",
    "    \n",
    "    \n",
    "    return M, InputData, annotation200, annot_200_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DocumentThresholdMaps(comp, folder2check, annotation200, low = 1, high = 99, category_list = None,\n",
    "                         script = './colorcode.sh', mask_list = None, minimum = 30, mode = 'counts'):\n",
    "    \n",
    "    turn = 0\n",
    "    count_dict = OrderedDict()\n",
    "    ccode_dict_again = OrderedDict()\n",
    "    allMasks = []\n",
    "    count_array = np.unique(annotation200[0], return_counts = True)\n",
    "    all_ids = count_array[0]\n",
    "    annot_shape = annotation200[0].shape\n",
    "    annot2d     = (annot_shape[0]*annot_shape[1]*annot_shape[2],1)\n",
    "    flat_annot = np.ndarray.flatten(annotation200[0],order = 'F')\n",
    "    all_ids = np.delete(all_ids,0)\n",
    "    all_parent_nodes = GetAncestor(all_ids, level = 1, mode = 'id')  \n",
    "    call([script,str(low),str(high),folder2check,str(comp)])\n",
    "    for mask in os.listdir(os.path.join(folder2check,'Colorcoding_experiments/')):\n",
    "        mask_key = mask.split('.nii')[0]\n",
    "        if mask_list is not None and mask_key not in mask_list: continue\n",
    "        allMasks.append(nib.load(os.path.join(folder2check,'Colorcoding_experiments',mask)).get_data())\n",
    "        ccode_dict_again[mask_key] = OrderedDict()\n",
    "    \n",
    "    for category in ccode_dict_again.keys():\n",
    "        for node in all_parent_nodes.values():\n",
    "            ccode_dict_again[category][node] = 0\n",
    "\n",
    "    for annot_id in range(len(count_array[0])):\n",
    "        count_dict[count_array[0][annot_id]] = count_array[1][annot_id]\n",
    "    for category in ccode_dict_again.keys():\n",
    "        blue_counts = np.asarray(np.unique(annotation200[0][np.nonzero(allMasks[turn])],\n",
    "                                               return_counts = True))\n",
    "        primary_src_acros = {key:all_parent_nodes[key] for key in blue_counts[0,:]\\\n",
    "                                 if key in all_parent_nodes.keys()}\n",
    "\n",
    "        turn+=1\n",
    "        for structure in np.unique(primary_src_acros.values()):\n",
    "            related_id_freq = np.asarray([(blue_counts[1, blue_counts[0,:] == key][0],\\\n",
    "                                            len(np.nonzero(annotation200[0] == key)[0]))\\\n",
    "                                            for key,value in primary_src_acros.items() \n",
    "                                            if value == structure])\n",
    "            ccode_dict_again[category][structure] = np.sum(related_id_freq[:,0])\n",
    "            if mode == 'freq':\n",
    "                ccode_dict_again[category][structure] = ccode_dict_again[category][structure]/(1.0*np.sum(related_id_freq[:,1]))\n",
    "\n",
    "    ccode_dict_again['label'] = OrderedDict()\n",
    "    if category_list == None: category_list = [key for key in ccode_dict_again.keys() if key !='label']\n",
    "    for structure in all_parent_nodes.values():\n",
    "        local_vals = [ccode_dict_again[category][structure] for category in category_list]\n",
    "        max_id = np.argmax(local_vals)\n",
    "        if np.max(local_vals) > 0: # Proxy built to make sure that labels are assigned based on non-zero values, zero-valued cases shall be labeled as 'other'\n",
    "            ccode_dict_again['label'][structure] = ccode_dict_again.keys()[max_id]\n",
    "        else:    \n",
    "            ccode_dict_again['label'][structure] = 'other'\n",
    "\n",
    "    return ccode_dict_again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ColorcodingRoutine(InputData,annotation200, thr = 10):\n",
    "    from subprocess import call\n",
    "    LoadDict = OrderedDict([('data/Outputs_O_param_source_vis_all/',[0,2,4,5,7,8]),\\\n",
    "                ('data/Outputs_O_param_200_source_cp/',[0,3,5]),\\\n",
    "                ('data/Outputs_O_param_200_source_mrn/', [0,4]),\n",
    "                ('data/Outputs_O_param_200/', [0,1,2,5,10,23])]) \n",
    "     \n",
    "    total_ccode_dict = OrderedDict()\n",
    "    Tasic_ovp_dict = OrderedDict()\n",
    "    mix_dict_test = OrderedDict()\n",
    "    tractography_dict = OrderedDict()\n",
    "    mix_dict = OrderedDict()\n",
    "    \n",
    "    Tasic_ovp_df = []\n",
    "    Tasic_19_dict = Genes_To_Brain('tasic_19')\n",
    "    \n",
    "    all_ids = np.unique(annotation200[0])\n",
    "    annot_shape = annotation200[0].shape\n",
    "    annot2d     = (annot_shape[0]*annot_shape[1]*annot_shape[2],1)\n",
    "    flat_annot = np.ndarray.flatten(annotation200[0],order = 'F')\n",
    "    all_ids = np.delete(all_ids,0)\n",
    "    all_parent_nodes = GetAncestor(all_ids, level = 1, mode = 'id')  \n",
    "    affine_res  = 0.2*np.eye(4); affine_res[3,3] = 1.0\n",
    "    up = 99; down = 1\n",
    "    high_perc = 100 - (100 - 97.5)/2; low_perc = (100 - 97.5)/2\n",
    "\n",
    "    for folder2check, comps_to_check in LoadDict.items():\n",
    "        M_loc = pk.load(open(os.path.join(folder2check,'M.p'),'rb'))\n",
    "        M_loc['W'] = np.asarray(M_loc['W'])\n",
    "        mixed_map = nib.load(os.path.join(folder2check,'niftiOut_mi1.nii.gz')).get_data()\n",
    "        if folder2check != 'data/Outputs_O_param_200/':                        \n",
    "            injection_group = folder2check.split('source_')[1].split('/')[0]\n",
    "            new_mask = pk.load(open('saves/masks_4_wild_type_{}.pkl'.format(injection_group),'rb'))\n",
    "        else:\n",
    "            injection_group = 'global'\n",
    "            new_mask = None\n",
    "\n",
    "        tmp_list_tmp = []\n",
    "        for comp in comps_to_check:\n",
    "            if folder2check != 'data/Outputs_O_param_200/':                        \n",
    "                injection_group = folder2check.split('source_')[1].split('/')[0]\n",
    "            else:\n",
    "                injection_group = 'global'            \n",
    "            final_key = injection_group + '_' + str(comp)\n",
    "            final_key = final_key.replace('vis_all','Visual group').replace('cp','Caudoputamen group\\\n",
    "            ').replace('mrn', 'Midbrain group').replace('_',' ').replace('global', 'Global')\n",
    "            Tasic_ovp_dict[final_key] = OrderedDict([('Gene Markers',[]),('Pathway',[]),('ICA',[])])\n",
    "            \n",
    "            print(final_key)\n",
    "            call(['./colorcode.sh','1','99',folder2check,str(comp)])\n",
    "            GreenMask = nib.load(os.path.join(folder2check,'Colorcoding_experiments/con_exp_mix_join.nii.gz')).get_data()\n",
    "            RedMask = nib.load(os.path.join(folder2check,'Colorcoding_experiments/con_dis.nii.gz')).get_data()\n",
    "            BlueMask = nib.load(os.path.join(folder2check,'Colorcoding_experiments/exp_dis.nii.gz')).get_data()\n",
    "            MixMapMask = nib.load(os.path.join(folder2check,'Colorcoding_experiments/mixed_maps_thr.nii.gz')).get_data()\n",
    "            YellowMask = nib.load(os.path.join(folder2check,'Colorcoding_experiments/mixed_maps_bin.nii.gz')).get_data()\n",
    "            TractMapMask = nib.load(os.path.join(folder2check,'Colorcoding_experiments/sum_con_thr.nii.gz')).get_data()\n",
    "            allMasks = [GreenMask,BlueMask,RedMask,MixMapMask,TractMapMask]#, YellowMask]\n",
    "            \n",
    "            ccode_dict_again = OrderedDict()\n",
    "            ccode_dict_again['green dict'] = OrderedDict(); ccode_dict_again['blue dict'] = OrderedDict()\n",
    "            ccode_dict_again['red dict'] = OrderedDict()\n",
    "            ccode_dict_again['sum dict'] = OrderedDict()\n",
    "            ccode_dict_again['tract dict'] = OrderedDict()\n",
    "            #ccode_dict_again['yellow dict'] = OrderedDict()\n",
    "            for category in ccode_dict_again.keys():\n",
    "                for node in all_parent_nodes.values():\n",
    "                    ccode_dict_again[category][node] = 0\n",
    "            \n",
    "            turn = 0\n",
    "            count_dict = OrderedDict()\n",
    "            count_array = np.unique(annotation200[0],return_counts = True)\n",
    "            for annot_id in range(len(count_array[0])):\n",
    "                count_dict[count_array[0][annot_id]] = count_array[1][annot_id]\n",
    "                \n",
    "            for category in ccode_dict_again.keys():\n",
    "                blue_counts = np.asarray(np.unique(annotation200[0][np.nonzero(allMasks[turn])],\n",
    "                                                       return_counts = True))\n",
    "                primary_src_acros = {key:all_parent_nodes[key] for key in blue_counts[0,:]\\\n",
    "                                         if key in all_parent_nodes.keys()}\n",
    "                    \n",
    "                turn+=1\n",
    "                for structure in np.unique(primary_src_acros.values()):\n",
    "\n",
    "                    related_id_freq = np.asarray([(blue_counts[1, blue_counts[0,:] == key][0],\\\n",
    "                                                       len(np.nonzero(annotation200[0] == key)[0]))\\\n",
    "                                                      for key,value in primary_src_acros.items() \n",
    "                                                      if value == structure])\n",
    "                    ccode_dict_again[category][structure] = np.sum(related_id_freq[:,0])\n",
    "               \n",
    "\n",
    "                        \n",
    "            ccode_dict_again['label'] = OrderedDict()\n",
    "            for structure in all_parent_nodes.values():\n",
    "                local_vals = [ccode_dict_again[category][structure] for category in \\\n",
    "                              ['green dict', 'blue dict','red dict']]\n",
    "                max_id = np.argmax(local_vals)\n",
    "                if np.max(local_vals) > 0:     # Proxy built to make sure that labels are assigned based on non-zero values, zero-valued cases shall be labeled as 'other'\n",
    "                    ccode_dict_again['label'][structure] = ccode_dict_again.keys()[max_id]\n",
    "                else:    \n",
    "                    ccode_dict_again['label'][structure] = 'other'\n",
    "            \n",
    "            total_ccode_dict[final_key] = pd.DataFrame.from_dict(ccode_dict_again).sort_values(by = 'green dict', \\\n",
    "                                                                                               ascending = False)\n",
    "            tractography_dict[final_key] = [key for key,value in ccode_dict_again['tract dict'].items() if value > thr]\n",
    "            mix_dict[final_key] = [key for key,value in ccode_dict_again['sum dict'].items() if value > thr] \n",
    "            tractography_dict[final_key] = np.unique(pd.DataFrame.from_dict(tractography_dict[final_key]).sort_index().values)\n",
    "            mix_dict[final_key] = np.unique(pd.DataFrame.from_dict(mix_dict[final_key]).sort_index().values)\n",
    "            mix_dict_test[final_key] = ccode_dict_again['sum dict'] \n",
    "            \n",
    "            sel_gene_footprint = M_loc['X'][0][:,comp]\n",
    "            gene_high_thr = np.percentile(sel_gene_footprint, high_perc); \n",
    "            gene_low_thr = np.percentile(sel_gene_footprint, low_perc)\n",
    "            gene_grp = [idx for idx,val in enumerate(sel_gene_footprint) \n",
    "                            if val >= gene_high_thr or val <= gene_low_thr]\n",
    "            gene_acro = [MesoPred.params['Gene Acronyms Original'][val] for val in gene_grp \n",
    "                             if MesoPred.params['Gene Acronyms Original'][val] is not None]\n",
    "            unique_set = np.unique(gene_acro)\n",
    "            \n",
    "            for idx,cluster in enumerate(Tasic_19_dict['cluster_label']):\n",
    "                gene_list2 = Tasic_19_dict['markers_global_specific'][idx].split(',') + \\\n",
    "                Tasic_19_dict['markers_subclass_specific'][idx].split(',') + \\\n",
    "                Tasic_19_dict['markers_node_specific'][idx].split(',')\n",
    "                gene_list2_ovrlp = [gene for gene in gene_list2 if gene in unique_set] \n",
    "                if len(gene_list2_ovrlp) > 0:\n",
    "                    pathway = Tasic_19_dict['class_label'][idx]\n",
    "                    for g_sub in gene_list2_ovrlp:\n",
    "                        if g_sub not in Tasic_ovp_dict[final_key]['Gene Markers']:\n",
    "                            Tasic_ovp_dict[final_key]['Gene Markers'].extend([g_sub])\n",
    "                            Tasic_ovp_dict[final_key]['Pathway'].extend([pathway])\n",
    "                            Tasic_ovp_dict[final_key]['ICA'].extend([key])\n",
    "\n",
    "            Tasic_ovp_df.append(pd.DataFrame.from_dict(Tasic_ovp_dict[final_key]))\n",
    "    \n",
    "    SummaryDict = OrderedDict()\n",
    "    for inj_group in total_ccode_dict.keys():\n",
    "        sub_group = [] \n",
    "        for label in total_ccode_dict[inj_group].keys():\n",
    "            if label == 'label' or label == 'yellow dict': continue\n",
    "            key = inj_group + ' ' + label\n",
    "            key = key.replace('blue dict',\\\n",
    "                              'gene expression driven').replace('red dict',\\\n",
    "                                                                'projection density driven').replace('green dict', 'bimodal')\n",
    "            SummaryDict[key] = total_ccode_dict[inj_group].loc[total_ccode_dict[inj_group]['label'] == label].loc\\\n",
    "            [total_ccode_dict[inj_group][label] > thr].sort_index().index\n",
    "    \n",
    "    MixDict_test_df = pd.DataFrame(mix_dict_test,index = mix_dict_test[list(mix_dict_test.keys())[0]].keys(),\\\n",
    "                                              columns= mix_dict_test.keys()).T\n",
    "    \n",
    "    return SummaryDict, Tasic_ovp_df, mix_dict, tractography_dict, MixDict_test_df, total_ccode_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BuildTargetMask(target_name = 'Primary visual area'):\n",
    "    mask_path = 'data/annotation200_mask.nii.gz'\n",
    "    n_mask = nib.load(os.path.join(mask_path)).get_data() \n",
    "    mask2d = np.reshape(n_mask,[n_mask.shape[0]*n_mask.shape[1]*n_mask.shape[2],1], order='F')\n",
    "    Non_zero_voxels = ~np.all(mask2d == 0, axis = 1)\n",
    "    annotation200 = nrrd.read('../25 3 2019/annotation/ccf_2017/annotation_200.nrrd')\n",
    "    flat_200 = np.ndarray.flatten(annotation200[0], order = 'F')\n",
    "    flat_200 = flat_200[Non_zero_voxels]\n",
    "    all_ids = np.unique(flat_200)\n",
    "    all_ids = np.delete(all_ids,0)\n",
    "    all_parent_nodes = GetAncestor(all_ids, level = 1, mode = 'id') \n",
    "    ids_of_int = [key for key,value in all_parent_nodes.items() if value == target_name]\n",
    "    vox_locs = np.unique([val2 for val in ids_of_int for val2 in np.where(flat_200 == val)[0]])\n",
    "    return vox_locs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BuildInjectionSubsets(Data_Modality, sources, group_num = 3, sources2 = None, \\\n",
    "                          Data_Modality2 = None, sources3 = None, Data_Modality3 = None,\n",
    "                          filename = None, annotation200 = None, target = 'motor',\n",
    "                          cell_specificity = None): \n",
    "    # sources 2 is the latest addition in case you want to mark brain areas for a second modality\n",
    "    # i.e. DTI or morphologies\n",
    "    # latest addition: cell-specificity for projections originating from specific cell classes\n",
    "    ## Isolate injections heavily clustered in one ROI, build a mask, do local flica on them\n",
    "    mask_path        = 'data/annotation200_mask.nii.gz'\n",
    "    n_mask           = nib.load(os.path.join(mask_path)).get_data() \n",
    "    mask2d           = np.reshape(n_mask,[n_mask.shape[0]*n_mask.shape[1]*n_mask.shape[2],1], order='F')\n",
    "    Non_zero_voxels  = ~np.all(mask2d == 0, axis=1)\n",
    "    annot_nzero      = []\n",
    "    if annotation200 is not None:\n",
    "        flat_annot       = np.ndarray.flatten(annotation200[0],order = 'F') \n",
    "        annot_nzero      = flat_annot[Non_zero_voxels]\n",
    "    if cell_specificity is not None:\n",
    "        NeuronCT         = pk.load(open('../Mouselight/saves/Neuron_CT.pkl','rb'))\n",
    "    new_masks = OrderedDict()\n",
    "\n",
    "    unique_srcs = np.unique(sources)\n",
    "    cls_membs = [] \n",
    "    for src in unique_srcs:\n",
    "        cls_membs.append([idx for idx,val in enumerate(sources) if val == src])\n",
    "      \n",
    "    # get the most frequent source \n",
    "    inj_freq = [len(val) for val in cls_membs]\n",
    "    top_cls = np.argsort(inj_freq)[::-1]\n",
    "    inj_grps = OrderedDict()\n",
    "    inj_grps['vis_all'] = np.unique([idx for idx,val in enumerate(unique_srcs) for val2 in cls_membs[idx] \\\n",
    "                      if 'VIS' in val and val !='VISC'])\n",
    "    inj_grps['MO_all'] = np.unique([idx for idx,val in enumerate(unique_srcs) for val2 in cls_membs[idx] \\\n",
    "                      if 'MOp' in val or 'MOs' in val])\n",
    "     \n",
    "    for selection in range(1,group_num): \n",
    "        inj_grps[unique_srcs[top_cls[selection]]] = [top_cls[selection]]\n",
    "    \n",
    "    # make a mask based on it \n",
    "    for inj_name,inj_grp in inj_grps.items():\n",
    "        new_masks[inj_name] = {}\n",
    "        src_mask = np.zeros(np.shape(sources))\n",
    "        if inj_name == 'vis_all':\n",
    "            new_masks[inj_name]['source_name'] = inj_name\n",
    "            for val in inj_grp:\n",
    "                src_mask[cls_membs[val]] = 1\n",
    "            if sources2 is not None:\n",
    "                new_masks[inj_name]['source_mask_II'] = [neuron_id for neuron_id,neuron_name in enumerate(sources2) \\\n",
    "                                               if 'VIS' in neuron_name and neuron_name !='VISC']\n",
    "            if sources3 is not None:\n",
    "                new_masks[inj_name]['source_mask_III'] = [neuron_id for neuron_id,neuron_name in enumerate(sources3) \\\n",
    "                                               if 'VIS' in neuron_name and neuron_name !='VISC']\n",
    "        elif inj_name == 'MO_all':\n",
    "            new_masks[inj_name]['source_name'] = inj_name\n",
    "            for val in inj_grp:\n",
    "                src_mask[cls_membs[val]] = 1\n",
    "            if sources2 is not None:\n",
    "                if cell_specificity is not None:\n",
    "                    new_masks[inj_name]['source_mask_II'] = [neuron_id for neuron_id,neuron_name in \\\n",
    "                                                             enumerate(sources2) if 'MOp' in neuron_name \\\n",
    "                                                             or 'MOs' in neuron_name and neuron_id in NeuronCT.keys() \\\n",
    "                                                             and NeuronCT[neuron_id] == cell_specificity]     \n",
    "                else:\n",
    "                    new_masks[inj_name]['source_mask_II'] = [neuron_id for neuron_id,neuron_name in \\\n",
    "                                                             enumerate(sources2) \\\n",
    "                                               if 'MOp' in neuron_name or 'MOs' in neuron_name] \n",
    "            if sources3 is not None:\n",
    "                new_masks[inj_name]['source_mask_III'] = [neuron_id for neuron_id,neuron_name in \\\n",
    "                                                              enumerate(sources3) if 'MOp' in neuron_name \\\n",
    "                                                              or 'MOs' in neuron_name]     \n",
    "        else: \n",
    "            new_masks[inj_name]['source_name'] = unique_srcs[inj_grp]\n",
    "            src_mask[cls_membs[inj_grp[0]]] = 1  \n",
    "            if sources2 is not None:\n",
    "                new_masks[inj_name]['source_mask_II'] = [neuron_id for neuron_id,neuron_name in enumerate(sources2) \\\n",
    "                                               if neuron_name == inj_name]\n",
    "            if sources3 is not None:\n",
    "                new_masks[inj_name]['source_mask_III'] = [neuron_id for neuron_id,neuron_name in enumerate(sources3) \\\n",
    "                                               if neuron_name == inj_name]    \n",
    "                \n",
    "        new_masks[inj_name]['source_mask'] = np.nonzero(src_mask)[0]\n",
    "        \n",
    "        # Find a common ground for those injections\n",
    "        tmp_mat = Data_Modality[new_masks[inj_name]['source_mask'],:]\n",
    "        BinMat  = np.zeros(np.shape(tmp_mat))\n",
    "        min_el  = np.min(BinMat)\n",
    "        BinMat[tmp_mat != min_el] = 1\n",
    "        common_vox = np.asarray(np.prod(BinMat, axis = 0))\n",
    "        new_masks[inj_name]['vox_mask'] = np.nonzero(common_vox)[0]\n",
    "        \n",
    "        if annotation200 is not None:\n",
    "            tmp_mat2 = tmp_mat + np.min(tmp_mat) + 1e-10\n",
    "            log_proj = np.log10(tmp_mat2)\n",
    "            npv_mask = []\n",
    "            for proj in range(len(log_proj[0])):\n",
    "                #low_iqr = np.median(log_proj[:,proj]) - 1.5*sci.stats.iqr(log_proj[:,proj])\n",
    "                low_iqr = np.median(log_proj[:,proj])\n",
    "                if low_iqr > -1.5:\n",
    "                    npv_mask.append(proj)\n",
    "\n",
    "            annot_nzero_mask = annot_nzero[npv_mask]\n",
    "            all_ids          = np.unique(annot_nzero_mask)\n",
    "            if len(annot_nzero_mask) > 0:\n",
    "                all_parent_nodes = GetAncestor(all_ids, level = 1, mode = 'id')\n",
    "                new_masks[inj_name]['target_mask'] = np.unique(all_parent_nodes.values())\n",
    "\n",
    "        if Data_Modality2 is not None and 'source_mask_II' in new_masks[inj_name].keys() and \\\n",
    "        len(new_masks[inj_name]['source_mask_II']) > 0:\n",
    "            tmp_mat = Data_Modality2[new_masks[inj_name]['source_mask_II'],:]\n",
    "            BinMat  = np.zeros(np.shape(tmp_mat))\n",
    "            min_el  = np.min(BinMat)\n",
    "            BinMat[tmp_mat != min_el] = 1\n",
    "            common_vox2 = np.asarray(np.sum(BinMat, axis = 0))\n",
    "            new_masks[inj_name]['vox_mask'] = np.nonzero(common_vox2)[0]\n",
    "            #new_masks[inj_name]['vox_mask'] = np.unique(np.concatenate((new_masks[inj_name]['vox_mask'], \\\n",
    "            #                                                            np.nonzero(common_vox2)[0])))\n",
    "            if annotation200 is not None:\n",
    "                annot_nzero_mask = annot_nzero[np.nonzero(common_vox2)[0]]\n",
    "                all_ids          = np.unique(annot_nzero_mask)\n",
    "                all_parent_nodes = GetAncestor(all_ids, level = 1, mode = 'id')\n",
    "                new_masks[inj_name]['target_mask_2'] = np.unique(all_parent_nodes.values())\n",
    "            \n",
    "            \n",
    "        if Data_Modality3 is not None and 'source_mask_III' in new_masks[inj_name].keys() and \\\n",
    "        len(new_masks[inj_name]['source_mask_III']) > 0:\n",
    "            tmp_mat = Data_Modality3[new_masks[inj_name]['source_mask_III'],:]\n",
    "            BinMat  = np.zeros(np.shape(tmp_mat))\n",
    "            min_el  = np.min(BinMat)\n",
    "            BinMat[tmp_mat != min_el] = 1\n",
    "            common_vox3 = np.asarray(np.prod(BinMat, axis = 0))\n",
    "            #new_masks[inj_name]['vox_mask'] = np.nonzero(common_vox2)[0]\n",
    "            new_masks[inj_name]['vox_mask'] = np.unique(np.concatenate((new_masks[inj_name]['vox_mask'], \\\n",
    "                                                                        np.nonzero(common_vox3)[0])))  \n",
    "        \n",
    "        print inj_name, len(new_masks[inj_name]['source_mask']), len(new_masks[inj_name]['vox_mask'])\n",
    "                \n",
    "        # Here I summon the allen annotation of 200, I reduce structures to parental ones and get their \n",
    "        # unique set, which will be stored as a key\n",
    "        # I shall repeat this for gene expression        \n",
    "        if 'source_mask_II' in new_masks[inj_name].keys():   \n",
    "            print len(new_masks[inj_name]['source_mask_II'])\n",
    "        if 'source_mask_III' in new_masks[inj_name].keys():\n",
    "            print len(new_masks[inj_name]['source_mask_III'])\n",
    "            \n",
    "            \n",
    "        if filename is not None:    \n",
    "            if cell_specificity is not None:\n",
    "                pk.dump(new_masks[inj_name], open(os.path.join('saves', filename + '_{}.pkl'.\\\n",
    "                                                               format(inj_name + ' ' + cell_specificity)),'wb'))\n",
    "            else:    \n",
    "                pk.dump(new_masks[inj_name], open(os.path.join('saves', filename + '_{}.pkl'.format(inj_name)),'wb'))\n",
    "\n",
    "    return new_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PairwiseVoxelCorrelation(comp_set, lica_file = 'data/Outputs_O_param_200', dict_file = 'temp_saves/'):\n",
    "    \n",
    "    import flica_various as alb_various\n",
    "    G_code = pk.load(open(os.path.join(dict_file, 'Atoms_4_GeneVol.pkl'),'rb')).T\n",
    "    P_code = pk.load(open(os.path.join(dict_file, 'Atoms_4_ProjVol.pkl'),'rb')).T\n",
    "    G_atom = pk.load(open(os.path.join(dict_file, 'Code_4_GeneVol.pkl'),'rb')).T\n",
    "    P_atom = pk.load(open(os.path.join(dict_file, 'Code_4_ProjVol.pkl'),'rb')).T\n",
    "    CC_atom = pk.load(open(os.path.join(dict_file, 'Atoms_4_GeneVol_conc.pkl'),'rb'))\n",
    "    CC_code = pk.load(open(os.path.join(dict_file, 'Code_4_GeneVol_conc.pkl'),'rb'))\n",
    "\n",
    "    LoadDict = OrderedDict([(lica_file,comp_set)]) \n",
    "    rounding_to_avoid = ['Source to LICA p','Spatial map p', 'Gene to LICA p']\n",
    "\n",
    "    InputData = [G_code,P_code]\n",
    "    dictsize = len(G_atom)\n",
    "    Dict2Dict_dict = OrderedDict()\n",
    "    Dict2Loc_dict = OrderedDict()\n",
    "    Dict2Dict_dict = OrderedDict()\n",
    "    LICA_2_dict = OrderedDict()\n",
    "    LICA_2_dict2 = OrderedDict()\n",
    "    LICA_2_dict = np.zeros((dictsize,dictsize))\n",
    "    Dict2Dict = np.zeros((dictsize,dictsize))\n",
    "    Dict2Dict2 = np.zeros((dictsize,dictsize))\n",
    "    Dict2Loc = np.zeros((dictsize,dictsize))\n",
    "    Dict2Loc2 = np.zeros((dictsize,dictsize))\n",
    "\n",
    "    M = pk.load(open(os.path.join(lica_file,'M.p'),'rb')) \n",
    "    injection_group = 'global'\n",
    "    M['W'] = np.asarray(M['W'])\n",
    "    G2_code = CC_code[0:3318,:]; P2_code = CC_code[3318:len(CC_code),:] \n",
    "    pc=np.divide(M['H_PCs'][0:-1][:],np.tile(np.sum(M['H_PCs'][0:-1][:],0),[M['H_PCs'].shape[0]-1, 1] ))\n",
    "    pc = np.asarray(pc)\n",
    "    comps_of_int = [0,1,2,4,5,10,23]\n",
    "    compsize = len(comps_of_int)\n",
    "    top_num = 0\n",
    "\n",
    "    Dict_Modality_sc = {}\n",
    "    k = 0\n",
    "    for mod in InputData:\n",
    "        Dict_Modality_sc[k] = InputData[k] - (np.matrix(np.mean(InputData[k],1)).T)       \n",
    "        scaling_data_transform = alb_various.rms(Dict_Modality_sc[k],[],[])       \n",
    "        Dict_Modality_sc[k] = np.divide(Dict_Modality_sc[k],scaling_data_transform)\n",
    "        Dict_Modality_sc[k] = np.asarray(Dict_Modality_sc[k])\n",
    "        k+=1\n",
    "\n",
    "    for dicti in range(dictsize):\n",
    "        for l_i in comps_of_int:\n",
    "            Dict2Loc[l_i,dicti] = sci.stats.pearsonr(np.abs(M['H'][l_i,:]),\\\n",
    "                                                     np.abs(Dict_Modality_sc[0][:,dicti]))[0]\n",
    "            Dict2Loc2[l_i,dicti] = sci.stats.pearsonr(np.abs(M['H'][l_i,:]),\\\n",
    "                                                      np.abs(Dict_Modality_sc[1][:,dicti]))[0]\n",
    "        for dictj in range(dictsize):\n",
    "            Dict2Dict[dicti,dictj] = sci.stats.pearsonr(np.abs(Dict_Modality_sc[0][:,dicti]),\\\n",
    "                                                         np.abs(Dict_Modality_sc[1][:,dictj]))[0]\n",
    "            LICA_2_dict[dicti,dictj] = sci.stats.pearsonr(np.abs(M['H'][dicti,:]),\\\n",
    "                                                         np.abs(CC_atom[dictj,:]))[0]    \n",
    "\n",
    "    for comp in comps_of_int:\n",
    "        print(comp)\n",
    "        top_num_to_pick = 0\n",
    "        new_key = '{} ICA {}'.format(injection_group, comp)\n",
    "        #max_glob_id = np.argmax(Dict2Loc[comp,:])\n",
    "        max_glob_id = np.argsort(Dict2Loc[comp,:])[::-1][top_num_to_pick]\n",
    "        #max_glob_val = np.max(Dict2Loc[comp,:])\n",
    "        max_glob_val = np.sort(Dict2Loc[comp,:])[::-1][top_num_to_pick]\n",
    "        max_glob_p   = sci.stats.pearsonr(np.abs(M['H'][comp,:]),\\\n",
    "                                          np.abs(Dict_Modality_sc[0][:,max_glob_id]))[1]\n",
    "\n",
    "        max_glob_id2 = np.argsort(Dict2Loc2[comp,:])[::-1][top_num_to_pick]\n",
    "        max_glob_val2 = np.sort(Dict2Loc2[comp,:])[::-1][top_num_to_pick]\n",
    "        max_glob_p2   = sci.stats.pearsonr(np.abs(M['H'][comp,:]),\\\n",
    "                                           np.abs(Dict_Modality_sc[1][:,max_glob_id2]))[1]\n",
    "\n",
    "        gene_coef_cor = sci.stats.pearsonr(np.abs(M['X'][0][:,comp]),\\\n",
    "                                           np.abs(G_atom[max_glob_id,:]))\n",
    "        source_coef_cor = sci.stats.pearsonr(np.abs(M['X'][1][:,comp]),\\\n",
    "                                             np.abs(P_atom[max_glob_id2,:]))\n",
    "\n",
    "        interdict_cor = sci.stats.pearsonr(np.abs(Dict_Modality_sc[0][:,max_glob_id]),\\\n",
    "                                           np.abs(Dict_Modality_sc[1][:,max_glob_id2]))\n",
    "\n",
    "        Dict2Loc_dict[new_key] = OrderedDict([('Gene Dictionary', max_glob_id),\\\n",
    "                                              ('A1 rho', max_glob_val),\\\n",
    "                                              ('A1 p', max_glob_p) ,\\\n",
    "                                              ('A2 rho', gene_coef_cor[0]),\\\n",
    "                                              ('A2 p', gene_coef_cor[1]),\n",
    "                                              ('Projection Dictionary', max_glob_id2),\\\n",
    "                                              ('B1 rho', max_glob_val2), \\\n",
    "                                              ('B1 p', max_glob_p2),\\\n",
    "                                              ('B2 rho', source_coef_cor[0]),\\\n",
    "                                              ('B2 p', source_coef_cor[1]),\\\n",
    "                                              ('interdict rho', interdict_cor[0]), \\\n",
    "                                              ('interdict p', interdict_cor[1])])\n",
    "        for key2 in Dict2Loc_dict[new_key].keys():\n",
    "            if key2 in rounding_to_avoid:\n",
    "                continue # for now\n",
    "            else:\n",
    "                Dict2Loc_dict[new_key][key2] = np.around(Dict2Loc_dict[new_key][key2],decimals = 3)\n",
    "\n",
    "        LICA_2_dict2[comp] = OrderedDict()\n",
    "        top_conc_dict = np.argsort(LICA_2_dict[comp,:])[::-1][top_num]\n",
    "        LICA_2_dict2[comp]['Concat Dictionary'] = top_conc_dict\n",
    "        LICA_2_dict2[comp]['Pearson rho']  = np.sort(LICA_2_dict[comp,:])[::-1][top_num]\n",
    "        LICA_2_dict2[comp]['Pearson p']    = sci.stats.pearsonr(np.abs(M['H'][comp,:]),np.abs(CC_atom[top_conc_dict,:]))[1]\n",
    "        LICA_2_dict2[comp]['C']     = sci.stats.pearsonr(np.abs(M['X'][0][:,comp]),np.abs(G2_code[:,top_conc_dict]))[0]\n",
    "        LICA_2_dict2[comp]['D']   = sci.stats.pearsonr(np.abs(M['X'][1][:,comp]),np.abs(P2_code[:,top_conc_dict]))[0]\n",
    "        LICA_2_dict2[comp]['E']  = pc[0][comp] \n",
    "        \n",
    "        for tmp_key in LICA_2_dict2[comp].keys():\n",
    "            LICA_2_dict2[comp][tmp_key] = np.around(LICA_2_dict2[comp][tmp_key],decimals = 3)\n",
    "\n",
    "    Dict2Loc_df = pd.DataFrame(Dict2Loc_dict,index = Dict2Loc_dict[Dict2Loc_dict.keys()[0]].keys(),\\\n",
    "                               columns = Dict2Loc_dict.keys()).T\n",
    "    Dict2Loc_df = Dict2Loc_df.astype({'Gene Dictionary': 'int32'}, copy = False)\n",
    "    Dict2Loc_df = Dict2Loc_df.astype({'Projection Dictionary': 'int32'}, copy = False)\n",
    "\n",
    "    LICA_2_dict_df = pd.DataFrame(LICA_2_dict2,index = LICA_2_dict2[LICA_2_dict2.keys()[0]].keys(),\\\n",
    "                                   columns = LICA_2_dict2.keys()).T  \n",
    "    LICA_2_dict_df = LICA_2_dict_df.astype({'Concat Dictionary': 'int32'}, copy = False)\n",
    "    return Dict2Loc_df, LICA_2_dict_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New additions 25-05-2020: update Component summary by including \n",
    "#                          a) spectral clustering of H and then nmi with the allen annotation\n",
    "#                          b) pearson correlation with the average template volume\n",
    "#a)\n",
    "from sklearn.cluster import spectral_clustering, SpectralClustering\n",
    "from sklearn.metrics import normalized_mutual_info_score as nmi    \n",
    "\n",
    "def PostHocLICA(annotation200, Non_zero_voxels, infile = 'data/Outputs_O_param_source_vis_all/M.p',\n",
    "                maskfile = 'saves/masks_4_wild_type_vis_all.pkl',\n",
    "                tempfile = 'average_template_200.nii.gz',\n",
    "                comp_filter = None):\n",
    "\n",
    "    M = pk.load(open(infile,'rb'))\n",
    "    new_masks = pk.load(open(maskfile,'rb'))\n",
    "    avg_template = nib.load(tempfile).get_data()\n",
    "    lica2gray = np.zeros((len(M['H']),2))\n",
    "    flat_avg_temp = np.ndarray.flatten(avg_template,order = 'F')\n",
    "    flat_annot = np.ndarray.flatten(annotation200[0],order = 'F')\n",
    "    \n",
    "    if comp_filter is None:\n",
    "        comp_filter = np.arange(len(M['H']))\n",
    "    \n",
    "    affinity_2d = np.corrcoef(M['H'][comp_filter,:].T)\n",
    "    spect_clus  = SpectralClustering(n_clusters=20, affinity = 'precomputed').fit(np.abs(affinity_2d))\n",
    "    labels      = spect_clus.labels_\n",
    "    nmi_score   = nmi(flat_annot[Non_zero_voxels][new_masks['vox_mask']],labels)\n",
    "    print np.shape(labels), nmi_score \n",
    "\n",
    "    flat_avg_temp_sub = flat_avg_temp[Non_zero_voxels][new_masks['vox_mask']]\n",
    "    flat_avg_temp_sub_z = sci.stats.zscore(flat_avg_temp_sub)\n",
    "    for i in comp_filter:\n",
    "        lica2gray[i,:] = sci.stats.pearsonr(M['H'][i,:],flat_avg_temp_sub_z)\n",
    "\n",
    "    plt.hist(lica2gray)    \n",
    "\n",
    "\n",
    "    return lica2gray, labels, nmi_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
