{"cells":[{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":["from cfg import *"],"execution_count":1,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true},"cell_type":"code","source":["API_PATH = \"http://api.brain-map.org/api/v2/data\"\n","GRAPH_ID = 1\n","PLANE_ID = 1 # coronal\n","MOUSE_PRODUCT_ID = 1 # aba\n","\n","DATA_CON_SET_QUERY_URL = (\"%s/SectionDataSet/query.json\" +\\\n","                          \"?criteria=[failed$eqfalse]\" +\\\n","                          \",products[id$in5]\" +\\\n","                          \",[green_channel$eqrAAV]\" +\\\n","                          #\",specimen(donor[transgenic_mouse_id$eqall donors])\" +\\\n","                          \",specimen(stereotaxic_injections(age[days$ge54],[days$le58]))\" +\\\n","                          \",plane_of_section[id$eq%d]\" +\\\n","                          \"&include=specimen(stereotaxic_injections(age,stereotaxic_injection_materials,stereotaxic_injection_coordinates,primary_injection_structure)),specimen(donor(age))\") \\\n","                          % (API_PATH, PLANE_ID)\n","\n","UNIONIZE_CON_FMT = \"%s/ProjectionStructureUnionize/query.json\" +\\\n","               \"?criteria=[section_data_set_id$eq%d],[is_injection$eqfalse]\" +\\\n","               \"&include=hemisphere\"\n","\n","STRUCTURES_URL = (\"%s/Structure/query.json?\" +\\\n","                      \"criteria=[graph_id$eq%d]\") \\\n","                      % (API_PATH, GRAPH_ID)\n","\n","    \n","DATA_EXP_SET_QUERY_URL = (\"%s/SectionDataSet/query.json\" +\\\n","                          \"?criteria=[failed$eq'false'][expression$eq'true']\" +\\\n","                          \",products[id$eq%d]\" +\\\n","                          \",plane_of_section[id$eq%d],genes\" +\\\n","                          \"&include=genes\") \\\n","                          % (API_PATH, MOUSE_PRODUCT_ID, PLANE_ID)\n","\n","UNIONIZE_EXP_FMT = \"%s/StructureUnionize/query.json\" +\\\n","               \"?criteria=[section_data_set_id$eq%d],structure[graph_id$eq1]\" +\\\n","               (\"&include=section_data_set(products[id$in%d])\" % (MOUSE_PRODUCT_ID)) +\\\n","               \"&only=id,structure_id,sum_pixels,expression_energy,section_data_set_id\"\n","    "],"execution_count":2,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true},"cell_type":"code","source":["# Make a query to the API via a URL.\n","def QueryAPI(url):\n","    start_row = 0\n","    num_rows = 2000\n","    total_rows = -1\n","    rows = []\n","    done = False\n","\n","    # The ontology has to be downloaded in pages, since the API will not return\n","    # more than 2000 rows at once.\n","    while not done:\n","        pagedUrl = url + '&start_row=%d&num_rows=%d' % (start_row,num_rows)\n","\n","        print pagedUrl\n","        source = urllib.urlopen(pagedUrl).read()\n","\n","        response = json.loads(source)\n","        rows += response['msg']\n","\n","        if total_rows < 0:\n","            total_rows = int(response['total_rows'])\n","\n","        start_row += len(response['msg'])\n","\n","        if start_row >= total_rows:\n","            done = True\n","\n","    print('Number of results: {}'.format(total_rows))\n","    return rows"],"execution_count":3,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true},"cell_type":"code","source":["def DownloadUnionizedData(dataSets):\n","    unionizes = [QueryAPI(UNIONIZE_FMT % (API_PATH,d['id'])) for d in dataSets]\n","    return unionizes"],"execution_count":4,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true},"cell_type":"code","source":["# Download the mouse brain structures in a structure graph.\n","def DownloadStructures():\n","    structs = QueryAPI(STRUCTURES_URL)\n","\n","    # Build a dict from structure id to structure and identify each node's\n","    # direct descendants.\n","    structHash = {}\n","    for s in structs:\n","        s['num_children'] = 0\n","        s['structure_id_path'] = [int(sid) for sid in s['structure_id_path'].split('/') if sid != '']\n","        structHash[s['id']] = s\n","\n","    for sid,s in structHash.iteritems():\n","        if len(s['structure_id_path']) > 1:\n","            parentId = s['structure_id_path'][-2]\n","            structHash[parentId]['num_children'] += 1\n","\n","    ## pull out the structure ids for structures in this structure graph that\n","    ## have no children (i.e. just the leaves)\n","    ## corrStructIds = [sid for sid,s in structHash.iteritems() if s['num_children'] == 0]\n","    # RB: no, leave all structures in and filter later\n","    corrStructIds = structHash.keys()\n","\n","    return sorted(corrStructIds), structHash"],"execution_count":5,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true},"cell_type":"code","source":["def CreateConnectivityMatrix(dataSets,structureIds,structHash,unionizes):\n","    # Each injection experiment will have a connectivity vector.  This vector will be as long\n","    # as the number of requested structures.\n","    nstructs = len(structureIds)\n","    ndata = len(unionizes)\n","    print('ndata {} ndatasets {}'.format(ndata,len(dataSets)))\n","\n","    sidHash = dict([(id,i) for (i,id) in enumerate(structureIds)])\n","    didHash = dict([(d['id'],i) for (i,d) in enumerate(dataSets)])\n","\n","    connectivityL = np.empty([nstructs,ndata])\n","    connectivityL.fill(np.nan)\n","    connectivityR = np.empty([nstructs,ndata])\n","    connectivityR.fill(np.nan)\n"," \n","    connectivityDict = {'projection_density': 0, 'projection_intensity': 0, 'projection_energy': 0, 'projection_volume': 0, 'normalized_projection_volume': 0}\n","    for key in connectivityDict.keys():\n","        connectivityDict[key] = np.empty([nstructs,ndata])\n","        connectivityDict[key].fill(np.nan)\n","\n","    # For each data set's set of unionizes, then for each individual structure,\n","    # fill in the structure's connectivity vector.\n","    for i,us in enumerate(unionizes):\n","        # for each unionize\n","        for j,u in enumerate(us):\n","            sid = u['structure_id']\n","            did = u['section_data_set_id']\n","\n","            struct = structHash[sid]\n","            struct['volume'] = u['sum_pixels']\n","\n","            if i ==0 and j == 0:\n","              print u\n","\n","            if sidHash.has_key(sid) and didHash.has_key(did):\n","                if u['hemisphere_id'] is 1:\n","                    connectivityL[sidHash[sid]][didHash[did]]  = u['normalized_projection_volume']\n","                elif u['hemisphere_id'] is 2:\n","                    connectivityR[sidHash[sid]][didHash[did]] = u['normalized_projection_volume']\n","                    for key in connectivityDict.keys():\n","                        connectivityDict[key][sidHash[sid]][didHash[did]] = u[key]\n","                elif u['hemisphere_id'] is 3:\n","                  pass\n","                  # this is just the average value of L+R\n","            else:\n","                print \"ERROR: structure {}/injection {} skipped.\".format(sid,did)\n","\n","    return connectivityL, connectivityR"],"execution_count":6,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true},"cell_type":"code","source":["def CreateExpressionMatrix(dataSets,structureIds,structHash,unionizes):\n","    # Each structure will have an expression vector.  This vector will be as long\n","    # as the number of requested structures.\n","    nstructs = len(structureIds)\n","    ndata = len(unionizes)\n","\n","    sidHash = dict([(id,i) for (i,id) in enumerate(structureIds)])\n","    didHash = dict([(d['id'],i) for (i,d) in enumerate(dataSets)])\n","\n","    expression = np.empty([nstructs,ndata])\n","    expression.fill(np.nan)\n","\n","    # For each data set's set of unionizes, then for each individual structure,\n","    # fill in the structure's expression vector.\n","    for i,us in enumerate(unionizes):\n","        # for each unionize\n","        for j,u in enumerate(us):\n","            sid = u['structure_id']\n","            did = u['section_data_set_id']\n","\n","            struct = structHash[sid]\n","            struct['volume'] = u['sum_pixels']\n","\n","            if sidHash.has_key(sid) and didHash.has_key(did):\n","                expression[sidHash[sid]][didHash[did]] = u['expression_energy']\n","\n","    return expression"],"execution_count":7,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true},"cell_type":"code","source":["def allChildren(acr,acr2parent):\n","   # Description: given a tree hierarchy and an entity,\n","   # this function returns all the children\n","   # of the entity\n","\n","   AC = []\n","   for a,p in acr2parent.items():\n","     if p == acr:\n","       AC.append(a)\n","       AC.extend(allChildren(a,acr2parent))\n","   return AC"],"execution_count":8,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true},"cell_type":"code","source":["def ReduceToLeafNodes(structure_acronyms,tree_file):\n","    # Description: this function checks the givenReduceToLeafNodes structures\n","    #              based on the tree hierarchy and returns the\n","    #              leaf nodes\n","\n","    leaf_nodes = []\n","    with open(tree_file) as fp:\n","        acr2parent = json.load(fp)\n","\n","    for idx,acro in enumerate(structure_acronyms):\n","        AC = allChildren(acro, acr2parent)\n","        if len(AC) == 0: # structure is a leaf node\n","           leaf_nodes.append((idx,acro))\n","    return leaf_nodes\n",""],"execution_count":9,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true},"cell_type":"code","source":["def GetConUnionizes():\n","\n","    infile1 = 'expression_files/inj_unionizes.nrrd'\n","    infile2 = 'expression_files/exp_density.nrrd'\n","    mcc = MouseConnectivityCache(resolution = 100)\n","    experiments = mcc.get_experiments(cre = True, dataframe=True)\n","    uni_con = []\n","    for idx,val in enumerate(experiments['id']):\n","        print idx\n","        tmp = mcc.get_experiment_structure_unionizes(experiment_id = val)\n","        uni_con.append(tmp)\n","    print uni_con.shape\n","    fp = h5py.File('unionized_connectivity.hdf5','w')\n","    fp.create_dataset('dataset1',data = uni_con)"],"execution_count":10,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":["def GetCreLines():\n","    infile = 'cre_inj_density.nrrd'\n","    infile2 = 'cre_pro_density.nrrd'\n","    mcc = MouseConnectivityCache(resolution = 100)\n","    mca = MouseConnectivityApi()\n","    cre_experiments2 = mca.experiment_source_search(injection_structures = 'root', transgenic_lines= True)\n","    cre_experiments = mcc.get_experiments(cre= True,dataframe=True)\n","    MetaPerCre = []\n","    InjPerCre  = []\n","    ProjPerCre = []\n","\n","    creDict = {}\n","    with open('Supplementary Table 1.csv') as fp:\n","         buff = csv.reader(fp)\n","         for idx,row in enumerate(buff):\n","             if len(row) > 1: # concatenate the two rows together - error caused by csv transition\n","                row[0] =  row[0] + row[1]\n","             remains = [x for x in filter(None,row[0].split(';'))]\n","             if remains[0].isdigit():\n","             #if len(remains) > 8 and idx > 2 and remains[1].isdigit() == False:\n","                creDict[remains[1]] = []\n","                creDict[remains[1]].append(remains[7])\n","                creDict[remains[1]].append(remains[8])\n","\n","    #pk.dump(cre_experiments,open('cre_experiments.pkl','wb'))\n","    #cre_experiments = pk.load(open('cre_experiments.pkl','rb'))\n","\n","    # download the projection density volume for one of the experiments\n","    for idx,val in enumerate(cre_experiments['id']):\n","        mcc.get_projection_density(val, infile2)\n","        tmp = cre_experiments['transgenic_line'][val]\n","        if 'A93' in cre_experiments['transgenic_line'][val]:\n","            tmp = 'A93-Tg1-Cre'\n","        selCre = [key for key in creDict.keys() if tmp == key]\n","        if len(selCre) > 0:\n","            selCre = selCre[0]\n","            MetaPerCre.append({})\n","            rx = len(MetaPerCre)-1\n","            MetaPerCre[rx]['injection-coordinates'] = \\\n","            [cre_experiments['injection_x'][val],\n","             cre_experiments['injection_y'][val],\n","             cre_experiments['injection_z'][val],\n","            ]\n","            MetaPerCre[rx]['structure-abbrev'] =\\\n","             cre_experiments['structure_abbrev'][val]\n","            MetaPerCre[rx]['transgenic-line'] = tmp\n","            MetaPerCre[rx]['id'] = cre_experiments['id'][val]\n","            MetaPerCre[rx]['layer'] = creDict[selCre][0]\n","            MetaPerCre[rx]['Cell Type'] = creDict[selCre][1]\n","            # read it into memory\n","            pd_array, pd_info = nrrd.read(infile2)\n","            ProjPerCre.append(pd_array)\n","\n","    f2 = h5py.File('ProjPerCre.hdf5','w')\n","    f2.create_dataset('dataset1',data = ProjPerCre)\n","    pk.dump(MetaPerCre,open('MetaPerCre.pkl','wb'))\n","    return MetaPerCre, ProjPerCre\n",""],"execution_count":11,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true},"cell_type":"code","source":["def ReadConnectivityData():\n","    infile = 'expression_files/inj_density.nrrd'\n","    infile2 = 'expression_files/proj_density.nrrd'\n","    mca = MouseConnectivityApi()\n","    mcc = MouseConnectivityCache(resolution = 100)\n","    all_experiments = mcc.get_experiments(dataframe=True)\n","    # get metadata for all non-Cre experiments\n","    experiments = mca.experiment_source_search(injection_structures = 'root', transgenic_lines = 0)\n","    ProjPerExp = []\n","    MetaPerInj = []\n","    # download the projection density volume for one of the experiments\n","    for idx,val in enumerate(experiments):\n","        mca.download_projection_density(infile2, val['id'], resolution = 100)\n","        MetaPerInj.append({})\n","        for key,item in val.iteritems():\n","            MetaPerInj[idx][key] = item\n","        # read it into memory\n","        pd_array, pd_info = nrrd.read(infile2)\n","        ProjPerExp.append(pd_array)\n","   \n","    ProjPerExp = np.asarray(ProjPerExp,dtype = 'float32')\n","    f2 = h5py.File('ProjPerExp.hdf5','w')\n","    f2.create_dataset('dataset1',data = ProjPerExp)\n","    pk.dump(MetaPerInj,open('MetaPerInj.pkl','wb'))\n","    return MetaPerInj, ProjPerExp"],"execution_count":12,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":["def ParseAtlasData():\n","    \n","    # *********** initial download of tracing experiments and brain structures ***********#\n","    print 'Commencing cre-line mission'\n","    CreMeta, ProjPerCre = GetCreLines()\n","    print 'Cre-line parsing has been completed'\n","    print 'Commencing rAAv mission'\n","    WTMeta, ProjPerExp = ReadConnectivityData()\n","    print 'rAAv parsing has been completed'\n","    structureIds,structHash = DownloadStructures() \n","    with open('structures.csv', \"w\") as fp:\n","        M = []\n","        for sid in structureIds:\n","            v = structHash[sid]\n","            M.append([v['id'], v['acronym'], v['name'],\n","                      v['parent_structure_id'], v['color_hex_triplet']])\n","        w = csv.writer(fp, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n","        w.writerow(['id', 'acronym', 'name',\n","                    'parent_structure_id', 'color_hex_triplet'])\n","        for line in M:\n","            w.writerow(line)\n","\n","    pk.dump(structureIds,open('structureIds.pkl','wb'))\n","    pk.dump(structHash,open('structHash.pkl','wb'))\n","    \n","    #*********** wild type-data unionization **************************************#\n","    unionizes_wt_proj = [QueryAPI(UNIONIZE_CON_FMT % (API_PATH,d['id'])) for d in WTMeta]\n","    pk.dump(unionizes_wt_proj, open('unionizes_wt_proj.pkl', 'wb'))\n","    print 'unionizing of wild-type is complete'\n","    connectivityL, connectivityR = CreateConnectivityMatrix(WTMeta,\n","                                   structureIds,structHash,\n","                                   unionizes_wt_proj)\n","   \n","    # Cre-data unionization *************************************************#\n","    unionizes_cre_proj = [QueryAPI(UNIONIZE_CON_FMT % (API_PATH,d['id'])) for d in CreMeta]\n","    pk.dump(unionizes_cre_proj, open('unionizes_cre_proj.pkl', 'wb'))\n","    print 'unionization is complete'\n","    cre_pr_L, cre_pr_R = CreateConnectivityMatrix(CreMeta, structureIds, structHash, unionizes_cre_proj)\n","    fp1 = h5py.File('cre_pr_L.hdf5', 'w')\n","    fp2 = h5py.File('cre_pr_R.hdf5', 'w')\n","    fp1.create_dataset('dataset1', data = cre_pr_L)\n","    fp2.create_dataset('dataset1', data = cre_pr_R)\n","    #************************************************************************#\n","\n","\n","    # Expression Data Parsing\n","    ExpMeta = QueryAPI(DATA_EXP_SET_QUERY_URL)\n","    pk.dump(ExpMeta,open('GeneMeta.pkl','wb'))\n","    unionizes_exp = [QueryAPI(UNIONIZE_EXP_FMT % (API_PATH,d['id'])) for d in ExpMeta]\n","    gene_expression = CreateExpressionMatrix(ExpMeta,structureIds,structHash,unionizes_exp)\n","    fp1 = h5py.File('G_Exp.hdf5','w')\n","    fp1.create_dataset('dataset1',data = gene_expression)\n","    \n","     \n","    '''conR              = pk.load(open('conR.pkl','rb'))\n","    conL              = pk.load(open('conL.pkl','rb'))\n","    WTMeta            = pk.load(open('MetaPerInj.pkl','rb'))\n","    CreMeta           = pk.load(open('MetaPerCre.pkl','rb'))\n","    structureIds      = pk.load(open('structureIds.pkl','rb'))\n","    structHash        = pk.load(open('structHash.pkl','rb'))\n","    cre_pr_L          = h5py.File('cre_pr_L.hdf5', 'r')['dataset1']\n","    cre_pr_R          = h5py.File('cre_pr_R.hdf5', 'r')['dataset1']\n","    #unionizes_wt_proj = pk.load(open('unionizes_wt_proj.pkl', 'rb'))'''\n","    \n","\n","    \n","    # *********** Partition tracers based on their cre-line *****#\n","    cre15      = ['Syt6-Cre_KI148', 'Ntsr1-Cre_GN220', 'Sim1-Cre_KJ18',\n","                'Efr3a-Cre_NO108', 'Chrna2-Cre_OE25', 'A93-Tg1-Cre',\n","                'Tlx3-Cre_PL56', 'Rbp4-Cre_KL100', 'Rorb-IRES2-Cre',\n","                'Scnn1a-Tg3-Cre', 'Nr5a1-Cre', 'Sepw1-Cre_NP39',\n","                'C57BL/6J', 'Emx1-IRES-Cre', 'Cux2-IRES-Cre']\n","\n","    creCategories = [cre['transgenic-line'] for cre in CreMeta]\n","    creCategories = list(set(creCategories))\n","\n","    Affinity = np.asarray([[0,0,0.001,-5.7125],\n","                           [-0.001,0,0,5.3625],\n","                           [0,-0.001,0,5.1625]])\n","\n","    InjCoo  = []\n","    InjCoo2 = []\n","    for idx,injection in enumerate(WTMeta):\n","      coord = injection['injection-coordinates'];\n","      coord = np.array([coord[0],coord[1],coord[2],1.0]);\n","      InjCoo.append( Affinity.dot(coord) )\n","    InjCoo = np.asarray(InjCoo,dtype = 'float32')      # Wild_type coordinates\n","    for idx,injection in enumerate(CreMeta):\n","      coord = injection['injection-coordinates'];\n","      coord = np.array([coord[0],coord[1],coord[2],1.0]);\n","      InjCoo2.append( Affinity.dot(coord) )\n","    InjCoo2 = np.asarray(InjCoo2); InjCoo = np.asarray(InjCoo) \n","    \n","    CreLineDict = OrderedDict()\n","    for category in cre15:\n","        cre_members = np.asarray([idx for idx, val in enumerate(CreMeta)\\\n","        if val['transgenic-line'] == category])\n","        if len(cre_members) > 0:\n","            if 'Syt6' in category:\n","                set_trace()\n","            CreLineDict[category] = \\\n","            {'ConMat' : cre_pr_R[:, cre_members],\\\n","            'structure-abbrev' : [CreMeta[val]['structure-abbrev'] \n","                                  for val in cre_members],\\\n","            'layer'       : [CreMeta[val]['layer'] for val in cre_members],\\\n","            'cell-type'   : [CreMeta[val]['Cell Type'] for val in cre_members],\\\n","            'indices'     : cre_members,\\\n","            'id'          : [CreMeta[val]['id'] for val in cre_members],\\\n","            'Coordinates' : InjCoo2[cre_members,:]}\n","\n","\n","    CreLineDict['wild_type'] = {'ConMat' : conR, \\\n","                                'structure-abbrev' : \\\n","                                [val['structure-abbrev'] for val in WTMeta],\\\n","                                'layer'    : ['inspecific' for idx in range(len(WTMeta))],\\\n","                                'cell-type': ['inspecific' for idx in range(len(WTMeta))],\\\n","                                'id'       : [val['id'] for val in WTMeta],\\\n","                                'Coordinates' : InjCoo}\n","    \n","    ConDict        = pk.dump(CreLineDict, open('CreLineDict.pkl','wb'))\n","    \n","    return CreLineDict"],"execution_count":13,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":["CreLineDict = ParseAtlasData()"],"execution_count":15,"outputs":[]}],"metadata":{"kernelspec":{"name":"python2","display_name":"Python 2","language":"python"},"language_info":{"mimetype":"text/x-python","nbconvert_exporter":"python","name":"python","pygments_lexer":"ipython2","version":"2.7.14","file_extension":".py","codemirror_mode":{"version":2,"name":"ipython"}}},"nbformat":4,"nbformat_minor":1}