{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cfg import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## readtable function\n",
    "### Author: Nestor Timonidis   \n",
    "### Description:   \n",
    "Given the file pointer of a .csv file, this function stores the csv elements into a dictionary\n",
    "*        #### input:    \n",
    "fp = the file pointer corresponding to the .csv file\n",
    "*        #### output:   \n",
    "T = the output dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readtable(fp):\n",
    "  r = csv.reader(fp)\n",
    "  keys = r.next()\n",
    "  T = [ [] for k in keys ]\n",
    "  for row in r:\n",
    "    for i,v in enumerate(row):T[i].append(v)\n",
    "  T = {keys[i]:col for i,col in enumerate(T)}\n",
    "  return T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CellTypeParser(filename):\n",
    "    fp = open(filename,'rb')\n",
    "    exclude_list = ['Probe', 'Gene.Symbol', 'GeneNames', 'GOTerms', 'GemmaIDs', 'NCBIids']\n",
    "    forbidden_list = []\n",
    "    X = []\n",
    "    gene_annot = []\n",
    "    cell_annot = []\n",
    "    # Part I: get genetic and cellular annotations\n",
    "    reader = csv.reader(fp)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
    "    for idx,row in enumerate(reader):\n",
    "      if idx == 0:\n",
    "          for idx2,col in enumerate(row):  \n",
    "               if idx2 > 0 and col not in exclude_list:\n",
    "                  cell_annot.append(col)\n",
    "               elif col in exclude_list:\n",
    "                   forbidden_list.append(idx2)     \n",
    "      elif idx > 0:    \n",
    "           gene_annot.append(row[1])\n",
    "    \n",
    "    # Part II: acquire the cell-type specific data\n",
    "    fp = open(filename,'rb')\n",
    "    reader = csv.reader(fp)\n",
    "    for row_idx,row in enumerate(reader):\n",
    "        #expressions = [val for val in row if val.isnumeric()]\n",
    "        if row_idx > 0:\n",
    "           expressionz = [val.replace('NA','NaN',1) for val in row]\t\t\n",
    "           expressions = [val for idx,val in enumerate(expressionz) if (val.replace('.','',1).isdigit() == True or val == 'NaN') and idx not in forbidden_list]\n",
    "           X.append(expressions)\n",
    "            \n",
    "    X = np.asfortranarray(X,dtype = 'float32')\n",
    "    fp.close()      \n",
    "    return X,gene_annot,cell_annot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sbaInterface_class:\n",
    "    def __init__(this,sbaUrl,sbaHost):\n",
    "        # In Javascript, load sbaInterface.js and create a new sbaInterface object\n",
    "        display(Javascript(\"\"\"\n",
    "            var script = document.createElement('script');\n",
    "            script.src = '{}/../js/sba-interface.js';\n",
    "            script.onload = function() {{\n",
    "              window.global_sbaInterface = new sbaInterface_class('{}');\n",
    "            }}\n",
    "            document.head.appendChild(script)\n",
    "        \"\"\".format(sbaHost,sbaUrl)))\n",
    "        clear_output()\n",
    "        \n",
    "    def send(this,sbaCommand):\n",
    "        display(Javascript(\"\"\"window.global_sbaInterface.send({})\"\"\".format(json_encode(sbaCommand))))\n",
    "        clear_output() # prevents clogging the Jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RemoveOutliers class\n",
    "### Author: Nestor Timonidis   \n",
    "### Description:   \n",
    "Removes outliers based on the input array's per column interquartile range. Elements exceeding the range of $(Q1 - 1.5*iqr, Q3 + 1.5*iqr)$,  \n",
    "where the interquartile range (iqr) and the two quartiles (1st and 3rd) are defined per column of the array, are detected as outliers, and their corresponding columns are being removed from the array. A corresponds to the updated array.\n",
    "*   ### functions:   \n",
    "       *        #### init:    \n",
    "initalizes the quartiles used for estimating outliers. By default are Q1 (1st) and Q3 (3rd)\n",
    "       *        #### fit:   \n",
    "scans the input array per column and checks for outliers based on the method provided in the description.\n",
    "For each detected outlier, the column corresponding to it is removed from the array.\n",
    "The function returns the updated array, as well as a list that stores the outlier indices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemoveOutliers():\n",
    "    __slots__ = ['q1','q2'] \n",
    "    def __init__(self, q1 = 25, q2 = 75):\n",
    "        self.q1 = q1\n",
    "        self.q2 = q2\n",
    "    def fit(self, A):\n",
    "        ranges = iqr(A, axis = 0)\n",
    "        Q1 = np.percentile(A, self.q1, axis = 0)\n",
    "        Q3 = np.percentile(A, self.q2, axis = 0)\n",
    "\n",
    "        genes4deletion = Set()\n",
    "        for row,cell in enumerate(A):\n",
    "            for col,gene in enumerate(cell):\n",
    "               #if gene >= Q3[col] + ranges[col]*3/2 or gene <= Q1[col] - ranges[col]*3/2:\n",
    "                if gene >= Q3[col] + ranges[col]*3/2 or gene <= - Q1[col] + ranges[col]*3/2:\n",
    "                   genes4deletion.add(col)\n",
    "                   break\n",
    "\n",
    "        outliers = list(genes4deletion)\n",
    "        A = np.delete(A,outliers,1)\n",
    "        return A,outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filter_cortex function\n",
    "### Author: Nestor Timonidis   \n",
    "### Description:   \n",
    "Given a mask corresponding to specific stuctures of the mouse cortex and a tract tracing experiment defined in a volumetric space, this function filters out voxels of the experiment not corresponding to the mask.\n",
    "*        #### input:    \n",
    "         * struct_idx_dict = dictionary that maps identities of anatomically distinct structures, as provided by the Allen Institute, to their position in a unionized space, meaning a 2D space of anatomical structures and brain related experiments.\n",
    "         * l_vals = the anatomical structures that will be used to filter the data, defined by their position in the unionized space.\n",
    "         * Annotation = the Annotation identities of the structures in the volumetric space, as provided by the Allen Institute.\n",
    "         * pd: the tract tracing experiment defined in the volumetric space, measured by projection density.\n",
    "*        #### output:   \n",
    "         * l_voxel_mask = the refined volumetric data based on the applied mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_cortex(struct_idx_dict,l_vals, Annotation, pd):\n",
    "        scis = [sci for sci,x in struct_idx_dict.items() if x in l_vals]\n",
    "        l_mask = []\n",
    "        for sci in scis:\n",
    "            tmp = Annotation == int(sci) \n",
    "            if l_mask == []:\n",
    "                l_mask = tmp\n",
    "            else:\n",
    "                l_mask = np.ma.mask_or(l_mask,tmp)   \n",
    "\n",
    "        l_voxel_mask = np.zeros(np.shape(pd))\n",
    "        l_nzero      = np.nonzero(l_mask)\n",
    "        l_voxel_mask[l_nzero] = pd[l_nzero]\n",
    "        return l_voxel_mask\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MakeArrayBorders function\n",
    "### Author: Nestor Timonidis   \n",
    "### Description:   \n",
    "This function creates a structural border mask given the annotation volume of the mouse brain, based on an anatomical parcelation made by the Allen Institute. The voxels are masked based on the ones that differ in value with their neighboring voxels and the rest.\n",
    "*        #### input:    \n",
    "         * Annotation_copy = annotation volume of the mouse brain (defined in micrometers).\n",
    "*        #### output:   \n",
    "         * Annotation_copy2 = the structural border mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeArrayBorders(Annotation_copy):\n",
    "    #description: given a matrix, comparison with neighbors so that different neighboring pixels get the value 1 \n",
    "    #             and create borders, else 0\n",
    "    \n",
    "    Annotation_copy2 = np.zeros(np.shape(Annotation_copy))\n",
    "    \n",
    "    for x,row in enumerate(Annotation_copy):\n",
    "       \n",
    "        for y,col in enumerate(row):\n",
    "            flag = 0\n",
    "            if x > 0:\n",
    "                if (Annotation_copy[x,y] - Annotation_copy[x-1,y]) != 0:\n",
    "                    flag = 1\n",
    "            if x < np.shape(Annotation_copy)[0] - 1:\n",
    "                if (Annotation_copy[x,y] - Annotation_copy[x+1,y]) != 0:\n",
    "                    flag = 1\n",
    "            if y > 0:\n",
    "                if (Annotation_copy[x,y] - Annotation_copy[x,y-1]) != 0:\n",
    "                    flag = 1\n",
    "            if y < np.shape(Annotation_copy)[1] - 1:\n",
    "                if (Annotation_copy[x,y] - Annotation_copy[x,y+1]) != 0:\n",
    "                    flag = 1    \n",
    "                    \n",
    "            if flag == 1:\n",
    "                Annotation_copy2[x,y] = 1\n",
    "            else:\n",
    "                Annotation_copy2[x,y] = 0\n",
    "                \n",
    "    return Annotation_copy2          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BrainPlotter class\n",
    "### Author: Nestor Timonidis   \n",
    "### Description:   \n",
    "This class contains functions that provide visualizations of the mouse brain, either in the form of brain slices or cortical flatmaps. The inputs of interest for visualization are identities of tract tracing or gene expression experiments, as defined by the Allen Institute.\n",
    "### init function:   \n",
    "   #### Description:  \n",
    "creates a brain plotter object by defining a number of parameters for the visualization process.\n",
    "  *  #### input:  \n",
    "      *  exp_id = the identity of the experiment  \n",
    "      *  driver  =  the type of the experiment. It could be wild_type (tract tracing), or gene_expression for the equivalent experiment categories, or the name of a cre category (tract tracing).\n",
    "      * resolution = resolution of the experiment in micrometers. Acceptable options are: 100, 25 and 10 for tract tracing experiments, and 200 for gene expression.\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrainPlotter:\n",
    "    \n",
    "    __slots__ = ['id','driver','pd','resolution']\n",
    "    def __init__(self,  exp_id = 181599674, driver = 'wild_type', resolution = 25):\n",
    "        \n",
    "        self.id         = exp_id\n",
    "        self.resolution = resolution\n",
    "        self.driver     = driver  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BrainPlotter class\n",
    "### ReduceToParent function\n",
    "### Author: Nestor Timonidis\n",
    "### Description:   \n",
    "In order to make a cortical flatmap visualization for the mouse cortex,    \n",
    "we need to initially ensure that we have reduced the complexity  \n",
    "of structural brain areas to a layer inspecific level that corresponds    \n",
    "to clear anatomically distinct areas like the primary visual or motor cortex.   \n",
    "Therefore we utilize the MouseConnectivityCache tool to get the ontology    \n",
    "tree of brain areas, then we reduce it by the isocortex and for each    \n",
    "isocortical area, we take the identity of its father node, or an area  \n",
    "belonging up in the tree hierarchy.      \n",
    "Having achieved that, the next step is to replace the identity of each    \n",
    "area in the annotation volume structure by the father identity.  \n",
    "* #### Input: \n",
    "  * Annotation = volumetric array containing information about the annotation volume of each voxel, as defined by the Allen Institute for Brain Science.\n",
    "  * mcc = an instance of mouse connectivity cache class of the same toolbox provided by the Allen Institute, whose functions will be used throughout the ReduceToParent function.\n",
    "* #### Output:\n",
    "  * Annotation_copy = the updated annotation volume, with structures having been reduced to their parent node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrainPlotter(BrainPlotter): \n",
    "    \n",
    "    def ReduceToParent(self, Annotation, mcc):\n",
    "        # grab the StructureTree instance\n",
    "        structure_tree = mcc.get_structure_tree()\n",
    "        oapi           = OntologiesApi()\n",
    "        # Get isocortex summary structures\n",
    "        kid_to_father = {}\n",
    "        summary_structures = structure_tree.get_structures_by_set_id([688152357])\n",
    "        for val in summary_structures:\n",
    "            kids = structure_tree.descendant_ids([val['id']])\n",
    "            for kid in kids[0]:\n",
    "                kid_to_father[kid] = val['id']\n",
    "\n",
    "        # For each distinct isocortical area, replace its children annotation with the area's id\n",
    "        father_acros_indices = {val['acronym']:val['id'] for val in summary_structures}\n",
    "\n",
    "        #store the original annotation shape because it will be flatten for computational simplicity when updating the parent nodes\n",
    "        orig_annot_shape = np.shape(Annotation)\n",
    "        flat_annot = np.ndarray.flatten(Annotation, order = 'C')\n",
    "\n",
    "        # Create a copy of the annotation structure that reduces areas to their parent nodes\n",
    "        Annotation_copy = np.zeros(np.shape(flat_annot))\n",
    "\n",
    "        for kid_id,parent_id in kid_to_father.items():\n",
    "            tmp  = flat_annot == int(kid_id)\n",
    "            nzero_ids = np.nonzero(tmp)\n",
    "            Annotation_copy[nzero_ids] = parent_id \n",
    "\n",
    "        return Annotation_copy    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BrainPlotter class\n",
    "### fit function\n",
    "### Author: Nestor Timonidis\n",
    "### Description:   \n",
    "In order to make a cortical flatmap visualization for the mouse cortex,    \n",
    "we need to initially ensure that we have reduced the complexity  \n",
    "of structural brain areas to a layer inspecific level that corresponds    \n",
    "to clear anatomically distinct areas like the primary visual or motor cortex.   \n",
    "Therefore we utilize the MouseConnectivityCache tool to get the ontology    \n",
    "tree of brain areas, then we reduce it by the isocortex and for each    \n",
    "isocortical area, we take the identity of its father node, or an area  \n",
    "belonging up in the tree hierarchy.      \n",
    "Having achieved that, the next step is to replace the identity of each    \n",
    "area in the annotation volume structure by the father identity.  \n",
    "* #### Input:\n",
    "   * projection = primary array corresponding to an experiment as a manual option. Default = None.\n",
    "   * projection2 = secondary array corresponding to an experiment as a manual option. Default = None.\n",
    "   * chosen_indices = In case that the input projection array contains less target brain areas than the complete ~ 1300, a list is required to map the reduced elements to their original space of ~1300 elements. Default = None.\n",
    "   * layer_profile = the layer profiles of interest that will be used to filter the data, defined by their position in the unionized space, in case they are being provided.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BrainPlotter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-57656ab30f80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mBrainPlotter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBrainPlotter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     def fit(self, projection = None, layer_profile = None, projection2 = None, \n\u001b[1;32m      4\u001b[0m             chosen_indices = None):  \n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BrainPlotter' is not defined"
     ]
    }
   ],
   "source": [
    "class BrainPlotter(BrainPlotter):\n",
    "    \n",
    "    def fit(self, projection = None, layer_profile = None, projection2 = None, \n",
    "            chosen_indices = None):  \n",
    "        \n",
    "        self.mcc = MouseConnectivityCache(resolution = self.resolution)\n",
    "        if projection is not None:\n",
    "                     \n",
    "            if len(projection) != 1327 and len(np.shape(projection)) != 3 and chosen_indices == None:\n",
    "                print('the specified projection experiment has been trimmed\\n\\\n",
    "                please provide indices linking the trimmed structures to the original ones (chosen_indices field)')\n",
    "                return -1\n",
    "            \n",
    "        \n",
    "        all_files = os.listdir('./')\n",
    "        \n",
    "        # For reducing time complexity, the brain slice templates provided by the Allen Institute's \n",
    "        # MouseConnectivityCache tool have already been uploaded and stored as files:\n",
    "        # average_template_10.nrrd and average_template_25.nrrd. The code commented bellow\n",
    "        # displays how the templates can be downloaded in a custom fashion.\n",
    "        \n",
    "        template_name = 'average_template_' +  str(self.resolution) + '.nrrd'\n",
    "        annot_name    = 'annotation_'+  str(self.resolution) + '.nrrd'\n",
    "        \n",
    "        if template_name in all_files:\n",
    "            self.template, template_info = nrrd.read(template_name)\n",
    "        else:\n",
    "            self.template, template_info = self.mcc.get_template_volume()\n",
    "        if annot_name in all_files:\n",
    "            Annotation, annot_info = nrrd.read(annot_name)\n",
    "        else:            \n",
    "            Annotation, annot_info = self.mcc.get_annotation_volume()\n",
    "\n",
    "        # projection density: number of projecting pixels / voxel volume\n",
    "        \n",
    "        if projection is not None and len(np.shape(projection)) == 3:\n",
    "            self.pd = projection  \n",
    "            self.pd2 = []\n",
    "            if projection2 is not None and len(np.shape(projection2)) == 3:\n",
    "                self.pd2 = projection2  \n",
    "            self.state = 'raw'  \n",
    "        elif projection is not None and len(np.shape(projection)) != 3:\n",
    "            \n",
    "            with open('structures.csv','rb') as fp:\n",
    "                structure       = readtable(fp) \n",
    "                struct_idx_dict = {val:idx for idx,val in enumerate(structure['id'])}\n",
    "                str_acronyms    = [val for val in structure['acronym']]\n",
    "             \n",
    "            self.pd, self.pd2   = FromUnion2Voxel(Annotation, struct_idx_dict, \n",
    "                                                  projection, Pred_ConStr = projection2, \n",
    "                                                  rem_ind = chosen_indices)\n",
    "            \n",
    "            self.state = 'union'\n",
    "            \n",
    "        else:    \n",
    "            experiment_list = [val for val in all_files if 'experiment_' in val]\n",
    "            file_to_look  = 'experiment_' + str(self.id)\n",
    "            if file_to_look in experiment_list:\n",
    "                pathway  = os.path.join('./',file_to_look)\n",
    "                pd_files = os.listdir(pathway)\n",
    "                if 'projection_density_' + str(self.resolution) + '.nrrd' in pd_files:\n",
    "                    self.pd, pd_info = nrrd.read(pathway +  '/projection_density_' + str(self.resolution) + '.nrrd') \n",
    "                else:\n",
    "                    self.pd, pd_info = self.mcc.get_projection_density(self.id)\n",
    "\n",
    "            else:\n",
    "                self.pd, pd_info = self.mcc.get_projection_density(self.id)\n",
    "            \n",
    "            \n",
    "            self.state = 'raw'\n",
    "            self.pd2 = []\n",
    "        \n",
    "        self.whole_pd = None   \n",
    "        self.whole_pd2 = None   \n",
    "        \n",
    "        #**************** Another step here would be to reduce to layers *******#\n",
    "        if layer_profile is not None:\n",
    "            laminar_prof   = LaminarRegistration(str_acronyms)\n",
    "            layer_ids      = [idx for idx, val in enumerate(laminar_prof) \n",
    "                                               if val == layer_profile]\n",
    "            self.whole_pd  = self.pd\n",
    "            self.whole_pd2 = self.pd2\n",
    "            self.pd        = filter_cortex(struct_idx_dict, layer_ids, \n",
    "                                         Annotation, self.pd)\n",
    "            self.pd2       = filter_cortex(struct_idx_dict, layer_ids, \n",
    "                                         Annotation, self.pd2)\n",
    "        #***********************************************************************#\n",
    "        \n",
    "        self.annotation    = Annotation #self.ReduceToParent(Annotation, self.mcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BrainPlotter class   \n",
    "   ### plot_slice function  \n",
    "   ### author: Nestor Timonidis\n",
    "   ### description:   \n",
    "  provides a visualization in the form of a brain slice, given the parameters defined by the init function and a set of other parameters. This is done by merging the experimental data together with a registration nissl template in an rgb form.\n",
    "*  #### input:  \n",
    "   * slice_idx = the point in the first dimension of the data (anterior-posterior range), which will be stable for the brain slice to be visualized. Default = 264.\n",
    "   * mode = mode with two values. None implies selection of slice_idx and 'max' implies selection of the maximum intensity projection along the anterior-posterior axis of the projection data being selected.  \n",
    "   * savefile = the save file name. Default = 'slice.png'.\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BrainPlotter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-3402fc1bdf63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mBrainPlotter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBrainPlotter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mplot_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m264\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msavefile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'slice.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m#1:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BrainPlotter' is not defined"
     ]
    }
   ],
   "source": [
    "class BrainPlotter(BrainPlotter):    \n",
    "    \n",
    "    def plot_slice(self, slice_idx = 264, savefile = 'slice.png', mode = None):    \n",
    "          \n",
    "        #1:\n",
    "        mask_lpc = np.zeros((np.shape(self.template)[1],np.shape(self.template)[2]))\n",
    "        laplace(self.template[slice_idx,:,:], mask_lpc)\n",
    "        \n",
    "        norm_mask_lpc = mask_lpc/100\n",
    "        norm_mask_lpc[norm_mask_lpc > 1] = 1\n",
    "        \n",
    "        cmap_mask = plt.get_cmap('gray') \n",
    "        rgb_mask  = cmap_mask(norm_mask_lpc, bytes = True)\n",
    "      \n",
    "        if mode == 'max':\n",
    "            pd_mip  = self.pd.max(axis=0)\n",
    "        else:\n",
    "            pd_mip  = self.pd[slice_idx,:,:]\n",
    "          \n",
    "        if self.whole_pd is not None:\n",
    "            pd_whole_mip = self.whole_pd[slice_idx,:,:]\n",
    "            pd_mip  = pd_mip/(1.0*np.max(pd_whole_mip))\n",
    "        elif len(np.unique(self.pd2)) > 1:  \n",
    "            max1 = np.max(self.pd)\n",
    "            max2 = np.max(self.pd2)\n",
    "            if max1 > 5*np.std(self.pd): max1 = np.max(pd_mip)\n",
    "            if max2 > 5*np.std(self.pd2): max2 = np.max(self.pd2[slice_idx,:,:])\n",
    "            pd_mip  = pd_mip/(1.0*max(max1,max2))\n",
    "        else:    \n",
    "            pd_mip  = pd_mip/(1.0*np.max(pd_mip))\n",
    "        #3:\n",
    "        cmap_pd = plt.get_cmap('hot') \n",
    "        rgb_pd  = cmap_pd(pd_mip, bytes = True)\n",
    "        #4:\n",
    "        new_mat = np.maximum(rgb_pd, rgb_mask)\n",
    "         \n",
    "        # Look at a slice from the average template and annotation volume\n",
    "        def format_func(value, tick_number):\n",
    "            return str(value * 0.025)\n",
    "           \n",
    "        fig  = plt.figure(figsize=(15, 6))\n",
    "        ax   = plt.axes()\n",
    "        ax.xaxis.set_major_formatter(plt.FuncFormatter(format_func))\n",
    "        ax.yaxis.set_major_formatter(plt.FuncFormatter(format_func))\n",
    "        pos = ax.imshow(new_mat, cmap = 'hot', aspect='equal', vmin = 0, vmax = 1)\n",
    "        fig.colorbar(pos, ax=ax)\n",
    "        #plt.title(savefile.split('.')[0], fontsize = 20)\n",
    "        plt.savefig(savefile)\n",
    "        plt.show()\n",
    "        \n",
    "        if len(np.unique(self.pd2)) > 1:\n",
    "            pd_mip  = self.pd2[slice_idx,:,:]\n",
    "            if self.whole_pd2 is not None:\n",
    "                pd_whole_mip = self.whole_pd2[slice_idx,:,:]\n",
    "                pd_mip  = pd_mip/(1.0*np.max(pd_whole_mip))\n",
    "            else: \n",
    "                pd_mip  = pd_mip/(1.0*max(max1,max2))\n",
    "\n",
    "            #3:\n",
    "            cmap_pd = plt.get_cmap('hot') \n",
    "            rgb_pd  = cmap_pd(pd_mip, bytes = True)\n",
    "            #4:\n",
    "            new_mat = np.maximum(rgb_pd, rgb_mask)\n",
    "\n",
    "            # Look at a slice from the average template and annotation volume\n",
    "            def format_func(value, tick_number):\n",
    "                return str(value * 0.025)\n",
    "\n",
    "            fig  = plt.figure(figsize=(15, 6))\n",
    "            ax   = plt.axes()\n",
    "            ax.xaxis.set_major_formatter(plt.FuncFormatter(format_func))\n",
    "            ax.yaxis.set_major_formatter(plt.FuncFormatter(format_func))\n",
    "            ax.imshow(new_mat, cmap = 'hot', aspect='equal', vmin = 0, vmax = 1)\n",
    "            #plt.title(savefile.split('.')[0] + '_2', fontsize = 20)\n",
    "            plt.savefig(savefile.split('.')[0] + '_pred.jpg')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BrainPlotter class   \n",
    "   ### plot_flatmap function  \n",
    "   ### author: Nestor Timonidis\n",
    "   ### description:   \n",
    " provides a visualization in the form of a cortical flatmap, given the parameters defined by the init function and a set of other parameters. This is done by converting the experimental data in a flatmap form (calling the cortical_map_10 function), and then merging them together with a registration nissl template and an annotation template in an rgb form.\n",
    "* #### input:  \n",
    "   * savefile = the save file name. Default = 'slice.png'.\n",
    "   * annot_mask_pd = the cortical flatmap merged with with a registration nissl template and an annotation template in an rgb form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BrainPlotter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-73e1095ee230>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mBrainPlotter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBrainPlotter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mplot_flatmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msavefile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'continuous'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolution\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BrainPlotter' is not defined"
     ]
    }
   ],
   "source": [
    "class BrainPlotter(BrainPlotter):\n",
    "    \n",
    "    def plot_flatmap(self,savefile = None, mode = 'continuous'):  \n",
    "        \n",
    "        if self.resolution != 10:\n",
    "           print 'warning! plot_flatmaps currently accepts only 10 micrometer resolution so please change your resolution settings to 10'\n",
    "           return \n",
    "        \n",
    "        cm1 = cm.CorticalMap(projection='top_view')\n",
    "        \n",
    "        #%% scale the nissl template\n",
    "        #%% initialize colormaps for rgb conversion\n",
    "        self.template = self.template/(1.0*np.max(self.template))\n",
    "        cmap_mask     = plt.get_cmap('gray') \n",
    "        cmap_pd       = plt.get_cmap('hot') \n",
    "        \n",
    "        # For reasons of time complexity, the trs_annot_lpc_bordered \n",
    "        # data structure is being loaded by a .pkl file. However,\n",
    "        # commented is the MakeArrayBorders function that creates it\n",
    "        # by masking the Annotation file based on borders and not\n",
    "        # borders between structural brain areas.\n",
    "        if 'Annotation_copy2.pkl' in os.listdir('./'):\n",
    "            self.transformed_annot = pk.load(open('Annotation_copy2.pkl','rb'))\n",
    "        else:   \n",
    "            transformed_annot      = cm1.transform(self.annotation, \n",
    "                                                    agg_func = np.mean)\n",
    "            self.transformed_annot = MakeArrayBorders(transformed_annot)\n",
    "        \n",
    "        #%% transform projection density and nissl template\n",
    "        rgb_annot     = cmap_mask(self.transformed_annot, bytes = True)\n",
    "        trs_pd        = cm1.transform(self.pd, agg_func = np.max)\n",
    "        if self.whole_pd is not None:\n",
    "            trs_original = cm1.transform(self.whole_pd, agg_func = np.mean)\n",
    "            trs_pd       = trs_pd/(1.0*np.max(trs_original))\n",
    "        elif len(np.unique(self.pd2)) > 1:  \n",
    "            trs_pd2      = cm1.transform(self.pd2, agg_func = np.max)\n",
    "            max1         = np.max(self.pd)\n",
    "            max2         = np.max(self.pd2)\n",
    "            if max1 > 5*np.std(self.pd): max1 = np.max(trs_pd)\n",
    "            if max2 > 5*np.std(self.pd2): max2 = np.max(trs_pd2)\n",
    "            trs_pd       = trs_pd/(1.0*max(max1,max2))    \n",
    "        else:    \n",
    "            max1   = np.max(trs_pd)\n",
    "            if max1 > 5*np.std(self.pd): max1 = np.max(trs_pd)\n",
    "            trs_pd = trs_pd/(1.0*max1)\n",
    "            \n",
    "        if mode == 'binary':\n",
    "            trs_pd[trs_pd <= 0.5] = 0\n",
    "            trs_pd[trs_pd > 0.5]  = 1\n",
    "            \n",
    "        trs_mask      = cm1.transform(self.template, agg_func = np.mean)\n",
    "        \n",
    "        #%% convert projection density and nissl template to rbg images\n",
    "        rgb_mask      = cmap_mask(trs_mask, bytes = True)\n",
    "        rgb_pd        = cmap_pd(trs_pd, bytes = True)\n",
    "        \n",
    "        #%% mix rgb images - projection density with nissl template and annotation\n",
    "        if self.state == 'union':\n",
    "            # gray background\n",
    "            pdZero = trs_mask == 0\n",
    "            mask_and_pd = rgb_pd.copy()\n",
    "            mask_and_pd[:,:,0][pdZero] = 128\n",
    "            mask_and_pd[:,:,1][pdZero] = 128\n",
    "            mask_and_pd[:,:,2][pdZero] = 128\n",
    "            nonzeroBorders = self.transformed_annot > 0\n",
    "            \n",
    "            for i in range(np.shape(mask_and_pd)[2]):\n",
    "                mask_and_pd[:,:,i][nonzeroBorders] = 190\n",
    "            self.out_image  = mask_and_pd\n",
    "            \n",
    "        elif self.state == 'raw': \n",
    "            annot_pd                = np.maximum(rgb_pd,rgb_annot)\n",
    "            annot_mask_pd           = np.maximum(annot_pd,rgb_mask)\n",
    "            self.out_image          = np.maximum(rgb_pd,rgb_mask) #annot_mask_pd\n",
    "         \n",
    "        #%% Save the files\n",
    "        def format_func(value, tick_number):\n",
    "            return str(value * 0.01)\n",
    "        \n",
    "        fig = plt.figure(figsize=(15, 6))\n",
    "        ax   = plt.axes()\n",
    "        ax.xaxis.set_major_formatter(plt.FuncFormatter(format_func))\n",
    "        ax.yaxis.set_major_formatter(plt.FuncFormatter(format_func))\n",
    "        N = np.shape(self.out_image)[0]\n",
    "        pos = ax.imshow(self.out_image[150:N-230,:,:])\n",
    "        if savefile is not None: plt.savefig(savefile)\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "        if len(np.unique(self.pd2)) > 1:\n",
    "            # compute the maximum intensity projection (along the anterior-posterior axis) of the projection data\n",
    "            \n",
    "            if self.whole_pd2 is not None:\n",
    "                trs_original = cm1.transform(self.whole_pd2, agg_func = np.mean)\n",
    "                trs_pd2      = trs_pd2/(1.0*np.max(trs_original))\n",
    "            else:    \n",
    "                trs_pd2  = trs_pd2/(1.0*max(max1,max2))  \n",
    "                \n",
    "            if mode == 'binary':\n",
    "                trs_pd2[trs_pd2 <= 0.5] = 0\n",
    "                trs_pd2[trs_pd2 > 0.5]  = 1   \n",
    "                \n",
    "            rgb_pd                  = cmap_pd(trs_pd2, bytes = True)\n",
    "            #%% mix rgb images - projection density with nissl template and annotation\n",
    "            if self.state == 'union':\n",
    "                # blue background\n",
    "                mask_and_pd = rgb_pd.copy()\n",
    "                mask_and_pd[:,:,0][pdZero] = 128\n",
    "                mask_and_pd[:,:,1][pdZero] = 128\n",
    "                mask_and_pd[:,:,2][pdZero] = 128\n",
    "                for i in range(np.shape(mask_and_pd)[2]):\n",
    "                    mask_and_pd[:,:,i][nonzeroBorders] = 190\n",
    "                 \n",
    "                out_image = mask_and_pd\n",
    "            elif self.state == 'raw': \n",
    "                annot_pd                = np.maximum(rgb_pd,rgb_annot)\n",
    "                annot_mask_pd           = np.maximum(annot_pd,rgb_mask)\n",
    "                out_image  = annot_mask_pd\n",
    "\n",
    "            #%% Save the files\n",
    "            def format_func(value, tick_number):\n",
    "                return str(value * 0.01)\n",
    "\n",
    "            plt.figure(figsize=(15, 6))\n",
    "            ax   = plt.axes()\n",
    "            ax.xaxis.set_major_formatter(plt.FuncFormatter(format_func))\n",
    "            ax.yaxis.set_major_formatter(plt.FuncFormatter(format_func))\n",
    "            N = np.shape(out_image)[0]\n",
    "            ax.imshow(out_image[150:N-230,:,:])\n",
    "            #ax.imshow(out_image)\n",
    "            if savefile is not None: plt.savefig(savefile.split('.')[0] + '_pred.jpg')\n",
    "            plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BrainPlotter class \n",
    "### Call_SBA function\n",
    "### author: Rembrandt Bakker, Nestor Timonidis\n",
    "### description: \n",
    "Thi function serves as an bridge between the CCP tool and the SBA composer tool. The data of choice are being converted to nifti format and its url is being converted into a command that can be given to the SBA tool for the visualization process to occur.\n",
    "* #### Input: \n",
    "    *  InputData = the input data for visualization.\n",
    "    *  mode      = two possible modes for the data to be converted are possible: json mode leads to the creation of a json file, usually for analysis statistics, while nifti leads to the creation of a nifti volume. json mode implies that the input data are in proper dictionary form, while nifti implies that the data are in a 3D array form.\n",
    "* #### Output: \n",
    "    *  sbaCommand = the command produced by the function that can be given to SBA as input through the sbaInterface_class interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrainPlotter(BrainPlotter):  \n",
    "    \n",
    "    def Call_SBA(self, InputData, mode):\n",
    "            \n",
    "            if isinstance(InputData,dict) == False and mode == 'json':\n",
    "                print 'Wrong data format for the specified mode'\n",
    "                return - 1\n",
    "            \n",
    "            if isinstance(InputData,np.ndarray) == False and mode == 'nifti': \n",
    "                print 'Wrong data format for the specified mode'\n",
    "                return - 1\n",
    "            \n",
    "            if mode == 'nifti':\n",
    "                nifti_url    = Save2Nifti(InputData, affine_scale = self.resolution*0.001)\n",
    "                with open(nifti_url, \"rb\") as fp:\n",
    "                    sbaCommand = {\n",
    "                                    \"method\":\"Composer.import\",\n",
    "                                    \"params\": {\n",
    "                                      \"name\": nifti_url.split('.')[0] + '.bas{sba.ABA_v3^corner,PIR,mm}.nii.gz', \n",
    "                                      \"encoding\": 'base64',\n",
    "                                      \"contents\": base64.b64encode(fp.read()).decode('utf-8')\n",
    "                                    }\n",
    "                                  }\n",
    "            if mode == 'json':\n",
    "                if 'bas' in InputData.keys() and 'markers' in InputData.keys() and 'style' in InputData.keys():\n",
    "                    sbaCommand = {\n",
    "                        \"method\":\"Composer.scatter3d\",\n",
    "                        'params' : InputData\n",
    "                    }\n",
    "                else:\n",
    "                    print 'Error! the input is not in a proper json form. Please call the Convert2JSON function and repeat.'\n",
    "                    return -1\n",
    "                \n",
    "            return sbaCommand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FromUnion2Voxel function\n",
    "### Author: Nestor Timonidis   \n",
    "### Description:   \n",
    "This function maps the projection patterns that are defined by a given tract tracing experiment across anatomically distinct brain areas (unionized space), to a volumetric 3D space as specified by an annotation volume provided by the Allen Institute.\n",
    "*        #### Input:  \n",
    "         * Annotation = the Annotation identities of the structures in the volumetric space, as provided by the Allen Institute.\n",
    "         * struct_idx_dict = dictionary that maps identities of anatomically distinct structures, as provided by the Allen Institute, to their position in a unionized space, meaning a 2D space of anatomical structures and brain related experiments.\n",
    "         * ConStr = the tract tracing experiment defined in the unionized space, measured by normalized projection density.\n",
    "         * Pred_ConStr = the equivalent tract tracing experiment as ConStr, but with values being predicted by the the gene expression based predictive model (MesoconnectomePredictor object).\n",
    "         * rem_ind = In case that the input projection array contains less target brain areas than the complete ~ 1300, a list is required to map the reduced elements to their original space of ~1300 elements. Default = None. \n",
    "*        #### Output:   \n",
    "         * UnionToVoxel      =  a 3D array containing the tracing data of the original experiment mapped to the volumetric space.\n",
    "         * UnionToVoxel_pred =  a 3D array containing the tracing data of the predicted experiment mapped to the volumetric space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FromUnion2Voxel(Annotation, struct_idx_dict, ConStr, Pred_ConStr = None, rem_ind = None):\n",
    "    \n",
    "    if Pred_ConStr == None:\n",
    "        Pred_ConStr = np.zeros(np.shape(ConStr))\n",
    "        \n",
    "    original_shape       = Annotation.shape\n",
    "    Annot_flat           = np.ndarray.flatten(Annotation, order = 'C')\n",
    "    UnionToVoxel         = np.zeros(np.shape(Annot_flat), dtype = np.float32)\n",
    "    UnionToVoxel_pred    = np.zeros(np.shape(Annot_flat), dtype = np.float32)\n",
    "  \n",
    "    if rem_ind != None: \n",
    "        rem_ind  = np.asarray(rem_ind) \n",
    "        struct_idx_dict_cp = struct_idx_dict.copy()\n",
    "        for key,value in struct_idx_dict.items():\n",
    "            if value not in rem_ind:\n",
    "               del struct_idx_dict_cp[key]\n",
    "        struct_idx_dict = struct_idx_dict_cp  \n",
    "    else: \n",
    "        rem_ind  = np.asarray([idx for idx in range(len(ConStr))])\n",
    "     \n",
    "    for area_id,area_num in struct_idx_dict.items():\n",
    "        area_mask                    = Annot_flat == int(area_id)\n",
    "        area_num_reind               = rem_ind    == area_num\n",
    "        union_values                 = ConStr[area_num_reind]\n",
    "        union_values2                = Pred_ConStr[area_num_reind]  \n",
    "        UnionToVoxel[area_mask]      = union_values\n",
    "        UnionToVoxel_pred[area_mask] = union_values2\n",
    "    \n",
    "    UnionToVoxel      = np.reshape(UnionToVoxel, original_shape, order = 'C')\n",
    "    UnionToVoxel_pred = np.reshape(UnionToVoxel_pred, original_shape, order = 'C')\n",
    "    \n",
    "   \n",
    "    return UnionToVoxel, UnionToVoxel_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BinarizeTheVector function\n",
    "### Author:   Nestor Timonidis    \n",
    "### Description:   \n",
    "Given an input vector with\n",
    "continuous values, the function binarizes\n",
    "its values based on a threshold value.     \n",
    "* ### Input:     \n",
    "     * input_vec = the vector to be binarized,  \n",
    "     * binThreshold = the desired ratio between the positive and negative class    \n",
    "* ### Output:   \n",
    "    * y = the binarized vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BinarizeTheVector(input_vec,binThreshold):\n",
    "    Q = np.percentile(input_vec, 100-binThreshold, axis = 0)\n",
    "    y = np.zeros(np.shape(input_vec))\n",
    "    for idx,val in enumerate(input_vec):\n",
    "        if input_vec[idx] > Q:\n",
    "            y[idx] = 1   \n",
    "    return y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CustomCrossval function\n",
    "### Author:   Nestor Timonidis    \n",
    "### Description:   \n",
    "A custom implementation of the cross-validation method, that validates the performance of a supervised learning model by partitioning into disjoint training and testing sets and testing each test set by the model trained by the train set. The difference of this implementation with the custom ones provided by sklearn is that this implementation stores a set of additional parameters at once, as described by the output section. the actual and the predicted values in corresponding lists, stores class , the model parameters and the model coefficients assigned to the trained features based on averaging over the training sets.    \n",
    "* ### Input:    \n",
    "     * mdl = the supervised learning model for utility.\n",
    "     * X = the input dataset or set of independent variables.   \n",
    "     * y = the dataset labels or dependent variables.\n",
    "     * cv = the cross validation method as defined by sklearn. \n",
    "* ### Output:   \n",
    "    * probs = assignment probabilities in case of support by the model.\n",
    "    * preds = list with all predicted data points, assigned to the list based on their corresponding index in the actual data.\n",
    "    * trues = the list of the actual samples, on one to one correspondance with the preds list.\n",
    "    * mdl_list = storage of all model parameters.\n",
    "    * mean_coeffs = the coefficients attributed to the data features, averaged across all training sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CustomCrossval(mdl,X,y,cv):\n",
    "    \n",
    "    probs = np.zeros((np.shape(y)[0],2)); preds = np.zeros((np.shape(y)));\n",
    "    trues = np.zeros((np.shape(y)))\n",
    "    coeffs = []; mdl_list = []\n",
    "    for train, test in cv.split(X,y):\n",
    "        mdl.fit(X[train],y[train])\n",
    "        mdl_list.append(mdl)\n",
    "        if hasattr(mdl, 'best_estimator_'):\n",
    "            if hasattr(mdl.best_estimator_, 'coef_'):\n",
    "                coeffs.append(mdl.best_estimator_.coef_)\n",
    "            elif hasattr(mdl.best_estimator_, 'feature_importances_'):\n",
    "                coeffs.append(mdl.best_estimator_.feature_importances_)\n",
    "        if hasattr(mdl, 'predict_proba'):        \n",
    "            probs[test,:] = mdl.predict_proba(X[test])\n",
    "        preds[test] = mdl.predict(X[test])\n",
    "        trues[test] = y[test]\n",
    "    mean_coeffs = np.mean(coeffs,0)        \n",
    "    return probs, preds, trues, mdl_list, mean_coeffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RemoveNanStructs class\n",
    "### Author:   Nestor Timonidis    \n",
    "### Description:   \n",
    "Given a dataset as input, this function removes columns or rows containing NaN (not-a-number), based on whether the percentage of NaN exceeds a given threshold.\n",
    "*   ### functions:   \n",
    "       *        #### init:    \n",
    "                *  #### description:  \n",
    "                initializes a RemoveNanStructs class object by selecting the default dimension of the dataset for NaN scanning, and the threshold for exclusion.\n",
    "                *  #### input:  \n",
    "                      *  dim = dimension for selection.\n",
    "                      *  nan_thr = the exclusion threshold.\n",
    "       *        ####   fit:   \n",
    "                * #### description:   \n",
    "                Performs the exclusion of rows or columns as described above.\n",
    "                 *  #### input:  \n",
    "                       * X = the dataset to be processed.\n",
    "\n",
    "                 *  #### output:  \n",
    "                       * X = the dataset after being processed.\n",
    "                       * nan_list = list containing the rows or columns that were excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemoveNanStructs():\n",
    "    __slots__ = ['dim','nan_thr']\n",
    "    def __init__(self, dim = 0, nan_thr = 0.1):\n",
    "        self.nan_thr = nan_thr\n",
    "        self.dim = dim\n",
    "    def fit(self,X):\n",
    "        nan_list = []\n",
    "        if self.dim == 0:    #row\n",
    "            for idx,row in enumerate(X):\n",
    "                nancols = [col for col in row if mt.isnan(col) == True]\n",
    "                if len(nancols) >= len(row)*self.nan_thr: # row for deletion spotted\n",
    "                    nan_list.append(idx)\n",
    "        elif self.dim == 1:   #column\n",
    "            for idy in range(len(X[0])):\n",
    "                nanrows = [X[idx,idy] for idx in range(len(X)) if mt.isnan(X[idx,idy]) == True]\n",
    "                if len(nanrows) >= len(X[0])*self.nan_thr: # column for deletion spotted\n",
    "                    nan_list.append(idy)\n",
    "        X = np.delete(X,nan_list,self.dim)\n",
    "        return X,nan_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BimodalImputation class\n",
    "### Author:   Nestor Timonidis    \n",
    "### Description:   \n",
    "Given a vector that is partitioned into two classes, imputes its missing values by sampling from the two classes according to their distribution.   \n",
    "*   ### functions:   \n",
    "       *        #### init:    \n",
    "                *  #### description:  \n",
    "                initializes a BimodalImputation class object by selecting the default class for selection as defined by mode.\n",
    "                *  #### input:  \n",
    "                      *  mode =  the default class for selection. Default = 0.\n",
    "                     \n",
    "       *        ####   fit:   \n",
    "                * #### description:   \n",
    "                Performs the bimodal imputation as described above.\n",
    "                 *  #### input:  \n",
    "                       * y = the vector for imputation.\n",
    "\n",
    "                 *  #### output:  \n",
    "                       * y = the imputed vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BimodalImputation():\n",
    "    __slots__ = ['mode']\n",
    "    def __init__(self, mode = 0):\n",
    "         self.mode = mode\n",
    "    def fit(self, y):\n",
    "        modality = {}\n",
    "        nan_poses = [idx for idx,val in enumerate(y) if mt.isnan(val) == True]\n",
    "        modality[0] = [idx for idx,val in enumerate(y) if val == self.mode]\n",
    "        modality[1] = [idx for idx,val in enumerate(y) if val != self.mode and mt.isnan(val) == False]\n",
    "        freq    = np.zeros((2,1))\n",
    "        freq[0] = len(modality[0])/float(len(y))\n",
    "        freq[1] = len(modality[1])/float(len(y))\n",
    "        max_pos = [idx for idx in range(2) if freq[idx] == max(freq)][0]\n",
    "        min_pos = [idx for idx in range(2) if freq[idx] == min(freq)][0]\n",
    "\n",
    "        for nan in nan_poses:   # rinse and repeat for all nans\n",
    "            # Draw a dice\n",
    "            dice = np.random.random_sample()\n",
    "            if dice <= min(freq):\n",
    "               rnd_sl = np.random.randint(low = 0, high = len(modality[min_pos]))\n",
    "               sample = modality[min_pos][rnd_sl]\n",
    "            else:\n",
    "               rnd_sl = np.random.randint(low = 0, high = len(modality[max_pos]))\n",
    "               sample = y[modality[max_pos][rnd_sl]]\n",
    "             \n",
    "            y[nan] = sample\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save2Nifti function \n",
    "### Author: Nestor Timonidis    \n",
    "### Description:\n",
    "This function takes as input volumetric data that converts to nifti volume, as well as converts the volume to url in order to be visualized in the Scalable Brain Composer 3D brain visualization tool.   \n",
    "* ### Input:  \n",
    "    * brainData = the voxelized gridded matrix - contains predictions of the connection between each grid and a number of target brain areas. \n",
    "    * affine_scale = the scale to be used for the affine transformation matrix. Default = 0.2 for 200 micrometer resolution data.\n",
    "    * outfile = the savefile of the nifti volume. Default = 'volume_density'\n",
    "* ### Output:    \n",
    "    * nifti_url = the urls of the produced nifti volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Save2Nifti(brainData, affine_scale = 0.2, outfile = 'volume_density'):\n",
    "    \n",
    "    print 'Saving results to nifti form ............... \\n'\n",
    "    Affine = np.eye(4)\n",
    "    for row in range(3):\n",
    "        Affine[row][row] = affine_scale\n",
    "     \n",
    "    density_img = nib.Nifti1Image(brainData, Affine)\n",
    "    outfile_revisited = str(outfile) + '.nii.gz'\n",
    "    nib.save(density_img, outfile_revisited)\n",
    "    nifti_url = outfile_revisited\n",
    "       \n",
    "    print 'Finished converting nifti files to url form ............... \\n'    \n",
    "    return nifti_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## render_mpl_table function\n",
    "### author: Nestor Timonidis\n",
    "### description:   \n",
    "This function renders for visualizing data in a table form "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_mpl_table(data, col_width = 13.0, row_height = 0.625, font_size=14,\n",
    "                     header_color='#40466e', row_colors=['#f1f1f2', 'w'], edge_color='w',\n",
    "                     bbox=[0, 0, 1, 1], header_columns=0,\n",
    "                     ax=None, **kwargs):\n",
    "    if ax is None: \n",
    "        size = (np.array(data.shape[::-1]) + np.array([0, 1])) * np.array([col_width, row_height])\n",
    "        fig, ax = plt.subplots(figsize=size)\n",
    "        ax.axis('off')\n",
    "\n",
    "    mpl_table = ax.table(cellText=data.values, bbox=bbox, colLabels=data.columns, **kwargs)\n",
    "\n",
    "    mpl_table.auto_set_font_size(False)\n",
    "    mpl_table.set_fontsize(font_size)\n",
    "\n",
    "    for k, cell in  six.iteritems(mpl_table._cells):\n",
    "        cell.set_edgecolor(edge_color)\n",
    "        if k[0] == 0 or k[1] < header_columns:\n",
    "            cell.set_text_props(weight='bold', color='w')\n",
    "            cell.set_facecolor(header_color)\n",
    "        else:\n",
    "            cell.set_facecolor(row_colors[k[0]%len(row_colors) ])\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MesoconnectomePredictor class\n",
    "### Author: Nestor Timonidis\n",
    "### Description:   \n",
    "This class nests a set of function that are involved in the mesoconnectome prediction pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### init function\n",
    "### Description:   \n",
    "   initalizes a MesoconnectomePredictor object by specifying the gene expression (GeneExp) and connectivity strength (ConStr) matrices for analysis.\n",
    "    A series of data structures that will be used in the analysis are being loaded and update the MesoconnectomePredictor elements (targetprofiles, params, models, ConDict). The data are being loaded for reasons of time efficiency.  The Classes_and_Modules notebook provides comments and descriptions for the functions in which the data were created.     \n",
    "    The models element stores default or user-given information regarding the regression models that will be used throughout the analysis, their parameters and the evaluation method. The default setup includes Ridge Regression and Random Forest Regressor as regressors, 3-fold cross-validation for generalization performance evaluation and grid-search cv as a method for optimizing the models' hyperparameters.  \n",
    " The ConDict is a dictionary that stores connectivity patterns over sets of different tract tracing experiments, as well as additional metadata such as layer profiles and acronyms of the source brain areas.      \n",
    " The params dictionary stores information which will be used throughout the analysis.\n",
    " It will be passed as a parameter through the functions and updated at various steps.  \n",
    " The leaf_keys contain information about target areas that are present on the finest possible level of description. For instance areas called VISp (primary visual area) and VISp l1 (primary visual area, layer 1) could both be found in the dataset, and therefore the leaf_keys are being used to filter out VISp as a non-leaf key in the structure hierarchy since a finer description is present (VISp l1).  \n",
    " The targetprofiles dictionary specializes in storage of information regarding the target mouse brain areas with regards to connectivity patterns in the mouse mesoconnectome.\n",
    " Its keys are: \n",
    "   *           str_acronym = acronyms of target areas/structures\n",
    "   *           alt laminar profiles = laminar profiles of cortical structures\n",
    "   *           father               = the brain regions in which each target belongs to   \n",
    " \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MesoconnectomePredictor:\n",
    "    \n",
    "    __slots__ = ['GeneExp','ConStr','params','models', 'targetprofiles', 'ConDict']\n",
    "    def __init__(self, GeneExp = [], ConStr = [], params = {}, \n",
    "                 targetprofiles = OrderedDict(), models = []):\n",
    "        \n",
    "        self.GeneExp = GeneExp\n",
    "        self.ConStr = ConStr\n",
    "        self.params = params\n",
    "        self.targetprofiles = targetprofiles\n",
    "        self.models = models\n",
    "        \n",
    "        self.ConDict = pk.load(open('CreLineDict.pkl','rb'))\n",
    "\n",
    "        if self.models == []:\n",
    "            \n",
    "            tol_num = 0.001\n",
    "            rfc_params = {'max_depth': [10, 60, 100, None],\n",
    "                         'max_features': ['auto', 'sqrt'],\n",
    "                         'min_samples_leaf': [1, 2, 4],\n",
    "                         'min_samples_split': [2, 5, 10],\n",
    "                         'n_estimators': [25,50,100,200],\\\n",
    "                         'oob_score': [True], 'random_state' : [None],\\\n",
    "                         'max_features' : ['auto']}\n",
    "\n",
    "            ridge_params = {'alpha': [10e-4,10e-3,10e-2,0.5,1,2.5,5,7.5,10,100],\n",
    "                          'solver' : ['svd', 'cholesky', 'lsqr', 'sparse_cg', 'saga'],\n",
    "                          'tol': [tol_num], 'max_iter' : [1000],\n",
    "                           'fit_intercept' : [True, False], 'normalize' : [True,False]}\n",
    "\n",
    "\n",
    "            self.models = [GridSearchCV(Ridge(), param_grid = ridge_params, \n",
    "                                         scoring = 'neg_mean_squared_error', \n",
    "                                         cv = 3, n_jobs = -1),\n",
    "                            RandomForestRegressor(),\n",
    "                            DummyRegressor()]\n",
    "      \n",
    "        if self.targetprofiles == OrderedDict():\n",
    "            \n",
    "            with open('structures.csv','rb') as fp:\n",
    "                structure = readtable(fp)      \n",
    "            self.targetprofiles['str_acronym'] = [val for val in structure['acronym']]\n",
    "            self.targetprofiles['alt laminar profiles'] = LaminarRegistration(targetprofiles['str_acronym'])\n",
    "            self.targetprofiles['thalamic_structures'] = pk.load(open('thalamic_structures.pkl','rb'))\n",
    "            self.targetprofiles['subcortical_structures'] = pk.load(open('subcortical_structures.pkl','rb'))\n",
    "            self.targetprofiles = self.LaminarFiltering(self.targetprofiles)\n",
    "         \n",
    "             \n",
    "        if self.params == {}:\n",
    "            leaves         = pk.load(open('leaf_nodes.pkl','rb'))\n",
    "            GeneExp        = h5py.File('G_Exp.hdf5', 'r')['dataset1']\n",
    "            GeneMeta       = pk.load(open('GeneMeta.pkl','r'))\n",
    "            GeneIds        = np.asarray([val['genes'][0]['entrez_id'] for val in GeneMeta])\n",
    "            GeneAcros      = np.asarray([val['genes'][0]['acronym'] for val in GeneMeta])\n",
    "\n",
    "            # storage of leaf-level structures\n",
    "            leaf_keys = [int(val[0]) for val in leaves]  \n",
    "            self.params = {'alter' : None, 'fetch' : 'all', \n",
    "                           'structure-abbrev' : self.ConDict['wild_type']['structure-abbrev'],\\\n",
    "                          'acronyms': targetprofiles['str_acronym'],\\\n",
    "                          'method': 'regression', 'leaf': True, \\\n",
    "                          'validation' :  KFold(n_splits = 3, shuffle = True),\\\n",
    "                          'prefix': 'Paper4/',\\\n",
    "                          'Gene Acronyms Original': GeneAcros, \n",
    "                          'Gene Ids Original': GeneIds,\n",
    "                          'leaf_keys' : leaf_keys}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MesoconnectomePredictor class\n",
    "### GetLayerResolvedArray function\n",
    "### Author: Nestor Timonidis\n",
    "### Description:  \n",
    "This function takes as input a set of anterograde tracer lines with descriptions of their layer and cell-type specific profiles, and constructs a unionized 3D connectivity array, defined by target brain area x source brain area x layer - cell type profile.   \n",
    "The normalized connection density measure is being used for quantifying connections between areas, that corresponds to the connectivity strength between two areas normalized by their injection and projection volume.\n",
    "The model used in this analysis is the RegionalizedModel taken from the mouse connectivity models (MCM) tool of the Allen Institute for Brain Science. The link for the documentation of the model can be found here:   \n",
    "https://mouse-connectivity-models.readthedocs.io/en/latest/modules/generated/mcmodels.models.voxel.RegionalizedModel.html\n",
    "* ### Input:\n",
    "    *   mode      = two possible modes for an array mode. voxelized or unionized depending of the interest is on the level of areas or on the level of voxels.\n",
    "    *   cre_file  = the input file given by the user that contains information about the cre-lines that will be included in the model. Default option is the supplementary table from the Harris et al paper (2018). \n",
    "    *   creFilter = a filter for cre lines that will be included in the model. Valid options are:\n",
    "        * True  = the default case at which the 15 tracer lines used by Harris et al (2018) shall be included.\n",
    "        * False = all cre-lines of the input file shall be included.\n",
    "        * list  = any valid python list given by the user with cre-line names as described in the Allen Brain Atlas webpage:   http://connectivity.brain-map.org/transgenic  \n",
    "    *     InputDict = dictionary containing user-given data for connectivity array construction. Must contain a 2D projection array in one of the InputDict keys, in order to be used as input to the MCM.\n",
    "    *     rem_ind = In case that the input projection array contains less target brain areas than the complete ~ 1300, a list is required to map the reduced elements to their original space of ~1300 elements. Default = None. \n",
    "    *     y_preds = key to look at InputDict in order to obtain the array of interest for analysis.\n",
    "    *     resolution = the desired resolution for mapping in volumetric space in case that an InputDict has been provided.\n",
    "* ### Output:\n",
    "    *        layer_resolved_array = the 3D layer and cell-type resolved array.\n",
    "    *        model_meta = metadata of the newly created matrix, such as source and target area acronyms and the layer profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MesoconnectomePredictor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-cc55f1624741>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mMesoconnectomePredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMesoconnectomePredictor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     def GetLayerResolvedArray(self, mode = 'unionized', \n\u001b[1;32m      4\u001b[0m                               \u001b[0mcre_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Supplementary Table 1.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                               \u001b[0mcreFilter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MesoconnectomePredictor' is not defined"
     ]
    }
   ],
   "source": [
    "class MesoconnectomePredictor(MesoconnectomePredictor):\n",
    "\n",
    "    def GetLayerResolvedArray(self, mode = 'unionized', \n",
    "                              cre_file = 'Supplementary Table 1.csv', \n",
    "                              creFilter = True,\n",
    "                              InputDict = None, \n",
    "                              rem_ind = None,\n",
    "                              data_type = 'y_preds',\n",
    "                              resolution = 200):\n",
    "        \n",
    "        if mode != 'unionized' and mode != 'voxelized':\n",
    "                print 'Error! wrong mode: please select unionized or voxelized.'\n",
    "                return -1\n",
    "\n",
    "        if InputDict is not None and rem_ind is None:\n",
    "            print 'Error! Please provide an index list for the input dictionary target structures' \n",
    "            return -1\n",
    "\n",
    "        if creFilter == True:\n",
    "            # Default filtering of cre-lines based on the 15 tracer lines used by Harris et al 2018\n",
    "            creFilter = ['Syt6-Cre_KI148', 'Ntsr1-Cre_GN220', 'Sim1-Cre_KJ18',\n",
    "                        'Efr3a-Cre_NO108', 'Chrna2-Cre_OE25', 'A93-Tg1-Cre',  \n",
    "                        'Tlx3-Cre_PL56', 'Rbp4-Cre_KL100', 'Rorb-IRES2-Cre',\n",
    "                        'Scnn1a-Tg3-Cre', 'Nr5a1-Cre', 'Sepw1-Cre_NP39',\n",
    "                        'C57BL/6J', 'Emx1-IRES-Cre', 'Cux2-IRES-Cre'] \n",
    "\n",
    "\n",
    "        creDict = {}\n",
    "        with open(cre_file) as fp:\n",
    "            buff = csv.reader(fp)\n",
    "            for idx,row in enumerate(buff):\n",
    "                 if len(row) > 1: # concatenate the two rows together - error caused by csv transition\n",
    "                    row[0] =  row[0] + row[1]\n",
    "                 remains = [x for x in filter(None, row[0].split(';'))]\n",
    "                 if remains[0].isdigit():\n",
    "                    if remains[1] in creFilter or creFilter == False:\n",
    "                        if 'A93' in remains[1]:         # name incoherence between the default creFilter and the cre_file for A93\n",
    "                            remains[1] = 'A930038C07Rik-Tg1-Cre'\n",
    "                        if '-' in remains[7]:           #  covers layers 2-6 and therefore is layer inspecific\n",
    "                            profile = 'layer inspecific'\n",
    "                        else:    \n",
    "                            profile = remains[7] + ' ' + remains[8]\n",
    "                        if profile not in creDict:\n",
    "                            creDict[profile] = []\n",
    "                            creDict[profile].append(remains[1])\n",
    "                        else:                \n",
    "                            creDict[profile].append(remains[1])\n",
    "         \n",
    "        cache = core.VoxelModelCache(manifest_file= 'voxel_model_manifest.json',\n",
    "                                     ccf_version = 'annotation/ccf_2016/')\n",
    "        layer_resolved_array = []\n",
    "        model_meta = {'layer profiles': [], 'source id': [], 'target id': [], 'model' : []}\n",
    "        model_meta['layer profiles'] = creDict.keys()\n",
    "        BP_200 = BrainPlotter(resolution = resolution)\n",
    "\n",
    "        for cur_profile in creDict.keys():\n",
    "            print 'Current analyzed cre lines are: {} -> {}'.format(cur_profile, creDict[cur_profile])\n",
    "\n",
    "            # gather injection and projection data from all tracing experiments\n",
    "            # belonging to the selected cre-line\n",
    "            \n",
    "            cre_input = creDict[cur_profile]\n",
    "            if InputDict is not None:\n",
    "                struct_ids = None\n",
    "                \n",
    "            else:\n",
    "                struct_ids = [315]\n",
    "\n",
    "            tracing_data = cache.get_experiment_data(cre = cre_input,\n",
    "                                                     injection_hemisphere_id  = 3,\n",
    "                                                     projection_hemisphere_id = 3,\n",
    "                                                     flip_experiments = True,\n",
    "                                                     injection_structure_ids  = struct_ids)\n",
    "\n",
    "            source_voxels  = tracing_data.injection_mask.coordinates\n",
    "            injections_64  = np.asarray(tracing_data.injections, dtype = np.float64)\n",
    "            projections_64 = np.asarray(tracing_data.projections, dtype = np.float64)\n",
    "            source_key = tracing_data.injection_mask.get_key()\n",
    "            target_key = tracing_data.projection_mask.get_key() \n",
    "             \n",
    "            if InputDict is not None:\n",
    "                annot_name = 'annotation_{}.nrrd'.format(resolution)\n",
    "                tracer_size = 0\n",
    "                concat_mat = []\n",
    "                for idx in range(len(creDict[cur_profile])):\n",
    "                    if 'A93' in creDict[cur_profile][idx]: creDict[cur_profile][idx] = 'A93-Tg1-Cre'\n",
    "                    if 'C57BL/6J' == creDict[cur_profile][idx]: continue  \n",
    "                    tracer_size += np.shape(InputDict[creDict[cur_profile][idx]][data_type])[1]\n",
    "                    if concat_mat == []:\n",
    "                        concat_mat = np.asarray(InputDict[creDict[cur_profile][idx]][data_type])\n",
    "                    else:    \n",
    "                        new_mat    = np.asarray(InputDict[creDict[cur_profile][idx]][data_type])\n",
    "                        concat_mat = np.concatenate((concat_mat,new_mat), axis = 1)\n",
    "                    \n",
    "                Annotation, annot_info = nrrd.read(annot_name)\n",
    "                old_shape  = (np.shape(Annotation)[0],np.shape(Annotation)[1],\n",
    "                              np.shape(Annotation)[2],\n",
    "                              tracer_size)\n",
    "                voxelized_200 = np.zeros((old_shape))\n",
    "                for j in range(tracer_size):\n",
    "                    BP_200.fit(concat_mat[:,j],\n",
    "                               chosen_indices = rem_ind)\n",
    "                    voxelized_200[:,:,:,j] = BP_200.pd\n",
    "                new_shape = (old_shape[0]*old_shape[1]*old_shape[2],old_shape[3])\n",
    "                target_key = Annotation.reshape((new_shape[0],1))\n",
    "                projections_64 = voxelized_200.reshape(new_shape).transpose()\n",
    "\n",
    "            # create a voxel-scale model of the mouse connectome given the injection and projection data\n",
    "            voxelModel    = models.VoxelModel(source_voxels)\n",
    "            voxelModel.fit((tracing_data.centroids, injections_64), projections_64)\n",
    "\n",
    "            # generate a voxel-scale connectivity array based on the fit model\n",
    "            voxel_array = models.voxel.VoxelConnectivityArray.from_fitted_voxel_model(voxelModel)\n",
    "            \n",
    "            with open('structures.csv','rb') as fp:\n",
    "                structure       = readtable(fp) \n",
    "                struct_idx_dict = {val:idx for idx,val in enumerate(structure['id'])}\n",
    "            \n",
    "            if mode == 'unionized':\n",
    "                regional_model = models.voxel.RegionalizedModel.from_voxel_array(\n",
    "                                 voxel_array, source_key, target_key, dataframe = True)\n",
    "                temporary      = regional_model.normalized_connection_density.T\n",
    "                model_meta['model'].append(regional_model) \n",
    "                model_meta['source id'] = list(temporary.columns)\n",
    "                model_meta['target id'] = list(temporary.index)\n",
    "                layer_resolved_array.append(np.asarray(temporary))  \n",
    "                 \n",
    "            elif mode == 'voxelized':\n",
    "                model_meta['source id'] = list(source_key)\n",
    "                model_meta['target id'] = list(target_key)\n",
    "                layer_resolved_array.append(voxel_array.T)\n",
    "                \n",
    "        # creation of a 3D array serving our laminarly resolved connectivity array    \n",
    "        layer_resolved_array = np.asarray(layer_resolved_array, dtype = np.float32)\n",
    "        model_meta['target id'] = [self.targetprofiles['str_acronym'][struct_idx_dict[str(val)]] for val in model_meta['target id']]\n",
    "        model_meta['source id'] = [self.targetprofiles['str_acronym'][struct_idx_dict[str(val)]] for val in model_meta['source id']]\n",
    "    \n",
    "        return layer_resolved_array, model_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MesoconnectomePredictor class\n",
    "### DictionaryDecomposition function\n",
    "### Author: Nestor Timonidis\n",
    "### Description:  \n",
    "This function takes as input a data structure and performs dictionary decomposition based on the Dictionary Learning and Sparse Coding algorithm proposed by (insert link here).\n",
    "* ### Input:\n",
    "    *   X        = the input data structure.\n",
    "* ### Output:\n",
    "    *       Atom = the decomposed atoms (features x independent components).\n",
    "    *       Code = the decomposed code (samples x independent components). \n",
    "    *       dlsc = the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MesoconnectomePredictor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f9bddc6e64a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mMesoconnectomePredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMesoconnectomePredictor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mDictionaryDecomposition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_comp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m'Commencing dictionary decomposition ...'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MesoconnectomePredictor' is not defined"
     ]
    }
   ],
   "source": [
    "class MesoconnectomePredictor(MesoconnectomePredictor):\n",
    "\n",
    "    def DictionaryDecomposition(self, X, n_comp = 200, y = None, x = None,\n",
    "                                alpha = 1.0, transform_alpha = None, state = None):\n",
    "        \n",
    "        print 'Commencing dictionary decomposition ...'\n",
    "        ErrorMat = []\n",
    "        alpha_zone = [0.01, 0.05, 0.1, 0.3, 0.5, 1.0, 2.0, 10]\n",
    "        comp_zone = [2, 10, 25, 50, 75, 100, 150, 200]\n",
    "       \n",
    "        dlsc_params = {'alpha': alpha_zone,\n",
    "                       'n_components': comp_zone,\n",
    "                       'fit_algorithm': ['lars', 'cd'],\n",
    "                       'transform_algorithm': ['lasso_cd', \n",
    "                                               'lars', 'omp']\n",
    "                      }\n",
    "        if y == None:\n",
    "            y = np.zeros((n_comp, len(X[0])))\n",
    "        x = np.zeros((len(X),n_comp))  \n",
    "        print np.shape(x), np.shape(y)\n",
    "        dlsc  = decomp.DictionaryLearning(n_components = n_comp, alpha = alpha, \n",
    "                                          positive_code = True, \n",
    "                                          positive_dict = True, \n",
    "                                          fit_algorithm = 'cd', \n",
    "                                          transform_algorithm = 'lasso_cd', \n",
    "                                          transform_alpha = transform_alpha,\n",
    "                                          n_jobs = -1,\n",
    "                                          dict_init = y,\n",
    "                                          code_init = x,\n",
    "                                          random_state = state)\n",
    "      \n",
    "          \n",
    "        start = time.time()\n",
    "        dlsc.fit(X)\n",
    "        Atoms = dlsc.components_\n",
    "        Code  = dlsc.transform(X)\n",
    "        sparsity = len([row for row in Atoms for col in row if col == 0])/(1.0*len(Atoms)*len(Atoms[0]))\n",
    "        X_hat = np.dot(Code,Atoms)\n",
    "        RecErr = 0.5*np.linalg.norm(X - X_hat, 'fro')\n",
    "        Code_set_len = len(Code[0])\n",
    "       \n",
    "        print 'Atoms : '   + str(Atoms.shape)\n",
    "        print 'Code : '    + str(Code.shape)\n",
    "        print 'Sparsity: {}'.format(np.median([len(Code[:,idx][Code[:,idx] > 0])/(1.0*len(Code)) for idx in range(Code_set_len)]))\n",
    "        print 'Participation: {}'.format(np.median([len(Atoms[idx,:][Atoms[idx,:] > 0])/(1.0*len(Atoms[0])) for idx in range(Code_set_len)]))\n",
    "        print 'Error: '    + str(RecErr)\n",
    "        end   = time.time()\n",
    "        print  'Elapsed time: ' + str((end - start)/60)\n",
    "        \n",
    "        return Atoms, Code, dlsc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MesoconnectomePredictor class\n",
    "### LaminarFiltering function\n",
    "### Author: Nestor Timonidis\n",
    "### Description:  \n",
    "This function takes as input data describing the acronyms of target brain areas and extracts the information relevant to their layer specificity.\n",
    "* ### Input:\n",
    "    *   targetprofiles = targetprofiles =  dictionary that specializes in storage of information regarding the target mouse brain areas with regards to connectivity patterns in the mouse mesoconnectome. \n",
    "* ### Output:\n",
    "    *   targetprofiles = the input dictionary updated with layer specific information regarding the target areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MesoconnectomePredictor(MesoconnectomePredictor):\n",
    "    \n",
    "    def LaminarFiltering(self,targetprofiles):\n",
    "        \n",
    "        other_profiles = ['polymorph layer', 'pyramidal layer', 'zonal layer',\\\n",
    "                              'optic layer', 'superficial gray', 'granule cell',\\\n",
    "                              'molecular layer', 'outer plexiform', 'glomerular layer',\n",
    "                              'mitral layer', 'granular layer', 'Purkinje layer',\\\n",
    "                              'granule layer', 'inner plexiform', 'layers 1-3', \\\n",
    "                              'layer 13', 'layer 4/5', 'layer 1-4', 'layer 5/6',\\\n",
    "                              'layer 1-2', 'layer 6', 'layer 1-3']\n",
    "\n",
    "        #** Fixing the messy strings in laminar_profiles ********#\n",
    "        key =  'alt laminar profiles'\n",
    "        for idx,prof in enumerate(targetprofiles[key]):\n",
    "            if prof.isspace(): targetprofiles[key][idx] = 'layer inspecific'\n",
    "            elif prof[0].isspace():\n",
    "                tok = targetprofiles[key][idx].split(' ')\n",
    "                targetprofiles[key][idx] = tok[1] + ' ' + tok[2]\n",
    "            if targetprofiles[key][idx] == 'layer 3' \\\n",
    "               or targetprofiles[key][idx] == 'layer 2'\\\n",
    "               or targetprofiles[key][idx] == 'layer 2a'\\\n",
    "               or targetprofiles[key][idx] == 'layer 2b'\\\n",
    "               or targetprofiles[key][idx] == 'layers 3'\\\n",
    "               or targetprofiles[key][idx] == 'layers 2':\\\n",
    "                  targetprofiles[key][idx] = 'layer 2/3'\n",
    "            elif targetprofiles[key][idx]  == 'layers 1':\\\n",
    "                  targetprofiles[key][idx] = 'layer 1'\n",
    "            elif targetprofiles[key][idx] in other_profiles:\n",
    "                 targetprofiles[key][idx] = 'other profiles'\n",
    "                    \n",
    "        return targetprofiles                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MesoconnectomePredictor class\n",
    "### PredictivePipeline function\n",
    "### Author: Nestor Timonidis\n",
    "### Description:  \n",
    "In our examples I utilize the unionized anatomical tract tracing data taken from the Allen Institute for Brain Science, as shown in the ParserLibrary notebook. The data come in the form of a dictionary with nested matrices, where each dictionary corresponds to a tracer category like wild type or Syt6-Cre_KI148 cre-line experiments for instance.   \n",
    "In order to demonstrate the MesoPred tool, a number of categories are selected as shown bellow. This analysis can take more than a day if run for all possible tracer categories. Therefore, for reasons of time efficiency one could skip this cell and move to the next one where the results can be loaded by stored pickle files.  \n",
    "In this pipeline a number of steps is being followed:  \n",
    "0.  Conversion of the gene expression data to numpy array format for computational simplicity, initialization of various dictionaries to store analysis related data and results per tracer category (key). Selection of the tracer category (driver) of interest, reduction of tracing data and gene expression data to the leaf-level structures and storage of a number of parameters to be used in the analysis, such as the layer profiles and acronyms of the target areas of the dataset.    \n",
    "1.  Preprocessing of the gene expression and tract tracing data. The function called in that step is PreProcessing\n",
    "2.  Prediction with a logistic or linear ridge regression model using the cross-validation method for performance evaluation. The function called in that step is CustomCrossval.\n",
    "3.  Prediction with a random forest classifier or regressor model using the cross-validation method for performance evaluation. The function called in that step is CustomCrossval.\n",
    "4.  Prediction with a control (baseline) model using the cross-validation method for performance evaluation. The function called in that step is CustomCrossval.\n",
    "5.  Storage of the predicted connectivity patterns, probability scores and gene importance (coefficient) scores per tracer in dictionaries stratified by the tested driver categories. The function called in that step is UnravelResults.\n",
    "6.  Based on the stored connectivity patterns and the actual (ground truth) ones, evaluation of the predictive process with series of statistical measures. Examples include r squared and root mean squared error for regression, and f1-score and area under the roc curve for classification. The function called in that step is Evaluation.\n",
    "8. Estimate the stability of the fittest model based on the optimal hyperparameters selected for each fold of the CustomCrossval function (step 3). In case that the model is considered as stable, then the model fits the whole data with the optimal hyperparameters.     \n",
    "\n",
    "### Input:\n",
    "   *   ConDict = dictionary with connectivity strength patterns per structural brain area used throughout the pipeline.\n",
    "   *   GeneExp = 2D array with gene expression values per structural brain area used throughout the pipeline.   \n",
    "   \n",
    "   \n",
    "### Output:\n",
    "   *   ClfResults_ridge = dictionary with predictive results of the main model (ridge in that instance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MesoconnectomePredictor(MesoconnectomePredictor):   \n",
    "    \n",
    "    def PredictivePipeline(self, ConDict, GeneExp):       \n",
    "        \n",
    "        GeneExp = np.asarray(GeneExp, dtype = np.float32)\n",
    "        ConStr_proc = {}; GeneExp_proc = {}; y = {}\n",
    "        # matrix reduction to leaf-level structures\n",
    "        GeneExp  = GeneExp[self.params['leaf_keys'],:] \n",
    "        for key in ConDict.keys():\n",
    "                ## Step 0:\n",
    "                # measure used not to repeat predictions that have already been done\n",
    "                outfile = key + '_all_results' \n",
    "                ConStr  = np.asarray(ConDict[key]['ConMat'], dtype = np.float32)\n",
    "                ConStr = ConStr[self.params['leaf_keys'],:]\n",
    "                self.params['injection_number'] = np.shape(ConStr)[1]  \n",
    "                self.params['cre-line'] = key\n",
    "                self.params['structure-abbrev'] = ConDict[key]['structure-abbrev']\n",
    "                self.params['layer'] = ConDict[key]['layer'][0]\n",
    "                self.params['cell-type'] = ConDict[key]['cell-type'][0]\n",
    "                self.params['method'] = 'regression'\n",
    "\n",
    "                print 'Driver line: '+ str(key) + '\\nTracer number: ' + str(ConStr.shape[1]) \n",
    "                \n",
    "                ## Step 1:\n",
    "                GeneExp_proc[key], sc_gene_exp, ConStr_proc[key], sc_sqrt = self.PreProcessing(\n",
    "                                                                GeneExp, self.params, ConStr)\n",
    "                str_copy = np.delete(self.params['acronyms'],self.params['nanCons'],0) \n",
    "                str_copy = np.delete(str_copy,self.params['nanGenes'],0)\n",
    "                pk.dump(self.params, open('updated_params.pkl','wb'))\n",
    "                start = time.time()\n",
    "                ## Step 2:\n",
    "                print 'Ridge ' + self.params['method'] +  ' predictive phase ..'    \n",
    "                ridge_pred  = list(CustomCrossval(self.models[0], GeneExp_proc[key], \n",
    "                                                     ConStr_proc[key], self.params['validation']))\n",
    "                ridge_pred.append(GeneExp_proc[key])\n",
    "                #pk.dump(ridge_pred, open(self.params['cre-line'] + '_ridge_pred.pkl', 'wb'))\n",
    "                print 'Ridge ' + self.params['method'] +  ' has been completed' \n",
    "\n",
    "                ## Step 3:\n",
    "                print 'Random forest ' + self.params['method'] + ' predictive phase ..'\n",
    "                rf_pred  = list(CustomCrossval(self.models[1], GeneExp_proc[key], \n",
    "                                                  ConStr_proc[key], self.params['validation']))\n",
    "                rf_pred.append(GeneExp_proc[key])                                  \n",
    "                #pk.dump(rf_pred, open(self.params['cre-line'] + '_rf_pred.pkl','wb'))\n",
    "                print 'Random forest ' + self.params['method'] +  ' has been completed' \n",
    "\n",
    "                ## Step 4:\n",
    "                print 'Control model ' + self.params['method'] +  ' predictive phase ..'\n",
    "                bl_pred  = list(CustomCrossval(self.models[2], GeneExp_proc[key], \n",
    "                                                  ConStr_proc[key], self.params['validation']))\n",
    "                bl_pred.append(GeneExp_proc[key])                                       \n",
    "\n",
    "                #pk.dump(bl_pred, open(self.params['cre-line'] + '_bl_pred.pkl','wb'))\n",
    "                print 'Control ' + self.params['method'] +  ' has been completed'  \n",
    "\n",
    "                end = time.time() \n",
    "                print  'Elapsed time: ' + str((end - start)/60)   \n",
    "\n",
    "                ## Step 5:\n",
    "                print 'Unraveling of results phase ..' \n",
    "                ClfResults_ridge = self.UnravelResults(ridge_pred, \n",
    "                                                       self.params, sc_sqrt, sc_gene_exp)\n",
    "                ClfResults_rf    = self.UnravelResults(rf_pred, \n",
    "                                                       self.params, sc_sqrt, sc_gene_exp)\n",
    "                #pk.dump(ClfResults_rf, open(outfile + '_rf.pkl','wb'))\n",
    "                ClfResults_bl    = self.UnravelResults(bl_pred, \n",
    "                                                       self.params, sc_sqrt, sc_gene_exp)\n",
    "                #pk.dump(ClfResults_bl, open(outfile + '_bl.pkl','wb')) \n",
    "\n",
    "                ## Step 6:\n",
    "                print 'Evaluation of results phase ..'\n",
    "                self.Evaluation(ClfResults_ridge, ClfResults_rf, ClfResults_bl, self.params) \n",
    "\n",
    "                # Step 7:\n",
    "                unanimity_thr = 0.2\n",
    "                verdict, params_to_fit = self.StabilityInvestigator(ClfResults_ridge['Model Storage'], unanimity_thr)\n",
    "                if verdict == 'Full Fit': \n",
    "                    final_model = Ridge(**params_to_fit).fit(ridge_pred[5], ridge_pred[2])\n",
    "                elif verdict == 'Partial Fit': \n",
    "                    final_model = [Ridge(**model.best_params_).fit(GeneExp_proc[key], ConStr_proc[key])\\\n",
    "                                   for model in ClfResults_ridge['Model Storage']]\n",
    "\n",
    "                ClfResults_ridge['final model'] = final_model\n",
    "                final_model = []\n",
    "                pk.dump(ClfResults_ridge, open(outfile + '_ridge.pkl','wb'))\n",
    "\n",
    "                #clear_output()\n",
    "                \n",
    "        return ClfResults_ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_mpl_table(data, col_width = 13.0, row_height = 0.625, font_size=14,\n",
    "                     header_color='#40466e', row_colors=['#f1f1f2', 'w'], edge_color='w',\n",
    "                     bbox=[0, 0, 1, 1], header_columns=0,\n",
    "                     ax=None, **kwargs):\n",
    "    if ax is None: \n",
    "        size = (np.array(data.shape[::-1]) + np.array([0, 1])) * np.array([col_width, row_height])\n",
    "        fig, ax = plt.subplots(figsize=size)\n",
    "        ax.axis('off')\n",
    "     \n",
    "    mpl_table = ax.table(cellText=data.values, bbox=bbox, colLabels=data.columns, **kwargs)\n",
    "    mpl_table.auto_set_font_size(False)\n",
    "    mpl_table.set_fontsize(font_size)\n",
    "\n",
    "    for k, cell in  six.iteritems(mpl_table._cells):\n",
    "        cell.set_edgecolor(edge_color)\n",
    "        if k[0] == 0 or k[1] < header_columns:\n",
    "            cell.set_text_props(weight='bold', color='w')\n",
    "            cell.set_facecolor(header_color)\n",
    "        else:\n",
    "            cell.set_facecolor(row_colors[k[0]%len(row_colors) ])\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MesoconnectomePredictor class\n",
    "### GOenrichment function\n",
    "### author: Paul Tiesinga, Nestor Timonidis\n",
    "### description:  \n",
    "Given a set of genes, this function performs gene ontology enrichment analysis by comparing the set with the org.Mm.eg.db mouse gene database provided by https://bioconductor.org/packages/release/data/annotation/html/org.Mm.eg.db.html. The script utilizes a nested script written in the R programming languages, using the rpy2 library\n",
    "* ### input:\n",
    "    * GeneList = the list of genes\n",
    "* ### output:\n",
    "    * BP = biological processes related to the gene list based on their statistical significance as indicated by the analysis\n",
    "    * MF = molecular functions related to the gene list based on their statistical significance as indicated by the analysis\n",
    "    * CC = cellular components related to the gene list based on their statistical significance as indicated by the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MesoconnectomePredictor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-39867e00341d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mMesoconnectomePredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMesoconnectomePredictor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mGOenrichment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGeneList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;31m# Description: given a set of selected genes, performs enrichment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m#              analysis comparing to a bioconductor database\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MesoconnectomePredictor' is not defined"
     ]
    }
   ],
   "source": [
    "class MesoconnectomePredictor(MesoconnectomePredictor):\n",
    "\n",
    "    def GOenrichment(self, GeneList):\n",
    "        # Description: given a set of selected genes, performs enrichment\n",
    "        #              analysis comparing to a bioconductor database\n",
    "        import rpy2.robjects as ro\n",
    "        from rpy2.robjects import numpy2ri\n",
    "        ro.numpy2ri.activate()\n",
    "        MGP = ro.r('''\n",
    "                GOenrichment <-function(GeneList){\n",
    "                    source(\"http://bioconductor.org/biocLite.R\")\n",
    "                    lib_path = \"~/R/x86_64-pc-linux-gnu-library/3.2/\"\n",
    "                    lib_path_2 = \"~/anaconda2/lib/R/library/org.Mm.eg.db\"\n",
    "                    #biocLite(\"ALL\")\n",
    "                    #biocLite(\"GOstats\", lib = lib_path)\n",
    "                    #biocLite(\"Category\", lib = lib_path)\n",
    "                    #biocLite(\"genefilter\", lib = lib_path)\n",
    "                    #biocLite(\"org.Mm.eg.db\", lib = lib_path_2)\n",
    "                    #biocLite(\"gage\", lib = lib_path)\n",
    "                    #biocLite(pkgs=c(\"Biobase\", \"IRanges\", \"AnnotationDbi\"),\n",
    "                     #   suppressUpdates=FALSE,\n",
    "                      #  suppressAutoUpdate=FALSE,\n",
    "                       # siteRepos=character(),\n",
    "                        #ask=TRUE, lib= lib_path)\n",
    "                    #install.packages(\"RSQLite\")\n",
    "                    #install.packages(\"devtools\")\n",
    "                    #require(devtools)\n",
    "                    #library(\"RSQLite\")\n",
    "                    #library(\"gage\")\n",
    "                    #library(\"genefilter\")\n",
    "                    #library(\"org.Mm.eg.db\")\n",
    "                    library(DBI)\n",
    "                    library(\"GO.db\")\n",
    "                    library(\"GOstats\")\n",
    "                    library(\"Category\")\n",
    "                    library(\"annotate\")\n",
    "                    data(ALL, package=\"ALL\")\n",
    "\n",
    "                    hgCutoff <- 0.01\n",
    "\n",
    "                    params <- new(\"GOHyperGParams\",\n",
    "                           geneIds = GeneList,\n",
    "                           universeGeneIds = NULL,\n",
    "                           annotation = \"org.Mm.eg.db\",\n",
    "                           ontology = \"CC\",\n",
    "                           pvalueCutoff = hgCutoff,\n",
    "                           conditional = FALSE,\n",
    "                           testDirection = \"over\")\n",
    "                    CC <- hyperGTest(params)\n",
    "                    sumCC <- data.frame(summary(CC))\n",
    "                    sumCC <- subset(sumCC, select=c(\"Term\"))\n",
    "\n",
    "                    foo <- vector(mode=\"list\", length = 3)\n",
    "                    foo[[3]] <- sumCC\n",
    "\n",
    "                    return(foo)\n",
    "            }''')\n",
    "        r_getname = ro.globalenv['GOenrichment']\n",
    "        verdict = r_getname(GeneList)\n",
    "        tmp = np.asarray(verdict[2])[0]\n",
    "        hits = [element for element in tmp if 'neur' in element or 'synap' in element]\n",
    "\n",
    "        return hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MesoconnectomePredictor class\n",
    "### LaminarEnrichment function\n",
    "### Author: Nestor Timonidis\n",
    "### Description: \n",
    "Given a list of published gene markers of cortico-laminar specificity,\n",
    "this function parses the list and compares an input list of given genes\n",
    "with the marker list, resulting in estimating frequencies of laminar\n",
    "profiles strongly enriched in the input list.\n",
    "* ### Input:\n",
    "    * gene_list = the input list of genes.\n",
    "    * infile = the gene marker list.\n",
    "* ### Output:\n",
    "    * laminar_hits = frequencies of enriched laminar profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MesoconnectomePredictor(MesoconnectomePredictor):   \n",
    "    \n",
    "    def LaminarEnrichment(self, gene_list, infile = 'Bellgard et al - ISH laminar markers.xls'):\n",
    "        \n",
    "        LaminarMarkers = {}\n",
    "        wb = xlrd.open_workbook(filename = infile)\n",
    "        ws = wb.sheet_by_index(0)\n",
    "        keywords = ws.row(0)\n",
    "        for row_id in range(1,ws.nrows):\n",
    "            gene_marker = ws.row(row_id)[0].value\n",
    "            LaminarMarkers[gene_marker] = {}\n",
    "            for key_id in range(1,7):\n",
    "                if ws.row(row_id)[key_id].value > 0.4:\n",
    "                    LaminarMarkers[gene_marker][keywords[key_id].value] = ws.row(row_id)[key_id].value\n",
    "         \n",
    "        gene_hits = [val for val in gene_list for val2 in LaminarMarkers.keys() if val.lower() == val2.lower()] \n",
    "        laminar_hits = OrderedDict([('Layers 2/3',0), ('Layer 4',0), ('Layer 5',0), ('Layer 6',0), ('Layer 6b',0)])\n",
    "        for key,value in LaminarMarkers.items():\n",
    "            if key in gene_hits:\n",
    "                for laminar_prof in value:\n",
    "                    laminar_hits[laminar_prof] = laminar_hits[laminar_prof] + 1\n",
    "        \n",
    "        return laminar_hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MesoconnectomePredictor class\n",
    "### PlotTheResults function\n",
    "### author: Nestor Timonidis\n",
    "### description:  \n",
    "This function plots and saves the results of the predictive analysis.\n",
    "* ### input:\n",
    "    * Results  = the analysis results\n",
    "    * MetaInfo = information related to the plots. Examples include the size of the plot ticks and the filename for save.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MesoconnectomePredictor(MesoconnectomePredictor):\n",
    "\n",
    "    def PlotTheResults(self,Results,MetaInfo):\n",
    "\n",
    "            k = len(MetaInfo['xtick'])\n",
    "            fsz = 9\n",
    "            fig = plt.figure(figsize=(8,6))\n",
    "\n",
    "            #plt.tight_layout()\n",
    "            plt.rcParams['figure.figsize']\n",
    "            plt.boxplot(Results, 0, 'gD', widths = 0.6, whis = [5,95])\n",
    "\n",
    "            ax = plt.gca()\n",
    "            ax.xaxis.set_tick_params(labelsize = 14)\n",
    "            ax.yaxis.set_tick_params(labelsize = 14)\n",
    "            plt.xticks([i+1 for i in range(k)], MetaInfo['xtick'],rotation = 0)\n",
    "            plt.text(0.51, MetaInfo['max1'][1], MetaInfo['max1'][0], fontsize = fsz)\n",
    "            plt.text(0.51, MetaInfo['min1'][1], MetaInfo['min1'][0], fontsize = fsz)\n",
    "            plt.text(0.51, MetaInfo['med1'][1], MetaInfo['med1'][0], fontsize = fsz)\n",
    "            plt.text(1.45, MetaInfo['max2'][1], MetaInfo['max2'][0], fontsize = fsz)\n",
    "            plt.text(1.45, MetaInfo['min2'][1], MetaInfo['min2'][0], fontsize = fsz)\n",
    "            plt.text(1.45, MetaInfo['med2'][1], MetaInfo['med2'][0], fontsize = fsz)\n",
    "            plt.text(2.45, MetaInfo['max3'][1], MetaInfo['max3'][0], fontsize = fsz)\n",
    "            plt.text(2.45, MetaInfo['min3'][1], MetaInfo['min3'][0], fontsize = fsz)\n",
    "            plt.text(2.45, MetaInfo['med3'][1], MetaInfo['med3'][0], fontsize = fsz)\n",
    "            plt.title(MetaInfo['title'], fontsize = 14)\n",
    "            # plt.xlabel(MetaInfo['xlabel'])\n",
    "            plt.ylabel(MetaInfo['ylabel'], fontsize = 14)\n",
    "            plt.yticks(np.arange(MetaInfo['lb'] - 0.1, \n",
    "                                 MetaInfo['ub'], 0.1))\n",
    "            #plt.yticks(np.arange(0.4,1.0,0.05))\n",
    "\n",
    "            plt.savefig(MetaInfo['save_file'])\n",
    "            plt.show(block = False)\n",
    "            plt.pause(1)\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MesoconnectomePredictor class\n",
    "### Evaluation function\n",
    "### author: Nestor Timonidis\n",
    "### description:  \n",
    "This function plots and saves the results of the predictive analysis.\n",
    "* ### input: \n",
    "    * ClfResults_lr = results related to a linear model (classifier or regressor).\n",
    "    * ClfResults_rf = results related to a random forest model (classifier or regressor).\n",
    "    * ClfResults_bl = results related to a control (baseline) model (classifier or regressor).\n",
    "    * params = parameters related to the evaluation of the analysis. Examples include the abbreviations of the brain structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MesoconnectomePredictor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-2a290894c6fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mMesoconnectomePredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMesoconnectomePredictor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mEvaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mClfResults_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mClfResults_rf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mClfResults_bl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeasure_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0mMetaInfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MesoconnectomePredictor' is not defined"
     ]
    }
   ],
   "source": [
    "class MesoconnectomePredictor(MesoconnectomePredictor):\n",
    "\n",
    "    def Evaluation(self, ClfResults_lr, ClfResults_rf, ClfResults_bl, params, measure_list = None):          \n",
    "          \n",
    "          MetaInfo = {}\n",
    "          templ = 'values for predicted tracer - projection patterns\\nof driver line:'\n",
    "          MetaInfo['xlabel'] = 'Classifier Models'\n",
    "          exclusions = ['Model Storage', 'Gene Scoring', 'Mean Gene Scoring',\n",
    "                       'y_preds','y_actual','Gene Expression', 'projection scaler',\n",
    "                       'gene scaler','binary_patterns']\n",
    "          \n",
    "          if measure_list is None:\n",
    "            measure_list = ClfResults_lr.keys()   \n",
    "          \n",
    "          for measure in measure_list:\n",
    "             if measure not in exclusions:\n",
    "               tmp1 = [val[0] for val in ClfResults_lr[measure]] \n",
    "               tmp2 = [val[0] for val in ClfResults_rf[measure]] \n",
    "               tmp3 = [val[0] for val in ClfResults_bl[measure]] \n",
    "               sort1 = np.argsort(tmp1)[::-1]; sort3 = np.sort(tmp1)[::-1]\n",
    "               sort2 = np.argsort(tmp2)[::-1]; sort4 = np.sort(tmp2)[::-1]\n",
    "               sort5 = np.argsort(tmp3)[::-1]; sort6 = np.sort(tmp3)[::-1]\n",
    "               set_length = len(sort1)   \n",
    "            \n",
    "               MetaInfo['min1']  = [params['structure-abbrev'][sort1[set_length-1]], sort3[set_length-1]]\n",
    "               MetaInfo['max1']  = [params['structure-abbrev'][sort1[0]], sort3[0]]\n",
    "               MetaInfo['med1']  = [params['structure-abbrev'][sort1[set_length/2]], sort3[set_length/2]]\n",
    "    \n",
    "               MetaInfo['min2']  = [params['structure-abbrev'][sort2[set_length-1]], sort4[set_length-1]]\n",
    "               MetaInfo['max2']  = [params['structure-abbrev'][sort2[0]], sort4[0]]\n",
    "               MetaInfo['med2']  = [params['structure-abbrev'][sort2[set_length/2]], sort4[set_length/2]]\n",
    "               MetaInfo['min3']  = [params['structure-abbrev'][sort5[set_length-1]], sort6[set_length-1]]\n",
    "               MetaInfo['max3']  = [params['structure-abbrev'][sort5[0]], sort6[0]]\n",
    "               MetaInfo['med3']  = [params['structure-abbrev'][sort5[set_length/2]], sort6[set_length/2]] \n",
    "               \n",
    "               MetaInfo['save_file'] = params['cre-line'] + '_' + measure +'.png'\n",
    "               MetaInfo['title']     = params['cre-line'] #measure + ' '+ templ + ' ' + params['cre-line']\n",
    "               MetaInfo['ylabel']    = measure\n",
    "               MetaInfo['xtick']     = ['Ridge Regression','Random Forest', 'Control']\n",
    "               if measure == 'AURoc': \n",
    "                  MetaInfo['lb'] = 0.4\n",
    "                  MetaInfo['ub'] = 1.0\n",
    "               elif measure == 'RMSE' or measure == 'r2': \n",
    "                  MetaInfo['lb'] = 0\n",
    "                  MetaInfo['ub'] = 1.0\n",
    "               else: \n",
    "                  MetaInfo['lb'] = 0.1 \n",
    "                  MetaInfo['ub'] = 1.0\n",
    "                \n",
    "               JoinResults = np.asarray([(a[0],b[0],c[0]) for a,b,c in zip(ClfResults_lr[measure],ClfResults_rf[measure],ClfResults_bl[measure])])\n",
    "               self.PlotTheResults(JoinResults,MetaInfo) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  MesoconnectomePredictor class\n",
    "### StabilityInvestigator function\n",
    "### Author: Nestor Timonidis\n",
    "### Description:  \n",
    "This function investigates the stability of the results provided by the nested cross-validation method that is being implemented at the CustomCrossval function.\n",
    "The optimal hyperparameters for all trained nested models are being compared for incoherence. In case of a strong agreement, it is being decided that the predictive results are stable. Otherwise, they are being considered as instable.\n",
    "* Input:\n",
    "   * model_list    = the list containing the hyperparameter values per data fold.\n",
    "   * unanimity_thr = stability is defined over common hypeparameter values between data folds. unanimity_thr is the stability threshold which define in which of the three stability states (see verdict) the model is.\n",
    "* Output:\n",
    "   * verdict = message regarding the stability of the model. The three verdicts are: Full Fit, Ensemble Fit or No Fit.\n",
    "   * params_to_fit = the output parameters over which the whole model is suggested to be trained in case of a full fit. Ensemble fit implies ensemble training over the hypeparameter set of each fold and No Fit implies no fitting at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MesoconnectomePredictor(MesoconnectomePredictor): \n",
    "    \n",
    "    def StabilityInvestigator(self, model_list, unanimity_thr):\n",
    "\n",
    "        frequency = {}; stability_set = []; params_to_fit = []\n",
    "        param_dict = [model.best_params_ for model in model_list]\n",
    "        param_list = [fold.values() for fold in param_dict]\n",
    "\n",
    "        for param in set(chain.from_iterable(param_list)):\n",
    "            frequency[param] = len([idx for idx,model_params in enumerate(param_list) if param in model_params])\n",
    "            if frequency[param] == len(param_dict): stability_set.append(param)\n",
    "\n",
    "        if len(stability_set)/(1.0*len(np.unique(param_list))) > unanimity_thr:\n",
    "            print 'model is stable, train the whole dataset with the parameters'\n",
    "            params_to_fit = param_dict[0]\n",
    "            verdict = 'Full Fit'\n",
    "        elif len(stability_set)/(1.0*len(np.unique(param_list))) > unanimity_thr - 0.2:\n",
    "            verdict = 'Emsemble Fit'\n",
    "            print 'model is slightly unstable, ensemble training over the cross-validation models'\n",
    "        else:\n",
    "            verdict = 'No Fit'\n",
    "            print 'model is highly unstable, possibly by overfitting'\n",
    "\n",
    "        return verdict, params_to_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mesoconnectome Predictor class\n",
    "### Convert2JSON function\n",
    "### Author: Nestor Timonidis\n",
    "### Description:  \n",
    "Given a dictionary composed of results from the predictive pipeline, this function converts them into JSON format for uses such as export to a .json file or call to the Scalable Brain Composer (SBA) API.  \n",
    "In order for results to be visualized in the SBA tool, a conversion to json format is necessary.  \n",
    "Therefore the dictionary bellow indicates the format that results must have in order to be visualized.  \n",
    "*   Specifically:    \n",
    "    *    provider    = the name of the tool to be used. sba stands for scalable brain atlas.\n",
    "    *    atlas       = the Common Coordinate Framework (CCF) version that was used by the Allen Institute to process the raw data that the user has used throughout the analysis. In this case ABA_v3 stands for Allen Brain Atlas version 3, since our data were processed according to that framework.  \n",
    "    *    orientation = the orientation used in 3D space for the input. RAS stands for: right-anterior-superior.\n",
    "    *    unit        = the scaling unit. mm stands for milimiters.  \n",
    "    \n",
    "\n",
    "*     Input:\n",
    "        * PredDict = the dictionary composed of predictive results.  \n",
    "        * ConDict  = the original dictionary with the tract tracing experiments plus additional metadata.\n",
    "        * pred_measure  = the predictive measure that whose distribution we want to store/visualize.\n",
    "*     Output:\n",
    "        * JSonDict = the dictionary that has stored the results in JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MesoconnectomePredictor(MesoconnectomePredictor): \n",
    "    \n",
    "    def Convert2JSON(self, PredDict, ConDict, pred_measure = 'r2'):\n",
    "        \n",
    "        JSonDict = {'bas': {'provider': \"sba\",\n",
    "             'atlas': \"ABA_v3\",\n",
    "             'orientation': \"RAS\",\n",
    "             'unit': \"mm\"},\n",
    "             'style': {}, 'markers':[]} \n",
    "        \n",
    "        if pred_measure not in PredDict[PredDict.keys()[0]].keys():\n",
    "            print 'Error! measure selected does not exist in the dictionary. Please select a new one'\n",
    "            return -1\n",
    "        for tracer_name in PredDict.keys():\n",
    "            if tracer_name == 'wild_type':\n",
    "               tracer_layer = ''\n",
    "               tracer_cell  = ''\n",
    "            else:\n",
    "               tracer_layer = ConDict[tracer_name]['layer'][0]\n",
    "               tracer_cell =  ConDict[tracer_name]['cell-type'][0]\n",
    "            for idx, measure in enumerate(PredDict[tracer_name][pred_measure]):\n",
    "                JSonDict['markers'].append({})\n",
    "                JSonDict['markers'][len(JSonDict['markers'])-1]['value']    = str(measure[0])\n",
    "                JSonDict['markers'][len(JSonDict['markers'])-1]['position'] = [str(coo) for coo in ConDict[tracer_name]['Coordinates'][idx]]\n",
    "                JSonDict['markers'][len(JSonDict['markers'])-1]['region']   = ConDict[tracer_name]['structure-abbrev'][idx]\n",
    "                JSonDict['markers'][len(JSonDict['markers'])-1]['relsize']  = \"0.5\"\n",
    "                JSonDict['markers'][len(JSonDict['markers'])-1]['cre-line'] = tracer_name\n",
    "                if 'layer' in ConDict[tracer_name]:\n",
    "                    JSonDict['markers'][len(JSonDict['markers'])-1]['layer']  = ConDict[tracer_name]['layer'][idx] \n",
    "        \n",
    "        return JSonDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MesoconnectomePredictor class\n",
    "### PreProcessing function\n",
    "### author: Nestor Timonidis\n",
    "### description:  \n",
    "This function applies a set of pre-processing steps to the input gene expression and connectivity strength data, given a set of parameters. The steps are as follows:\n",
    "*  1: Removal of structures dominated by NaN (not-a-number) values.\n",
    "*  2: For gene expression data, imputation of the rest of NaN values by the median value of their corresponding column (feature) vector. For the connectivity data, sampling per column (feature) vector based on the frequency of zero elements and non-zero ones. A custom function called for the sampling is the BimodalImputation function that is described in the above cells.\n",
    "*  3: Scaling both matrices by transforming them based on their cubic square root and then z-scoring them.\n",
    "*  4: Removing outlier genes for the gene expression data based on their interquartile range. The function being called for that purpose is the RemoveOutliers function.\n",
    "*  5: Binarizing the connectivity strength matrix based on the 66th percentile per column (source brain area).\n",
    "\n",
    "* ### input: \n",
    "    * GeneExp = gene expression matrix.\n",
    "    * ConStr = connectivity strength matrix.\n",
    "    * params = information related to the pre-processing steps. Examples include removed structures (restCons, nanGenes) and gene acronyms. \n",
    "* ### output:\n",
    "    * GeneExp_hat_ctr = Processed Gene Expression matrix, with a cubic square root transformation and z-scoring.\n",
    "    * ConStr_sqrt_ctr = Processed Connectivity Strength matrix, with a cubic square root transformation and z-scoring.\n",
    "    * sc_gene_exp     = the scaler used to z-score the Gene Expression array.\n",
    "    * sc_sqrt         = the scaler used to z-score the Connectivity Strength array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MesoconnectomePredictor(MesoconnectomePredictor):\n",
    "\n",
    "    def PreProcessing(self, GeneExp = None, params = None, ConStr = None):\n",
    "        \n",
    "        if ConStr is None:\n",
    "            ConStr = np.zeros(np.shape(GeneExp))\n",
    "            \n",
    "        if GeneExp is None:\n",
    "            GeneExp = np.zeros(np.shape(ConStr))\n",
    "         \n",
    "        # ************ Step 1: Cleaning the dataset from rows containing complete nans *******************#\n",
    "        print 'Data cleaning phase ...\\n'\n",
    "        print 'Size before data cleaning {} - {}'.format(np.shape(GeneExp), np.shape(ConStr))\n",
    "        original_target_size = len(ConStr)\n",
    "        ConStr,nanCons = RemoveNanStructs(0,0.1).fit(ConStr)\n",
    "        print 'Size after intermediate cleaning ' + str(np.shape(ConStr))\n",
    "        self.params['restCons'] = [idx for idx in range(len(GeneExp)) if idx not in nanCons]\n",
    "        GeneExp = np.delete(GeneExp,nanCons,0)\n",
    "       \n",
    "        GeneExp,nanGenes = RemoveNanStructs(0,0.03).fit(GeneExp)\n",
    "        self.params['nanGenes'] = nanGenes; params['nanCons'] = nanCons\n",
    "        self.params['restGenes'] = [idx for idx in range(len(ConStr)) if idx not in nanGenes]\n",
    "        ConStr = np.delete(ConStr,nanGenes,0)\n",
    "        print 'Size after data cleaning ' + str(np.shape(ConStr))  \n",
    "        if original_target_size == 1327:\n",
    "            tmp = params['restCons']\n",
    "        elif original_target_size <= len(params['leaf_keys']):\n",
    "            tmp = [params['leaf_keys'][val] for val in params['restCons']]\n",
    "            \n",
    "        self.params['remaining_indices']   = [tmp[val] for val in params['restGenes']]\n",
    "        self.params['str_cpy'] = [val for idx, val in enumerate(params['acronyms']) if idx in params['remaining_indices']]\n",
    "        # ************** Impute the value of the remaining nans with column averaging **************#\n",
    "        print 'Data imputation phase ...\\n'\n",
    "        for task in range(len(ConStr[0])):\n",
    "            nans = [val for val in ConStr[:,task] if mt.isnan(val) == True]\n",
    "            if len(nans) > 0:\n",
    "                ConStr[:,task] = BimodalImputation().fit(ConStr[:,task])\n",
    "\n",
    "        GeneExp = Imputer(missing_values = 'NaN', strategy = 'median', axis = 0, verbose=0, copy=True).fit_transform(GeneExp) \n",
    "        \n",
    "        print 'Data normalization phase ....\\n'\n",
    "        GeneExp_hat = np.power(GeneExp, 1./3)\n",
    "        sc_gene_exp = StandardScaler()\n",
    "        sc_gene_exp.fit(GeneExp_hat)\n",
    "        GeneExp_hat_ctr = sc_gene_exp.transform(GeneExp_hat)\n",
    "        ConStr_sqrt = np.power(ConStr, 1./3)\n",
    "        sc_sqrt     = StandardScaler()\n",
    "        sc_sqrt.fit(ConStr_sqrt)\n",
    "        ConStr_sqrt_ctr = sc_sqrt.transform(ConStr_sqrt)\n",
    "         \n",
    "        self.params['Gene Acronyms']   = params['Gene Acronyms Original']\n",
    "        self.params['Gene Ids']        = params['Gene Ids Original'] \n",
    "        self.params['rem_genes']       = []\n",
    "        GeneExp_hat                    = np.delete(GeneExp_hat, params['rem_genes'], \n",
    "                                              axis = 1)\n",
    "        sc_gene_exp.fit(GeneExp_hat)\n",
    "        \n",
    "        if len(np.unique(ConStr_sqrt_ctr)) == 1:\n",
    "            return GeneExp_hat_ctr, sc_gene_exp \n",
    "        if len(np.unique(GeneExp_hat_ctr)) == 1:\n",
    "            return ConStr_sqrt_ctr, sc_sqrt\n",
    "        \n",
    "        return GeneExp_hat_ctr, sc_gene_exp, ConStr_sqrt_ctr, sc_sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MesoconnectomePredictor class\n",
    "### LaminarRegistration function\n",
    "### author: Nestor Timonidis\n",
    "### description:  \n",
    "Given a set of brain structure acronyms, this function scans the keywords for laminar-specific information and registers structures based on it.\n",
    "* ### input: \n",
    "    * acronyms = brain structure acronyms.\n",
    "* ### output:\n",
    "    * laminar_profiles = layer specific profiles attributed to specific brain structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LaminarRegistration(acronyms):\n",
    " \n",
    "    laminar_profiles = []\n",
    "    for acro in acronyms:\n",
    "        numbers = [idx for idx,val in enumerate(acro) if val.isdigit()]\n",
    "        if numbers != []: \n",
    "            split_letter =  acro[numbers[0]-1]\n",
    "            split_choices =  acro.split(split_letter)\n",
    "            potential_choice = split_choices[len(split_choices)-1]\n",
    "            blobs = [val for val in potential_choice if val.isdigit() == False]\n",
    "            if len(blobs) < 2 and potential_choice != '' and 'r' not in potential_choice: \n",
    "                laminar_profiles.append('layer ' + potential_choice)\n",
    "            elif '6a' in acro:\n",
    "                laminar_profiles.append('layer 6a')\n",
    "            else:\n",
    "                laminar_profiles.append('layer inspecific')\n",
    "        else:\n",
    "            laminar_profiles.append('layer inspecific')\n",
    "          \n",
    "    return laminar_profiles   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Convert2ROC function\n",
    " ### Author: Nestor Timonidis\n",
    " ### Description:\n",
    "This function selects the most optimal binarization threshold for a set of connectivity patterns of a tracing experiment, based on the area under the ROC curve (AURoc).  \n",
    "For a given tracing experiment, the predicted patterns are converted in connectivity scores with the use of the logistic sigmoid function. Moreover a set thresholds are being used to binarize the actual patterns.   \n",
    "Afterwards the AURoc is being applied to the binarized pattern and the connectivity scores. For each experiment, the threshold that is associated with the highest AURoc score is being selected as an indicative threshold for future experiments associated with the given tracer and its source brain area.\n",
    "### Input:  \n",
    "*   t_actual = the actual connectivity patterns.\n",
    "*   t_pred   = the predicted connectivity patterns.  \n",
    "\n",
    "### Output:\n",
    "*   cutoff_collector = a dictionary that stores for each tracer the most optimal threshold and the AUROc value associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MesoconnectomePredictor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-f80205d6e854>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mMesoconnectomePredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMesoconnectomePredictor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mConvert2ROC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt_actual\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MesoconnectomePredictor' is not defined"
     ]
    }
   ],
   "source": [
    "class MesoconnectomePredictor(MesoconnectomePredictor):\n",
    "   \n",
    "    def Convert2ROC(self,t_actual,t_pred):\n",
    "         \n",
    "        from sklearn.metrics import roc_curve\n",
    "         \n",
    "        def binarize(val,Q):\n",
    "            if val < Q:\n",
    "                return 0\n",
    "            else:\n",
    "                return 1\n",
    "            \n",
    "        def RocFigure(tpr,fpr, auc, ext_thr, int_thr): \n",
    "            plt.figure(figsize = (12,8))\n",
    "            lw = 2\n",
    "            for idx in range(len(tpr)):\n",
    "                #color='darkorange',\n",
    "                #, label='ROC curve (area = {}, external threshold = {}, internal threshold = {})'.format(np.max(auc), np.argmax(auc[0,:]),opt_thr[np.argmax(auc)])\n",
    "                plt.plot(fpr[idx], tpr[idx], \n",
    "                         lw=lw)\n",
    "            plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "            plt.xlim([0.0, 1.0])\n",
    "            plt.ylim([0.0, 1.05])\n",
    "            ax = plt.gca()\n",
    "            ax.xaxis.set_tick_params(labelsize = 14)\n",
    "            ax.yaxis.set_tick_params(labelsize = 14)\n",
    "            plt.xlabel('False Positive Rate', fontsize = 18)\n",
    "            plt.ylabel('True Positive Rate', fontsize = 18)\n",
    "            plt.title('Receiver operating characteristic example', fontsize = 18)\n",
    "            plt.savefig('paper1_figures/mass_roc_curve_fig.jpg')\n",
    "            plt.show() \n",
    "            print 'ROC curve (area = {}, external threshold = {}, internal threshold = {})'.format(np.max(auc), ext_thr, int_thr)\n",
    "        \n",
    "        cutoff_collector = {}\n",
    "        \n",
    "        cutoffs = np.floor(np.linspace(0,100,100))\n",
    "        auc_vec = np.zeros((np.shape(t_actual)[1], len(cutoffs)))\n",
    "        for tracer in range(np.shape(t_actual)[1]):\n",
    "            fprs = []; tprs = []\n",
    "            opt_thrs = np.zeros((len(cutoffs),1))\n",
    "            for cutoff in cutoffs:\n",
    "                Q = np.percentile(list(t_actual[:,tracer]), 100-cutoff, axis = 0)\n",
    "                y_actual = np.vectorize(binarize)(t_actual[:,tracer],Q)\n",
    "                y_pred   = 1/(1 + np.exp(- t_pred[:,tracer]))\n",
    "                ones = [val for val in y_actual if val == 1]\n",
    "                if cutoff >= 20 and cutoff <= 80 and len(np.unique(y_actual)) == 2:\n",
    "                    auc = metrics.roc_auc_score(y_actual, y_pred, average = 'micro')\n",
    "                    fpr, tpr, thresholds = roc_curve(y_actual, y_pred)\n",
    "                    opt_thrs[int(cutoff),0] = thresholds[np.argmax(tpr[1:len(tpr)-1]/fpr[1:len(fpr)-1]) + 1] \n",
    "                    #RocFigure(tpr,fpr, auc)]\n",
    "                    fprs.append(fpr); tprs.append(tpr)\n",
    "                    auc_vec[tracer,int(cutoff)] = auc\n",
    "                 \n",
    "            if tracer == 0:  \n",
    "                ext_thr = 100 - np.argmax(auc_vec[0,:])\n",
    "                internal_thr = opt_thrs[np.argmax(auc_vec[0,:]),0]\n",
    "                RocFigure(tprs,fprs, auc_vec, ext_thr, internal_thr)\n",
    "            cutoff_collector[tracer] = (cutoffs[np.argmax(auc_vec[tracer,:])], np.max(auc_vec[tracer,:]))    \n",
    "\n",
    "        return cutoff_collector       \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MesoconnectomePredictor class\n",
    "### UnravelResults function\n",
    "### author: Nestor Timonidis\n",
    "### description:  \n",
    "This function receives a ground truth vector and predictive results from a set of analyses \n",
    "and makes statistical evaluations by unraveling the results and comparing them with the ground truth.\n",
    "* ### input:  \n",
    "    * predictions = predictive results and ground truth from a set of analyses. \n",
    "    * params = information related to the predictive evaluation. Examples include the method category (classification or regression) and acronyms of genes used in the analysis.\n",
    "    * scaler_con = model used to z-score the connectivity data before the predictive process. During the function restores the actual and predicted results into their original scale.\n",
    "    * scaler_gene = model used to z-score the gene expression data before the predictive process. During the function restores the actual and predicted results into their original scale.\n",
    "* ### output:\n",
    "    * ClfResults = evaluations of the predictive results stored in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MesoconnectomePredictor(MesoconnectomePredictor):\n",
    "    \n",
    "    def UnravelResults(self,predictions, params, scaler_con, scaler_gene):\n",
    "         \n",
    "        num      = 1   \n",
    "        y        = predictions[2]\n",
    "        y_preds  = predictions[1]\n",
    "        y_scores = predictions[0]\n",
    "        mdls     = predictions[3]\n",
    "        coeffs   = predictions[4]\n",
    "        y_preds  = scaler_con.inverse_transform(y_preds)\n",
    "        y        = scaler_con.inverse_transform(y)\n",
    "        GeneExpression = predictions[5]\n",
    "        GeneExpression = scaler_gene.inverse_transform(GeneExpression)\n",
    "        \n",
    "        ClfResults = {}\n",
    "        ClfResults['Model Storage']     = mdls\n",
    "        ClfResults['RMSE']              = np.zeros((np.shape(y)[num],1))\n",
    "        ClfResults['r2']                = np.zeros((np.shape(y)[num],1))\n",
    "        ClfResults['y_actual']          = y \n",
    "        ClfResults['y_preds']           = y_preds\n",
    "        ClfResults['Gene Expression']   = GeneExpression\n",
    "        ClfResults['projection scaler'] = scaler_con\n",
    "        ClfResults['gene scaler']       = scaler_gene\n",
    "            \n",
    "        if len(predictions) > 4 and np.shape(coeffs)!= ():\n",
    "            mean_coeffs = np.mean(coeffs,0)\n",
    "            tmp_mean    = np.argsort(mean_coeffs)[::-1]\n",
    "            ClfResults['Mean Gene Scoring'] = [params['Gene Acronyms'][tmp_mean],\n",
    "                                               mean_coeffs[tmp_mean], tmp_mean]\n",
    "            ClfResults['Gene Scoring'] = []\n",
    "            for coeff in coeffs:\n",
    "                tmp = np.argsort(coeff)[::-1]\n",
    "                ClfResults['Gene Scoring'].append(\n",
    "                    [params['Gene Acronyms'][tmp],coeff[tmp], tmp])    \n",
    " \n",
    "        for num in range(len(y[0])):\n",
    "            ClfResults['RMSE'][num]      = metrics.mean_squared_error(y[:,num], y_preds[:,num])\n",
    "            ClfResults['r2'][num]        = metrics.r2_score(y[:,num], y_preds[:,num])\n",
    "\n",
    "        return ClfResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MesoconnectomePredictor class\n",
    "### PredictProjections function\n",
    "### Author: Nestor Timonidis\n",
    "### Description:   \n",
    "this function is based on the product of our predictive pipeline (PredictivePipeline function), which provides predictions of tracing projection patterns as mentioned in the Overview section. Therefore, since the predictive models are being stored\n",
    "in our tool, users can select their data by giving them as input as well as selecting layer profiles and source brain areas of interest, and the predictive model best fitting the input selections will be used to predict the input data. The end result is a connectivity/projection matrix in a dataframe format, with rows corresponding to target brain areas and columns to source areas, as well as an export of the matrix to a csv file.\n",
    "* ### Input:   \n",
    "    * InputArray    =  the input array that will be given as input to the models for the predictive process.\n",
    "    * layer_profile =  filter regarding layer profiles of interest. For instance, a user might be interested in projections from corticothalamic or layer 5 neuronal populations.\n",
    "* ### Output:\n",
    "    * ProjectionMat = the predicted projection matrix in dataframe format.\n",
    "    * exportFile    = the name of the exported csv file.\n",
    "    * ProjectionDict = dictionary that stores more detailed information about the predicted projections\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MesoconnectomePredictor(MesoconnectomePredictor):   \n",
    "    \n",
    "    def PredictProjections(self, InputArray, layer_profile = None):\n",
    "         \n",
    "        ProjectionMat  = []\n",
    "        source_areas   = []\n",
    "        ModelToUse     = {} \n",
    "        ProjectionDict = {}\n",
    "        \n",
    "        if 'remaining_indices' not in self.params.keys():\n",
    "            temp_params = pk.load(open('updated_params.pkl','rb'))\n",
    "            self.params['remaining_indices'] = temp_params['remaining_indices']\n",
    "        target_areas  = np.asarray(self.targetprofiles['str_acronym'])[self.params['remaining_indices']]\n",
    "        \n",
    "        \n",
    "        InputArray_scaled = StandardScaler().fit_transform(InputArray)\n",
    "        \n",
    "        print 'Restoring predicted models ....'\n",
    "        for key in self.ConDict.keys():\n",
    "            if os.path.isfile('./' + key + '_all_results' + '_ridge.pkl') == True:\n",
    "                ModelToUse[key] = {}\n",
    "                infile = key + '_all_results' + '_ridge.pkl'\n",
    "                tmp    = pk.load(open(infile,'r'))\n",
    "                ModelToUse[key]['model']   = tmp['final model']\n",
    "                if 'projection scaler' in tmp.keys():\n",
    "                    ModelToUse[key]['scaler']  = tmp['projection scaler']\n",
    "                ModelToUse[key]['profile'] = self.ConDict[key]['layer'][0] + ' ' + self.ConDict[key]['cell-type'][0]\n",
    "                ModelToUse[key]['areas']   = self.ConDict[key]['structure-abbrev']\n",
    "                                                                                                    \n",
    "        acceptable_profiles = [value['profile'] for value in ModelToUse.values()]    \n",
    "        \n",
    "        # Restore profiles  and brain areas ....\n",
    "        if layer_profile == 'all':\n",
    "            selected_models = ModelToUse.keys() \n",
    "        elif layer_profile != None:\n",
    "            if layer_profile != None and layer_profile not in acceptable_profiles:\n",
    "                print 'Error! layer profile is not properly specified. Acceptable values are:\\n{}, {} and {}'.format(acceptable_profiles, None, 'all')\n",
    "                return -1\n",
    "            selected_models = [key for key,val in ModelToUse.iteritems() if layer_profile in val['profile']]\n",
    "        else:\n",
    "            selected_models = ['wild_type']\n",
    "              \n",
    "        # predict the projection patterns\n",
    "        for key in selected_models:\n",
    "            temp_areas = ModelToUse[key]['areas']\n",
    "            ProjectionDict[key] = {}\n",
    "            for idx,area_to_inspect in enumerate(temp_areas):\n",
    "                #frequency = np.nonzero(temp_areas == val)\n",
    "                if area_to_inspect in source_areas:\n",
    "                    cnt = 2;  \n",
    "                    while area_to_inspect + ' ' + str(cnt) in source_areas:\n",
    "                          cnt = cnt + 1  \n",
    "                    source_areas = source_areas + [area_to_inspect + ' ' + str(cnt)]\n",
    "                else:\n",
    "                    source_areas = source_areas + [area_to_inspect] \n",
    "                \n",
    "            temp_mat      = ModelToUse[key]['model'].predict(InputArray_scaled)\n",
    "            if 'scaler' in ModelToUse[key].keys():\n",
    "                temp_mat = ModelToUse[key]['scaler'].inverse_transform(temp_mat)  \n",
    "            \n",
    "            ProjectionDict[key]['y_preds'] = temp_mat\n",
    "            if ProjectionMat == []:\n",
    "                ProjectionMat = temp_mat\n",
    "            else:    \n",
    "                ProjectionMat = np.concatenate((ProjectionMat,temp_mat),axis = 1)\n",
    "                \n",
    "        \n",
    "        # convert to dataframe\n",
    "        ProjectionMat = pd.DataFrame(data = ProjectionMat,index = target_areas, \\\n",
    "                                            columns = source_areas)\n",
    "        \n",
    "        # export to csv\n",
    "        exportFile = 'predicted projections from {} sources.csv'.format(layer_profile)\n",
    "        ProjectionMat.to_csv(path_or_buf = exportFile)\n",
    "        \n",
    "        clear_output()\n",
    "        \n",
    "        return ProjectionMat, exportFile, ProjectionDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MesoconnectomePredictor class\n",
    "### Source_statistics function\n",
    "### author: Nestor Timonidis\n",
    "### description:  \n",
    "Given the predictive pipeline results, this function makes summary statistics regarding projection volumes to specific layer profiles for all given measured and predicted tract tracing data.\n",
    "* ### input:  \n",
    "    * ClfResults = dictionary containing the predictive pipeline results.\n",
    "    * tracer_profile =  dictionary that contains information regarding the category a series of tracing experiments.\n",
    "    * laminar_profiles = dictionary that contains information regarding the layer profiles present or absent in all target brain areas.\n",
    "    * subcortical_index = indices for target thalamic areas.\n",
    "    * thalamic_index    = indices for target subcortical areas.\n",
    "* ### output:\n",
    "    * proj_summary = layer specific summary statistics for the measured tracing data.\n",
    "    * proj_pred_summary = layer specific summary statistics for the predicted tracing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MesoconnectomePredictor(MesoconnectomePredictor):\n",
    "    \n",
    "    def Source_statistics(self, ClfResults, tracer_profile, laminar_profiles, thalamic_index, subcortical_index):\n",
    "        # Description: makes summary statistics of predictions for specific\n",
    "        #              layer to layer projection - pattern predictions\n",
    "\n",
    "        # predictive statistics per tracer\n",
    "        projection_stats_true = OrderedDict()\n",
    "        projection_stats_pred = OrderedDict()\n",
    "        for tracer in range(np.shape(ClfResults['y_preds'])[1]):\n",
    "\n",
    "            if tracer_profile[tracer] in projection_stats_pred.keys(): new_key = tracer_profile[tracer] + '_' + str(tracer)\n",
    "            else: new_key = tracer_profile[tracer]     \n",
    "\n",
    "            projection_stats_pred[new_key] = OrderedDict()\n",
    "            projection_stats_true[new_key] = OrderedDict()\n",
    "            sub_unique = sorted(set(laminar_profiles)) # get the unique values of laminar profiles\n",
    "            sub_unique.remove('other profiles'); sub_unique.remove('layer inspecific')\n",
    "            for prof in sub_unique:\n",
    "                prof_areas  = [idx for idx,val in enumerate(laminar_profiles) if val == prof]\n",
    "                projection_stats_pred[new_key][prof] = sum(ClfResults['y_preds'][prof_areas,tracer]) \n",
    "                projection_stats_true[new_key][prof] = sum(ClfResults['y_actual'][prof_areas,tracer]) \n",
    "            projection_stats_pred[new_key]['thalamic'] = sum(ClfResults['y_preds'][thalamic_index,tracer]) \n",
    "            projection_stats_pred[new_key]['subcortical'] = sum(ClfResults['y_preds'][subcortical_index,tracer]) \n",
    "            projection_stats_true[new_key]['thalamic'] = sum(ClfResults['y_actual'][thalamic_index,tracer]) \n",
    "            projection_stats_true[new_key]['subcortical'] = sum(ClfResults['y_actual'][subcortical_index,tracer])    \n",
    "         \n",
    "        proj_summary      = np.asarray(pd.DataFrame.from_dict(projection_stats_true,orient='index')).mean(axis = 0)\n",
    "        kurtosis          = sci.stats.kurtosis(np.asarray(pd.DataFrame.from_dict(projection_stats_true,orient='index')), axis = 0)\n",
    "        print kurtosis\n",
    "        proj_summary      /= proj_summary.sum()\n",
    "        proj_pred_summary = np.asarray(pd.DataFrame.from_dict(projection_stats_pred,orient='index')).mean(axis = 0) \n",
    "        proj_pred_summary /= proj_pred_summary.sum()\n",
    "\n",
    "        return proj_summary, proj_pred_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MesoconnectomePredictor class\n",
    "### PlotStatistics function\n",
    "### author: Nestor Timonidis\n",
    "### description:  \n",
    "This function calls the Source_statistics function to make layer specific summary statistics regarding the predictive pipeline results. Furthermore, provides visual bargraph plots of the aforementioned statistics.\n",
    "* ### input:  \n",
    "    * PredResults    = dictionary containing the predictive pipeline results.\n",
    "    * laminar_hits   = statistical results from gene laminar enrichment analysis per tracer category.  \n",
    "    (LaminarEnrichment function).\n",
    "    * cellular_hits  = statistical results from gene ontology enrichment analysis per tracer category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MesoconnectomePredictor(MesoconnectomePredictor):\n",
    "    \n",
    "    def PlotStatistics(self, PredResults, structure_summary = False, laminar_summary = False,\n",
    "                       tracer_category = None, selected_tracer = None, \n",
    "                       laminar_hits = None):\n",
    "        \n",
    "        if 'nanCons' not in self.params:\n",
    "            temp_params = pk.load(open('updated_params.pkl','rb'))\n",
    "            self.params['nanCons'] = temp_params['nanCons']\n",
    "        if 'remaining_indices' not in self.params:         \n",
    "            self.params['remaining_indices'] = temp_params['remaining_indices']\n",
    "        \n",
    "        all_files = os.listdir('./')\n",
    "        self.targetprofiles = self.LaminarFiltering(self.targetprofiles)   \n",
    "        profiles_of_interest = list(np.unique(self.targetprofiles['alt laminar profiles']))\n",
    "        profiles_of_interest.remove('layer inspecific'); profiles_of_interest.remove('other profiles')\n",
    "        profiles_of_interest.append('thalamic');profiles_of_interest.append('subcortical')\n",
    "        \n",
    "        matplotlib.rcParams['figure.figsize'] = (20.0, 10.0)\n",
    "        laminar_Accuracy = OrderedDict()\n",
    "        Laminar_Stats = OrderedDict()\n",
    "        laminar_copy = [self.targetprofiles['alt laminar profiles'][val] for idx,val in enumerate(self.params['remaining_indices'])]\n",
    "        thalamic_copy = [idx2 for val in self.targetprofiles['thalamic_structures'] for idx2,val2 in enumerate(self.params['remaining_indices']) if val == val2]\n",
    "        subcortical_copy = [idx2 for val in self.targetprofiles['subcortical_structures'] for idx2,val2 in enumerate(self.params['remaining_indices']) if val == val2]\n",
    "        proj_summary = {}; proj_pred_summary = {}\n",
    "        A = None; B = None\n",
    "        \n",
    "        if tracer_category == None:\n",
    "            tracer_category = PredResults.keys()\n",
    "        else:\n",
    "            tracer_category = [tracer_category]\n",
    "            \n",
    "        for tracer_name in tracer_category:\n",
    "            print tracer_name\n",
    "    \n",
    "            if laminar_summary is not False:  \n",
    "                laminar_indices = [idx for idx,val in enumerate(laminar_copy) if val in profiles_of_interest]\n",
    "                tracer_layer    = self.ConDict[tracer_name]['layer'][0]\n",
    "                tracer_cell     = self.ConDict[tracer_name]['cell-type'][0]\n",
    "                tracer_profiles = [self.ConDict[tracer_name]['structure-abbrev'][idx] for idx in range(len(self.ConDict[tracer_name]['structure-abbrev']))] \n",
    "                xlabel          = profiles_of_interest \n",
    "                title           = tracer_name\n",
    "                proj_summary[tracer_name], proj_pred_summary[tracer_name] = self.Source_statistics(PredResults[tracer_name], \\\n",
    "                                                                                                   tracer_profiles, \\\n",
    "                                                                                                   laminar_copy,\n",
    "                                                                                                   thalamic_copy,\n",
    "                                                                                                   subcortical_copy)\n",
    "                ind = np.arange(len(xlabel))\n",
    "                ax = plt.subplot(111)\n",
    "                plt.figure\n",
    "                ax.bar(ind, proj_summary[tracer_name], width = 0.2, align = 'center')\n",
    "                ax.bar(ind + 0.2, proj_pred_summary[tracer_name], width = 0.2, align = 'center')\n",
    "                ax.set_xticklabels(xlabel)\n",
    "                ax.set_xticks(ind + 0.1)\n",
    "                ax.xaxis.set_tick_params(labelsize = 18)\n",
    "                ax.yaxis.set_tick_params(labelsize = 18)\n",
    "                plt.yticks(np.arange(0, 0.5, step = 0.05))\n",
    "                plt.title(title, fontsize = 22)\n",
    "                plt.legend(['predicted','actual'], prop={'size': 20})\n",
    "                save_name = 'laminar_stats_for_{}.jpg'.format(title)\n",
    "                plt.savefig(save_name)\n",
    "                plt.show()\n",
    "            \n",
    "            \n",
    "            #***********************************************************************#\n",
    "            if structure_summary == True and selected_tracer is not None:\n",
    "                \n",
    "                subcortical_structures = pk.load(open('subcortical_structures.pkl', 'rb'))\n",
    "                cortical_structures    = pk.load(open('cortical_structures.pkl', 'rb'))\n",
    "                \n",
    "                cortical_remains_idx = np.asarray([idx for val2 in cortical_structures for idx,val in enumerate(self.params['remaining_indices']) if val == val2])\n",
    "                subcortical_remains_idx = np.asarray([idx for val2 in subcortical_structures for idx,val in enumerate(self.params['remaining_indices']) if val == val2])\n",
    "                cortical_remains_acro = [self.targetprofiles['str_acronym'][val] for val2 in cortical_structures for idx,val in enumerate(self.params['remaining_indices']) if val == val2]\n",
    "                subcortical_remains_acro = [self.targetprofiles['str_acronym'][val] for val2 in subcortical_structures for idx,val in enumerate(self.params['remaining_indices']) if val == val2]\n",
    "                \n",
    "                if isinstance(PredResults, (np.ndarray, np.generic)) == True:\n",
    "                    A = PredResults[:,selected_tracer]/(1.0*np.max(PredResults[:,selected_tracer]))\n",
    "                    source_name = selected_tracer\n",
    "                else:    \n",
    "                    if 'y_actual' in PredResults[tracer_name].keys():\n",
    "                        max1 = 1.0*np.max(PredResults[tracer_name]['y_actual'][:,selected_tracer])\n",
    "                        A = PredResults[tracer_name]['y_actual'][:,selected_tracer]/max1\n",
    "                    if 'y_preds' in PredResults[tracer_name].keys():    \n",
    "                        max2 = 1.0*np.max(PredResults[tracer_name]['y_preds'][:,selected_tracer])\n",
    "                        B = PredResults[tracer_name]['y_preds'][:,selected_tracer]/max2\n",
    "                    source_name = self.ConDict[tracer_name]['structure-abbrev'][selected_tracer]\n",
    "                \n",
    "                \n",
    "                if A is not None:\n",
    "                    top_cortex_idx   = np.argsort(A[cortical_remains_idx])[::-1][0:30]\n",
    "                    top_cortex       = np.sort(A[cortical_remains_idx])[::-1][0:30]\n",
    "                    top_cortex_acros = [cortical_remains_acro[val] for val in top_cortex_idx]\n",
    "\n",
    "                    plt.figure(figsize = (15,5))\n",
    "                    ax = plt.gca()\n",
    "                    plt.xticks(rotation = 25)\n",
    "                    plt.bar(top_cortex_acros, top_cortex)\n",
    "                    title = '{} - {} - {} measured'.format(tracer_name,\n",
    "                                                            source_name,\n",
    "                                                            'Cortex')\n",
    "                    plt.title(title)\n",
    "                    plt.savefig(title + '.jpg')\n",
    "                    plt.show()\n",
    "\n",
    "                    top_subcortex_idx   = np.argsort(A[subcortical_remains_idx])[::-1][0:30]\n",
    "                    top_subcortex       = np.sort(A[subcortical_remains_idx])[::-1][0:30]\n",
    "                    top_subcortex_acros = [subcortical_remains_acro[val] for val in top_subcortex_idx]\n",
    "\n",
    "                    plt.figure(figsize = (15,5))\n",
    "                    plt.xticks(rotation = 25)\n",
    "                    plt.bar(top_subcortex_acros, top_subcortex)\n",
    "                    title = '{} - {} - {} measured'.format(tracer_name, \n",
    "                                                    source_name, 'Subcortex')\n",
    "                    plt.title(title)\n",
    "                    plt.savefig(title + '.jpg')\n",
    "                    plt.show()\n",
    "                \n",
    "                if isinstance(PredResults, (np.ndarray, np.generic)) == False and B is not None:\n",
    "                    top_cortex_idx   = np.argsort(B[cortical_remains_idx])[::-1][0:30]\n",
    "                    top_cortex       = np.sort(B[cortical_remains_idx])[::-1][0:30]\n",
    "                    top_cortex_acros = [cortical_remains_acro[val] for val in top_cortex_idx]\n",
    "\n",
    "                    plt.figure(figsize = (15,5))\n",
    "                    ax = plt.gca()\n",
    "                    plt.xticks(rotation = 25)\n",
    "                    plt.bar(top_cortex_acros, top_cortex)\n",
    "                    title = '{} - {} - {} predicted'.format(tracer_name,\n",
    "                                                    source_name, 'Cortex')\n",
    "                    plt.title(title)\n",
    "                    plt.savefig(title + '.jpg')\n",
    "                    plt.show()\n",
    "\n",
    "                    top_subcortex_idx   = np.argsort(B[subcortical_remains_idx])[::-1][0:30]\n",
    "                    top_subcortex       = np.sort(B[subcortical_remains_idx])[::-1][0:30]\n",
    "                    top_subcortex_acros = [subcortical_remains_acro[val] for val in top_subcortex_idx]\n",
    "\n",
    "                    plt.figure(figsize  = (15,5))\n",
    "                    plt.xticks(rotation = 25)\n",
    "                    plt.bar(top_subcortex_acros, top_subcortex)\n",
    "                    title = '{} - {} - {} predicted'.format(tracer_name, \n",
    "                                                    source_name, 'Subcortex')\n",
    "                    plt.title(title)\n",
    "                    plt.savefig(title + '.jpg')\n",
    "                    plt.show()\n",
    "            #***********************************************************************#\n",
    "            if laminar_hits is not None:\n",
    "                lam_labels = list(np.unique(self.targetprofiles['alt laminar profiles']))\n",
    "                lam_labels.remove('layer inspecific'); lam_labels.remove('other profiles'); lam_labels.remove('layer 1') \n",
    "                plt.figure\n",
    "                plt.bar(lam_labels, laminar_hits[tracer_name].values(), width = 0.2, align = 'center')\n",
    "                ax = plt.gca()\n",
    "                ax.xaxis.set_tick_params(labelsize = 18)\n",
    "                ax.yaxis.set_tick_params(labelsize = 18)\n",
    "                plt.title(tracer_name, fontsize =  20)\n",
    "                plt.xlabel('Enrichment hits', fontsize = 20)\n",
    "                plt.ylabel('Frequency', fontsize = 20)\n",
    "                plt.savefig('blabla.png')\n",
    "                plt.show()\n",
    "\n",
    "\n",
    "        return proj_summary, proj_pred_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  MesoconnectomePredictor class\n",
    "### CTEnrichment function\n",
    "### Author: Nestor Timonidis\n",
    "### Description: \n",
    "compares an input list of known genes with stored lists of known cell-type markers.\n",
    "### Input:\n",
    "* gene_list = input list of known genes.   \n",
    "\n",
    "### Output:\n",
    "* CT_hits = scores of cell type marker hits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MesoconnectomePredictor(MesoconnectomePredictor):   \n",
    "    \n",
    "    def CTEnrichment(self, gene_list):\n",
    "        \n",
    "        #** Miller files\n",
    "        Miller = {}\n",
    "        with open('Miller et al 2014.csv') as fp:\n",
    "            A = csv.reader(fp)\n",
    "            for line in A:\n",
    "                if line[0] == 'Oligodendrocytes':\n",
    "                    key = 'Oligodendrocytes'\n",
    "                    Miller[key] = []\n",
    "                elif line[0] == 'Microglia':\n",
    "                    key = 'Microglia'\n",
    "                    Miller[key] = []\n",
    "                elif line[0] == 'Astrocytes':\n",
    "                    key = 'Astrocytes'\n",
    "                    Miller[key] = []\n",
    "                elif line[0] == 'Neurons':\n",
    "                    key = 'Neurons'\n",
    "                    Miller[key] = []\n",
    "                else: \n",
    "                    Miller[key].append(line[0].split(' ')[0].lower())\n",
    "         \n",
    "        #** Lein Files\n",
    "        CTMarkers['Lein_enrichment'] = {}\n",
    "        keep_outs = [None, 'Non-Expressors']\n",
    "        wb = xl.load_workbook('Lein et al 2007.xlsx')\n",
    "        ws = wb['Cell Type-Enriched Markers']\n",
    "\n",
    "        for j,col in enumerate(list(ws.rows)[0]):\n",
    "            if col.value not in keep_outs:\n",
    "               CTMarkers['Lein_enrichment'][col.value] = []\n",
    "               for i,row in enumerate(ws.rows):\n",
    "                   if row[j].value not in keep_outs and i > 0 and row[j].value != None:\n",
    "                      CTMarkers['Lein_enrichment'][col.value].append(row[j].value.lower())\n",
    "                        \n",
    "        wb = xlrd.open_workbook('Lein_target_mRNA.xls')\n",
    "        ws = wb.sheet_by_index(0)\n",
    "\n",
    "        cell_type_id = 0\n",
    "        ontology_id  = 1\n",
    "        gene_id      = 2\n",
    "        region_id    = 3\n",
    "\n",
    "        CTMarkers['Lein_targetting'] = {}\n",
    "        keywords = ['dendrites', 'astrocytes', 'oligodendrocytes']\n",
    "        for row_id in range(ws.nrows):\n",
    "            row =  ws.row(row_id)\n",
    "            hit = [row[0].value for val in keywords if val.lower() in str(row[0].value.lower())]\n",
    "            if hit != []: \n",
    "                new_ctype = hit[0]\n",
    "                if new_ctype not in CTMarkers['Lein_targetting'].keys(): CTMarkers['Lein_targetting'] [new_ctype] = {'genes': [], 'ontology': []}\n",
    "\n",
    "            if CTMarkers['Lein_targetting'] != {}:   \n",
    "                tmp_str  = ''.join(row[gene_id].value)\n",
    "                tmp_str2 = ''.join(row[ontology_id].value)\n",
    "                if tmp_str != '':\n",
    "                    CTMarkers['Lein_targetting'] [new_ctype]['genes'].append(tmp_str.lower())\n",
    "                if tmp_str2 != '':   \n",
    "                    CTMarkers['Lein_targetting'] [new_ctype]['ontology'].append(''.join(row[ontology_id].value))\n",
    "\n",
    "        #** Winden Files\n",
    "        CTMarkers['Winden'] = {}\n",
    "        wb = xl.load_workbook('Winden et al 2009.xlsx')\n",
    "        ws = wb['Sheet1']\n",
    "        for i,row in enumerate(ws.rows):\n",
    "            if i> 0:\n",
    "                if row[3].value not in CTMarkers['Winden'].keys():\n",
    "                    CTMarkers['Winden'][row[3].value] = []\n",
    "                CTMarkers['Winden'][row[3].value].append(row[2].value.lower())\n",
    "\n",
    "        for key in CTMarkers['Winden'].keys():\n",
    "            if key == 'purple3':\n",
    "               CTMarkers['Winden']['Cortical'] = CTMarkers['Winden'].pop(key)\n",
    "            elif key == 'sienna2':\n",
    "               CTMarkers['Winden']['Somatostatin & Parvalbumin'] = CTMarkers['Winden'].pop(key)\n",
    "            else:\n",
    "               CTMarkers['Winden']['Cholecystokinin'] = CTMarkers['Winden'].pop(key)\n",
    " \n",
    "        #** Galazo Files\n",
    "        CTMarkers['Galazo'] = OrderedDict()\n",
    "        wb = xl.load_workbook('Galazo et al 2016.xlsx')\n",
    "        ws = wb['Sheet1']\n",
    "        CTMarkers['Galazo']['corticothalamic ontologies'] = []\n",
    "        CTMarkers['Galazo']['corticothalamic genes'] = []\n",
    "        for i,row in enumerate(ws.rows):\n",
    "            if i == 0:\n",
    "                for idx in range(len(row)):\n",
    "                    CTMarkers['Galazo']['corticothalamic ontologies'].append(row[idx].value)\n",
    "            else:\n",
    "                keys = CTMarkers['Galazo'].keys()\n",
    "                for idx in range(len(row)):\n",
    "                    tmp = row[idx].value.split(',')\n",
    "                    CTMarkers['Galazo']['corticothalamic genes'].extend(tmp)\n",
    "        CTMarkers['Galazo']['corticothalamic genes'] = list(set(CTMarkers['Galazo']['corticothalamic genes']))\n",
    "\n",
    "        #** Zeisel Files\n",
    "        keywords = ['pyramidal', 'interneuron']\n",
    "        CTMarkers['Zeisel'] = OrderedDict()\n",
    "        store_pos = []\n",
    "        wb = xl.load_workbook('Zeisel et al 2015.xlsx')\n",
    "        ws = wb['Modules']\n",
    "        for i,row in enumerate(ws.rows):\n",
    "            if i == 0:\n",
    "                for idx in range(len(row)):\n",
    "                    hits = [row[idx].value.lower() for val in keywords if val in row[idx].value.lower()]\n",
    "                    if len(hits) > 0:\n",
    "                       CTMarkers['Zeisel'][hits[0]] = []\n",
    "                       store_pos.append(idx)\n",
    "            elif i > 1:\n",
    "                keys = CTMarkers['Zeisel'].keys()\n",
    "                for idx,key in zip(store_pos, keys):\n",
    "                    if row[idx].value != None:\n",
    "                       CTMarkers['Zeisel'][key].append(row[idx].value)\n",
    "        \n",
    "        \n",
    "        Markerlist = ['Winden', 'Lein_enrichment', 'Lein_targetting',\n",
    "                      'Miller', 'Zeisel', 'Galazo']\n",
    "         \n",
    "        CT_hits = {}\n",
    "        for Marker in Markerlist:\n",
    "            for celltype in CTMarkers[Marker].keys(): \n",
    "                if Marker == 'Lein_targetting':\n",
    "                    geneset  = CTMarkers[Marker][celltype]['genes']\n",
    "                    key_name = Marker + '- ' + celltype\n",
    "                elif Marker == 'Galazo':\n",
    "                    geneset  = CTMarkers[Marker]['corticothalamic genes'] \n",
    "                    key_name = Marker + '- ' + 'corticothalamic genes'\n",
    "                else:    \n",
    "                    geneset  = CTMarkers[Marker][celltype]\n",
    "                    key_name = Marker + '- ' + celltype\n",
    "                 \n",
    "                gene_hits = [val for val in gene_list for val2 in geneset if val.lower() == val2.lower()] \n",
    "                \n",
    "                if len(gene_hits) > 0:\n",
    "                    CT_hits[key_name] = gene_hits\n",
    "                \n",
    "        return CT_hits       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
