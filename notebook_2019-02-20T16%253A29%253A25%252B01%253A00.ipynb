{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#git clone https://github.com/AllenInstitute/mouse_connectivity_models.git\n",
    "#!pip install mouse_connectivity_models/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle as pk\n",
    "import h5py\n",
    "import sys\n",
    "import math as mt\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xlrd\n",
    "import nrrd\n",
    "import numpy as np\n",
    "import json\n",
    "import string\n",
    "import csv\n",
    "import scipy as sci\n",
    "from IPython.core.debugger import set_trace\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "from os import chdir\n",
    "from os.path import basename,dirname,realpath\n",
    "from scipy.spatial.distance import euclidean\n",
    "from scipy.stats import skew, kurtosis, zscore\n",
    "from scipy.stats.mstats import normaltest\n",
    "from allensdk.api.queries.grid_data_api import *\n",
    "from allensdk.api.queries.mouse_connectivity_api import MouseConnectivityApi\n",
    "from allensdk.api.queries.reference_space_api import ReferenceSpaceApi\n",
    "from allensdk.core.mouse_connectivity_cache import MouseConnectivityCache\n",
    "from scipy.io import loadmat\n",
    "from subprocess import call\n",
    "from urllib import urlretrieve\n",
    "from sklearn.preprocessing import Imputer, normalize\n",
    "from collections import OrderedDict\n",
    "from scipy.special import expit\n",
    "import nrrd\n",
    "from mcmodels.core import VoxelModelCache\n",
    "from mcmodels.models.voxel import RegionalizedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "API_PATH = \"http://api.brain-map.org/api/v2/data\"\n",
    "GRAPH_ID = 1\n",
    "PLANE_ID = 1 # coronal\n",
    "MOUSE_PRODUCT_ID = 1 # aba\n",
    "\n",
    "DATA_CON_SET_QUERY_URL = (\"%s/SectionDataSet/query.json\" +\\\n",
    "                          \"?criteria=[failed$eqfalse]\" +\\\n",
    "                          \",products[id$in5]\" +\\\n",
    "                          \",[green_channel$eqrAAV]\" +\\\n",
    "                          #\",specimen(donor[transgenic_mouse_id$eqall donors])\" +\\\n",
    "                          \",specimen(stereotaxic_injections(age[days$ge54],[days$le58]))\" +\\\n",
    "                          \",plane_of_section[id$eq%d]\" +\\\n",
    "                          \"&include=specimen(stereotaxic_injections(age,stereotaxic_injection_materials,stereotaxic_injection_coordinates,primary_injection_structure)),specimen(donor(age))\") \\\n",
    "                          % (API_PATH, PLANE_ID)\n",
    "\n",
    "UNIONIZE_CON_FMT = \"%s/ProjectionStructureUnionize/query.json\" +\\\n",
    "               \"?criteria=[section_data_set_id$eq%d],[is_injection$eqfalse]\" +\\\n",
    "               \"&include=hemisphere\"\n",
    "\n",
    "STRUCTURES_URL = (\"%s/Structure/query.json?\" +\\\n",
    "                      \"criteria=[graph_id$eq%d]\") \\\n",
    "                      % (API_PATH, GRAPH_ID)\n",
    "\n",
    "    \n",
    "DATA_EXP_SET_QUERY_URL = (\"%s/SectionDataSet/query.json\" +\\\n",
    "                          \"?criteria=[failed$eq'false'][expression$eq'true']\" +\\\n",
    "                          \",products[id$eq%d]\" +\\\n",
    "                          \",plane_of_section[id$eq%d],genes\" +\\\n",
    "                          \"&include=genes\") \\\n",
    "                          % (API_PATH, MOUSE_PRODUCT_ID, PLANE_ID)\n",
    "\n",
    "UNIONIZE_EXP_FMT = \"%s/StructureUnionize/query.json\" +\\\n",
    "               \"?criteria=[section_data_set_id$eq%d],structure[graph_id$eq1]\" +\\\n",
    "               (\"&include=section_data_set(products[id$in%d])\" % (MOUSE_PRODUCT_ID)) +\\\n",
    "               \"&only=id,structure_id,sum_pixels,expression_energy,section_data_set_id\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make a query to the API via a URL.\n",
    "def QueryAPI(url):\n",
    "    start_row = 0\n",
    "    num_rows = 2000\n",
    "    total_rows = -1\n",
    "    rows = []\n",
    "    done = False\n",
    "\n",
    "    # The ontology has to be downloaded in pages, since the API will not return\n",
    "    # more than 2000 rows at once.\n",
    "    while not done:\n",
    "        pagedUrl = url + '&start_row=%d&num_rows=%d' % (start_row,num_rows)\n",
    "\n",
    "        print pagedUrl\n",
    "        source = urllib.urlopen(pagedUrl).read()\n",
    "\n",
    "        response = json.loads(source)\n",
    "        rows += response['msg']\n",
    "\n",
    "        if total_rows < 0:\n",
    "            total_rows = int(response['total_rows'])\n",
    "\n",
    "        start_row += len(response['msg'])\n",
    "\n",
    "        if start_row >= total_rows:\n",
    "            done = True\n",
    "\n",
    "    print('Number of results: {}'.format(total_rows))\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def DownloadUnionizedData(dataSets):\n",
    "    unionizes = [QueryAPI(UNIONIZE_FMT % (API_PATH,d['id'])) for d in dataSets]\n",
    "    return unionizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Download the mouse brain structures in a structure graph.\n",
    "def DownloadStructures():\n",
    "    structs = QueryAPI(STRUCTURES_URL)\n",
    "\n",
    "    # Build a dict from structure id to structure and identify each node's\n",
    "    # direct descendants.\n",
    "    structHash = {}\n",
    "    for s in structs:\n",
    "        s['num_children'] = 0\n",
    "        s['structure_id_path'] = [int(sid) for sid in s['structure_id_path'].split('/') if sid != '']\n",
    "        structHash[s['id']] = s\n",
    "\n",
    "    for sid,s in structHash.iteritems():\n",
    "        if len(s['structure_id_path']) > 1:\n",
    "            parentId = s['structure_id_path'][-2]\n",
    "            structHash[parentId]['num_children'] += 1\n",
    "\n",
    "    ## pull out the structure ids for structures in this structure graph that\n",
    "    ## have no children (i.e. just the leaves)\n",
    "    ## corrStructIds = [sid for sid,s in structHash.iteritems() if s['num_children'] == 0]\n",
    "    # RB: no, leave all structures in and filter later\n",
    "    corrStructIds = structHash.keys()\n",
    "\n",
    "    return sorted(corrStructIds), structHash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CreateConnectivityMatrix(dataSets,structureIds,structHash,unionizes):\n",
    "    # Each injection experiment will have a connectivity vector.  This vector will be as long\n",
    "    # as the number of requested structures.\n",
    "    nstructs = len(structureIds)\n",
    "    ndata = len(unionizes)\n",
    "    print('ndata {} ndatasets {}'.format(ndata,len(dataSets)))\n",
    "\n",
    "    sidHash = dict([(id,i) for (i,id) in enumerate(structureIds)])\n",
    "    didHash = dict([(d['id'],i) for (i,d) in enumerate(dataSets)])\n",
    "\n",
    "    connectivityL = numpy.empty([nstructs,ndata])\n",
    "    connectivityL.fill(numpy.nan)\n",
    "    connectivityR = numpy.empty([nstructs,ndata])\n",
    "    connectivityR.fill(numpy.nan)\n",
    " \n",
    "    connectivityDict = {'projection_density': 0, 'projection_intensity': 0, 'projection_energy': 0, 'projection_volume': 0, 'normalized_projection_volume': 0}\n",
    "    for key in connectivityDict.keys():\n",
    "        connectivityDict[key] = numpy.empty([nstructs,ndata])\n",
    "        connectivityDict[key].fill(numpy.nan)\n",
    "\n",
    "    # For each data set's set of unionizes, then for each individual structure,\n",
    "    # fill in the structure's connectivity vector.\n",
    "    for i,us in enumerate(unionizes):\n",
    "        # for each unionize\n",
    "        for j,u in enumerate(us):\n",
    "            sid = u['structure_id']\n",
    "            did = u['section_data_set_id']\n",
    "\n",
    "            struct = structHash[sid]\n",
    "            struct['volume'] = u['sum_pixels']\n",
    "\n",
    "            if i ==0 and j == 0:\n",
    "              print u\n",
    "\n",
    "            if sidHash.has_key(sid) and didHash.has_key(did):\n",
    "                if u['hemisphere_id'] is 1:\n",
    "                    connectivityL[sidHash[sid]][didHash[did]]  = u['normalized_projection_volume']\n",
    "                elif u['hemisphere_id'] is 2:\n",
    "                    connectivityR[sidHash[sid]][didHash[did]] = u['normalized_projection_volume']\n",
    "                    for key in connectivityDict.keys():\n",
    "                        connectivityDict[key][sidHash[sid]][didHash[did]] = u[key]\n",
    "                elif u['hemisphere_id'] is 3:\n",
    "                  pass\n",
    "                  # this is just the average value of L+R\n",
    "            else:\n",
    "                print \"ERROR: structure {}/injection {} skipped.\".format(sid,did)\n",
    "\n",
    "    return connectivityL, connectivityR, connectivityDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CreateExpressionMatrix(dataSets,structureIds,structHash,unionizes):\n",
    "    # Each structure will have an expression vector.  This vector will be as long\n",
    "    # as the number of requested structures.\n",
    "    nstructs = len(structureIds)\n",
    "    ndata = len(unionizes)\n",
    "\n",
    "    sidHash = dict([(id,i) for (i,id) in enumerate(structureIds)])\n",
    "    didHash = dict([(d['id'],i) for (i,d) in enumerate(dataSets)])\n",
    "\n",
    "    expression = numpy.empty([nstructs,ndata])\n",
    "    expression.fill(numpy.nan)\n",
    "\n",
    "    # For each data set's set of unionizes, then for each individual structure,\n",
    "    # fill in the structure's expression vector.\n",
    "    for i,us in enumerate(unionizes):\n",
    "        # for each unionize\n",
    "        for j,u in enumerate(us):\n",
    "            sid = u['structure_id']\n",
    "            did = u['section_data_set_id']\n",
    "\n",
    "            struct = structHash[sid]\n",
    "            struct['volume'] = u['sum_pixels']\n",
    "\n",
    "            if sidHash.has_key(sid) and didHash.has_key(did):\n",
    "                expression[sidHash[sid]][didHash[did]] = u['expression_energy']\n",
    "\n",
    "    return expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def allChildren(acr,acr2parent):\n",
    "   # Description: given a tree hierarchy and an entity,\n",
    "   # this function returns all the children\n",
    "   # of the entity\n",
    "\n",
    "   AC = []\n",
    "   for a,p in acr2parent.items():\n",
    "     if p == acr:\n",
    "       AC.append(a)\n",
    "       AC.extend(allChildren(a,acr2parent))\n",
    "   return AC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ReduceToLeafNodes(structure_acronyms,tree_file):\n",
    "    # Description: this function checks the givenReduceToLeafNodes structures\n",
    "    #              based on the tree hierarchy and returns the\n",
    "    #              leaf nodes\n",
    "\n",
    "    leaf_nodes = []\n",
    "    with open(tree_file) as fp:\n",
    "        acr2parent = json.load(fp)\n",
    "\n",
    "    for idx,acro in enumerate(structure_acronyms):\n",
    "        AC = allChildren(acro, acr2parent)\n",
    "        if len(AC) == 0: # structure is a leaf node\n",
    "           leaf_nodes.append((idx,acro))\n",
    "    return leaf_nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GetConUnionizes():\n",
    "\n",
    "    infile1 = 'expression_files/inj_unionizes.nrrd'\n",
    "    infile2 = 'expression_files/exp_density.nrrd'\n",
    "    mcc = MouseConnectivityCache(resolution = 100)\n",
    "    experiments = mcc.get_experiments(cre = True, dataframe=True)\n",
    "    uni_con = []\n",
    "    for idx,val in enumerate(experiments['id']):\n",
    "        print idx\n",
    "        tmp = mcc.get_experiment_structure_unionizes(experiment_id = val)\n",
    "        uni_con.append(tmp)\n",
    "    print uni_con.shape\n",
    "    fp = h5py.File('py_files/unionized_connectivity.hdf5','w')\n",
    "    fp.create_dataset('dataset1',data = uni_con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-e39a7c2e090b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;31m# Download the list of data sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m \u001b[0mdataSets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDownloadDataSetList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;31m#************ Nestors Intervention ************\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;31m#dataSets = h5py.File('py_files/MetaPerExp.hdf5','r')['dataset1']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-e39a7c2e090b>\u001b[0m in \u001b[0;36mDownloadDataSetList\u001b[0;34m(n)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;31m# Download the first `n` data sets.  For negative `n` , download them all.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mDownloadDataSetList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mdataSets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQueryAPI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_SET_QUERY_URL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-e39a7c2e090b>\u001b[0m in \u001b[0;36mQueryAPI\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0mpagedUrl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpagedUrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python2/lib/python2.7/urllib.pyc\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, proxies, context)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_urlopener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python2/lib/python2.7/urllib.pyc\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data)\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python2/lib/python2.7/urllib.pyc\u001b[0m in \u001b[0;36mopen_http\u001b[0;34m(self, url, data)\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddheaders\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mputheader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0merrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetreply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0merrcode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python2/lib/python2.7/httplib.pyc\u001b[0m in \u001b[0;36mgetreply\u001b[0;34m(self, buffering)\u001b[0m\n\u001b[1;32m   1194\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1196\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m                 \u001b[0;31m#only add this keyword if non-default for compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python2/lib/python2.7/httplib.pyc\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self, buffering)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m             \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwill_close\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0m_UNKNOWN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_CS_IDLE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python2/lib/python2.7/httplib.pyc\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python2/lib/python2.7/httplib.pyc\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;31m# Initialize with Simple-Response defaults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python2/lib/python2.7/socket.pyc\u001b[0m in \u001b[0;36mreadline\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mEINTR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def GetCreLines():\n",
    "    infile = 'cre_inj_density.nrrd'\n",
    "    infile2 = 'cre_pro_density.nrrd'\n",
    "    mcc = MouseConnectivityCache(resolution = 100)\n",
    "    mca = MouseConnectivityApi()\n",
    "    cre_experiments2 = mca.experiment_source_search(injection_structures = 'root', transgenic_lines= True)\n",
    "    cre_experiments = mcc.get_experiments(cre= True,dataframe=True)\n",
    "    MetaPerCre = []\n",
    "    InjPerCre  = []\n",
    "    ProjPerCre = []\n",
    "\n",
    "    creDict = {}\n",
    "    with open('Supplementary Table 1.csv') as fp:\n",
    "         buff = csv.reader(fp)\n",
    "         for idx,row in enumerate(buff):\n",
    "             if len(row) > 1: # concatenate the two rows together - error caused by csv transition\n",
    "                row[0] =  row[0] + row[1]\n",
    "             remains = [x for x in filter(None,row[0].split(';'))]\n",
    "             if remains[0].isdigit():\n",
    "             #if len(remains) > 8 and idx > 2 and remains[1].isdigit() == False:\n",
    "                creDict[remains[1]] = []\n",
    "                creDict[remains[1]].append(remains[7])\n",
    "                creDict[remains[1]].append(remains[8])\n",
    "\n",
    "    #pk.dump(cre_experiments,open('py_files/cre_experiments.pkl','wb'))\n",
    "    #cre_experiments = pk.load(open('py_files/cre_experiments.pkl','rb'))\n",
    "\n",
    "    # download the projection density volume for one of the experiments\n",
    "    for idx,val in enumerate(cre_experiments['id']):\n",
    "        mcc.get_projection_density(val, infile2)\n",
    "        tmp = cre_experiments['transgenic-line'][val]\n",
    "        if 'A93' in cre_experiments['transgenic-line'][val]:\n",
    "            tmp = 'A93-Tg1-Cre'\n",
    "        selCre = [key for key in creDict.keys() if tmp == key]\n",
    "        if len(selCre) > 0:\n",
    "            selCre = selCre[0]\n",
    "            MetaPerCre.append({})\n",
    "            rx = len(MetaPerCre)-1\n",
    "            MetaPerCre[rx]['injection-coordinates'] = \\\n",
    "            cre_experiments['injection-coordinates'][val]\n",
    "            MetaPerCre[rx]['structure-abbrev'] =\\\n",
    "             cre_experiments['structure-abbrev'][val]\n",
    "            MetaPerCre[rx]['transgenic-line'] = tmp\n",
    "            MetaPerCre[rx]['id'] = cre_experiments['id'][val]\n",
    "            MetaPerCre[rx]['layer'] = creDict[selCre][0]\n",
    "            MetaPerCre[rx]['Cell Type'] = creDict[selCre][1]\n",
    "            # read it into memory\n",
    "            pd_array, pd_info = nrrd.read(infile2)\n",
    "            ProjPerCre.append(pd_array)\n",
    "\n",
    "    f2 = h5py.File('ProjPerCre.hdf5','w')\n",
    "    f2.create_dataset('dataset1',data = ProjPerCre)\n",
    "    pk.dump(MetaPerCre,open('MetaPerCre.pkl','wb'))\n",
    "    return MetaPerCre, ProjPerCre\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ReadConnectivityData():\n",
    "    infile = 'expression_files/inj_density.nrrd'\n",
    "    infile2 = 'expression_files/proj_density.nrrd'\n",
    "    mca = MouseConnectivityApi()\n",
    "    mcc = MouseConnectivityCache(resolution = 100)\n",
    "    all_experiments = mcc.get_experiments(dataframe=True)\n",
    "    # get metadata for all non-Cre experiments\n",
    "    experiments = mca.experiment_source_search(injection_structures = 'root', transgenic_lines = 0)\n",
    "    ProjPerExp = []\n",
    "    MetaPerInj = []\n",
    "    # download the projection density volume for one of the experiments\n",
    "    for idx,val in enumerate(experiments):\n",
    "        mca.download_projection_density(infile2, val['id'], resolution = 100)\n",
    "        MetaPerInj.append({})\n",
    "        for key,item in val.iteritems():\n",
    "            MetaPerInj[idx][key] = item\n",
    "        # read it into memory\n",
    "        pd_array, pd_info = nrrd.read(infile2)\n",
    "        ProjPerExp.append(pd_array)\n",
    "   \n",
    "    ProjPerExp = np.asarray(ProjPerExp,dtype = 'float32')\n",
    "    f2 = h5py.File('ProjPerExp.hdf5','w')\n",
    "    f2.create_dataset('dataset1',data = ProjPerExp)\n",
    "    pk.dump(MetaPerInj,open('py_files/MetaPerInj.pkl','wb'))\n",
    "    return MetaPerInj, ProjPerExp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-f5fa01f63b58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;31m# Download the list of structures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m \u001b[0mstructureIds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstructHash\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDownloadStructures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructuresfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-f5fa01f63b58>\u001b[0m in \u001b[0;36mDownloadStructures\u001b[0;34m()\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;31m# Download the mouse brain structures in a structure graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mDownloadStructures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m     \u001b[0mstructs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQueryAPI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSTRUCTURES_URL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;31m# Build a dict from structure id to structure and identify each node's\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-f5fa01f63b58>\u001b[0m in \u001b[0;36mQueryAPI\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0mpagedUrl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpagedUrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python2/lib/python2.7/urllib.pyc\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, proxies, context)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_urlopener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python2/lib/python2.7/urllib.pyc\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data)\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python2/lib/python2.7/urllib.pyc\u001b[0m in \u001b[0;36mopen_http\u001b[0;34m(self, url, data)\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddheaders\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mputheader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0merrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetreply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0merrcode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python2/lib/python2.7/httplib.pyc\u001b[0m in \u001b[0;36mgetreply\u001b[0;34m(self, buffering)\u001b[0m\n\u001b[1;32m   1194\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1196\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m                 \u001b[0;31m#only add this keyword if non-default for compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python2/lib/python2.7/httplib.pyc\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self, buffering)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m             \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwill_close\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0m_UNKNOWN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_CS_IDLE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python2/lib/python2.7/httplib.pyc\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python2/lib/python2.7/httplib.pyc\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;31m# Initialize with Simple-Response defaults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python2/lib/python2.7/socket.pyc\u001b[0m in \u001b[0;36mreadline\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mEINTR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print 'Commencing cre-line mission'\n",
    "CreMeta, ProjPerCre = GetCreLines()\n",
    "print 'Cre-line parsing has been completed'\n",
    "print 'Commencing rAAv mission'\n",
    "WTMeta, ProjPerExp = ReadConnectivityData()\n",
    "print 'rAAv parsing has been completed'\n",
    "\n",
    "structureIds,structHash = DownloadStructures() \n",
    "with open('structures.csv', \"w\") as fp:\n",
    "    M = []\n",
    "    for sid in structureIds:\n",
    "        v = structHash[sid]\n",
    "        M.append([v['id'], v['acronym'], v['name'],\n",
    "                  v['parent_structure_id'], v['color_hex_triplet']])\n",
    "    w = csv.writer(fp, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    w.writerow(['id', 'acronym', 'name',\n",
    "                'parent_structure_id', 'color_hex_triplet'])\n",
    "    for line in M:\n",
    "        w.writerow(line)\n",
    "\n",
    "pk.dump(structureIds,open('structureIds.pkl','wb'))\n",
    "pk.dump(structHash,open('structHash.pkl','wb'))\n",
    "\n",
    "'''InjPerExp = h5py.File('py_files/InjPerExp.hdf5', 'r')['dataset1']\n",
    "ProjPerExp = h5py.File('py_files/ProjPerExp.hdf5', 'r')['dataset1']\n",
    "InjPerCre = h5py.File('py_files/InjPerCre.hdf5', 'r')['dataset1']\n",
    "CreSet = pk.load(open('py_files/MetaPerCre.pkl', 'rb'))\n",
    "NCreSet = pk.load(open('py_files/MetaPerInj.pkl', 'rb'))\n",
    "connectivityL = h5py.File('py_files/conL.hdf5', 'r')['dataset1']\n",
    "connectivityR = h5py.File('py_files/conR.hdf5', 'r')['dataset1']\n",
    "NCreSet2 = pk.load(open('py_files/Cre_clf.pkl', 'rb'))\n",
    "layerity = pk.load(open('py_files/target_layerity.pkl', 'rb'))'''\n",
    "\n",
    "\n",
    "#WTMeta2 = InjectionClassification(CreMeta,WTMeta)\n",
    "#print 'Injection classification is complete'\n",
    "#CreMeta2,WTMeta3 = CreFiltering(CreMeta,WTMeta2,'all')\n",
    "#print 'Layer filtering is complete'\n",
    "unionizes_wt_proj = [QueryAPI(UNIONIZE_CON_FMT % (API_PATH,d['id'])) for d in WTMeta]\n",
    "print 'unionizing of wild-type is complete'\n",
    "\n",
    "print 'structures have successfully been downloaded'\n",
    "connectivityL, connectivityR, connectivityDict = CreateConnectivityMatrix(WTMeta,structureIds,structHash,unionizes_wt_proj)\n",
    "fp1 = h5py.File('conL.hdf5','w')\n",
    "fp2 = h5py.File('conR.hdf5','w')\n",
    "fp1.create_dataset('dataset1',data = connectivityL)\n",
    "fp2.create_dataset('dataset1',data = connectivityR)\n",
    "print 'Connectivity Matrix stage 1 has been created'\n",
    "\n",
    "\n",
    "# Cre-data unionization *************************************************#\n",
    "unionizes_cre_proj = [QueryAPI(UNIONIZE_CON_FMT % (API_PATH,d['id'])) for d in CreMeta]\n",
    "# pk.dump(unionizes_cre_proj, open('unionizes_cre_proj.pkl', 'wb'))\n",
    "print 'unionization is complete'\n",
    "cre_pr_L, cre_pr_R = CreateConnectivityMatrix(CreMeta, structureIds, structHash, unionizes_cre_proj)\n",
    "fp1 = h5py.File('py_files/cre_pr_L.hdf5', 'w')\n",
    "fp2 = h5py.File('py_files/cre_pr_R.hdf5', 'w')\n",
    "fp1.create_dataset('dataset1', data = cre_pr_L)\n",
    "fp2.create_dataset('dataset1', data = cre_pr_R)\n",
    "#************************************************************************#\n",
    "\n",
    "\n",
    "# Expression Data Parsing\n",
    "ExpMeta = QueryAPI(DATA_EXP_SET_QUERY_URL)\n",
    "unionizes_exp = [QueryAPI(UNIONIZE_EXP_FMT % (API_PATH,d['id'])) for d in ExpMeta]\n",
    "gene_expression = CreateExpressionMatrix(ExpMeta,structureIds,structHash,unionizes_exp)\n",
    "fp1 = h5py.File('py_files/G_Exp.hdf5','w')\n",
    "fp1.create_dataset('dataset1',data = gene_expression) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# *********** Step 4: Partition injections based on their cre-line *****#\n",
    "cre15      = ['Syt6-Cre_KI148', 'Ntsr1-Cre_GN220', 'Sim1-Cre_KJ18',\n",
    "            'Efr3a-Cre_NO108', 'Chrna2-Cre_OE25', 'A93-Tg1-Cre',\n",
    "            'Tlx3-Cre_PL56', 'Rbp4-Cre_KL100', 'Rorb-IRES2-Cre',\n",
    "            'Scnn1a-Tg3-Cre', 'Nr5a1-Cre', 'Sepw1-Cre_NP39',\n",
    "            'C57BL/6J', 'Emx1-IRES-Cre', 'Cux2-IRES-Cre']\n",
    "\n",
    "creCategories = [cre['transgenic-line'] for cre in CreMeta]\n",
    "creCategories = list(set(creCategories))\n",
    "\n",
    "Affinity = np.asarray([[0,0,0.001,-5.7125],\n",
    "                       [-0.001,0,0,5.3625],\n",
    "                       [0,-0.001,0,5.1625]])\n",
    "\n",
    "InjCoo  = []\n",
    "InjCoo2 = []\n",
    "for idx,injection in enumerate(WTMeta):\n",
    "  coord = injection['injection-coordinates'];\n",
    "  coord = np.array([coord[0],coord[1],coord[2],1.0]);\n",
    "  InjCoo.append( Affinity.dot(coord) )\n",
    "InjCoo = np.asarray(InjCoo,dtype = 'float32')      # Wild_type coordinates\n",
    "for idx,injection in enumerate(CreMeta):\n",
    "  coord = injection['injection-coordinates'];\n",
    "  coord = np.array([coord[0],coord[1],coord[2],1.0]);\n",
    "  InjCoo2.append( Affinity.dot(coord) )\n",
    "\n",
    "CreLineDict = OrderedDict()\n",
    "for category in cre15:\n",
    "    cre_members = np.asarray([idx for idx, val in enumerate(CreMeta)\\\n",
    "    if val['transgenic-line'] == category])\n",
    "    if len(cre_members) > 0:\n",
    "        CreLineDict[category] = \\\n",
    "        {'ConMat' : cre_pr_R[:, cre_members],\\\n",
    "        'structure-abbrev' : [CreMeta[val]['structure-abbrev'] for val in cre_members],\\\n",
    "        'layer' : [CreMeta[val]['layer'] for val in cre_members],\\\n",
    "        'cell-type' : [CreMeta[val]['Cell Type'] for val in cre_members],\\\n",
    "        'indices' : cre_members,\\\n",
    "        'id'      : [CreMeta[val]['id'] for val in cre_members],\\\n",
    "        'Coordinates' : InjCoo2[cre_members]}\n",
    "     \n",
    "\n",
    "CreLineDict['wild_type'] = {'ConMat' : conR, \\\n",
    "                            'structure-abbrev' : \\\n",
    "                            [val['structure-abbrev'] for val in WTMeta],\\\n",
    "                            'layer'    : ['inspecific' for idx in range(len(WTMeta))],\\\n",
    "                            'cell-type': ['inspecific' for idx in range(len(WTMeta))],\\\n",
    "                            'id'       : [val['id'] for val in WTMeta],\\\n",
    "                            'Coordinates' : InjCoo}\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MesoconnectomePredictor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-b88672f7e7c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mMesoconnectomePredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMesoconnectomePredictor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mGetLayerResolvedArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcre_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Supplementary Table 1.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreFilter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcreFilter\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MesoconnectomePredictor' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ClfResults_ridge = {}; ridge_pred = {}\n",
    "for key in ConDict.keys():\n",
    "    if os.path.isfile('./' + key +'_ridge_pred.pkl') == True and os.path.isfile('./' + key + '_all_results' + '_ridge.pkl') == True:\n",
    "        ridge_pred[key] = pk.load(open('./' + key +'_ridge_pred.pkl','rb'))[2] \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
