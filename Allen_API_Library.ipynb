{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from cfg import *\n",
    "import PrimaryLibrary as PL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "API_PATH = \"http://api.brain-map.org/api/v2/data\"\n",
    "GRAPH_ID = 1\n",
    "PLANE_ID = 1 # coronal\n",
    "MOUSE_PRODUCT_ID = 1 # aba\n",
    "\n",
    "DATA_CON_SET_QUERY_URL = (\"%s/SectionDataSet/query.json\" +\\\n",
    "                          \"?criteria=[failed$eqfalse]\" +\\\n",
    "                          \",products[id$in5]\" +\\\n",
    "                          \",[green_channel$eqrAAV]\" +\\\n",
    "                          #\",specimen(donor[transgenic_mouse_id$eqall donors])\" +\\\n",
    "                          \",specimen(stereotaxic_injections(age[days$ge54],[days$le58]))\" +\\\n",
    "                          \",plane_of_section[id$eq%d]\" +\\\n",
    "                          \"&include=specimen(stereotaxic_injections(age,stereotaxic_injection_materials,stereotaxic_injection_coordinates,primary_injection_structure)),specimen(donor(age))\") \\\n",
    "                          % (API_PATH, PLANE_ID)\n",
    "\n",
    "UNIONIZE_CON_FMT = \"%s/ProjectionStructureUnionize/query.json\" +\\\n",
    "               \"?criteria=[section_data_set_id$eq%d],[is_injection$eqfalse]\" +\\\n",
    "               \"&include=hemisphere\"\n",
    "\n",
    "STRUCTURES_URL = (\"%s/Structure/query.json?\" +\\\n",
    "                      \"criteria=[graph_id$eq%d]\") \\\n",
    "                      % (API_PATH, GRAPH_ID)\n",
    "\n",
    "    \n",
    "DATA_EXP_SET_QUERY_URL = (\"%s/SectionDataSet/query.json\" +\\\n",
    "                          \"?criteria=[failed$eq'false'][expression$eq'true']\" +\\\n",
    "                          \",products[id$eq%d]\" +\\\n",
    "                          \",plane_of_section[id$eq%d],genes\" +\\\n",
    "                          \"&include=genes\") \\\n",
    "                          % (API_PATH, MOUSE_PRODUCT_ID, PLANE_ID)\n",
    "\n",
    "UNIONIZE_EXP_FMT = \"%s/StructureUnionize/query.json\" +\\\n",
    "               \"?criteria=[section_data_set_id$eq%d],structure[graph_id$eq1]\" +\\\n",
    "               (\"&include=section_data_set(products[id$in%d])\" % (MOUSE_PRODUCT_ID)) +\\\n",
    "               \"&only=id,structure_id,sum_pixels,expression_energy,section_data_set_id\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make a query to the API via a URL.\n",
    "def QueryAPI(url):\n",
    "    start_row = 0\n",
    "    num_rows = 2000\n",
    "    total_rows = -1\n",
    "    rows = []\n",
    "    done = False\n",
    "\n",
    "    # The ontology has to be downloaded in pages, since the API will not return\n",
    "    # more than 2000 rows at once.\n",
    "    while not done:\n",
    "        pagedUrl = url + '&start_row=%d&num_rows=%d' % (start_row,num_rows)\n",
    "\n",
    "        print pagedUrl\n",
    "        source = urllib.urlopen(pagedUrl).read()\n",
    "\n",
    "        response = json.loads(source)\n",
    "        rows += response['msg']\n",
    "\n",
    "        if total_rows < 0:\n",
    "            total_rows = int(response['total_rows'])\n",
    "\n",
    "        start_row += len(response['msg'])\n",
    "\n",
    "        if start_row >= total_rows:\n",
    "            done = True\n",
    "\n",
    "    print('Number of results: {}'.format(total_rows))\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def DownloadUnionizedData(dataSets):\n",
    "    unionizes = [QueryAPI(UNIONIZE_FMT % (API_PATH,d['id'])) for d in dataSets]\n",
    "    return unionizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Download the mouse brain structures in a structure graph.\n",
    "def DownloadStructures():\n",
    "    structs = QueryAPI(STRUCTURES_URL)\n",
    "    # Build a dict from structure id to structure and identify each node's\n",
    "    # direct descendants.\n",
    "    structHash = {}\n",
    "    for s in structs:\n",
    "        s['num_children'] = 0\n",
    "        s['structure_id_path'] = [int(sid) for sid in s['structure_id_path'].split('/') if sid != '']\n",
    "        structHash[s['id']] = s\n",
    "\n",
    "    for sid,s in structHash.iteritems():\n",
    "        if len(s['structure_id_path']) > 1:\n",
    "            parentId = s['structure_id_path'][-2]\n",
    "            structHash[parentId]['num_children'] += 1\n",
    "\n",
    "    ## pull out the structure ids for structures in this structure graph that\n",
    "    ## have no children (i.e. just the leaves)\n",
    "    ## corrStructIds = [sid for sid,s in structHash.iteritems() if s['num_children'] == 0]\n",
    "    # RB: no, leave all structures in and filter later\n",
    "    corrStructIds = structHash.keys()\n",
    "\n",
    "    return sorted(corrStructIds), structHash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CreateConnectivityMatrix(dataSets,structureIds,structHash,unionizes):\n",
    "    # Each injection experiment will have a connectivity vector.  This vector will be as long\n",
    "    # as the number of requested structures.\n",
    "    nstructs = len(structureIds)\n",
    "    ndata = len(unionizes)\n",
    "    print('ndata {} ndatasets {}'.format(ndata,len(dataSets)))\n",
    "\n",
    "    sidHash = dict([(id,i) for (i,id) in enumerate(structureIds)])\n",
    "    didHash = dict([(d['id'],i) for (i,d) in enumerate(dataSets)])\n",
    "\n",
    "    connectivityL = np.empty([nstructs,ndata])\n",
    "    connectivityL.fill(np.nan)\n",
    "    connectivityR = np.empty([nstructs,ndata])\n",
    "    connectivityR.fill(np.nan)\n",
    " \n",
    "    connectivityDict = {'projection_density': 0, 'projection_intensity': 0, 'projection_energy': 0, 'projection_volume': 0, 'normalized_projection_volume': 0}\n",
    "    for key in connectivityDict.keys():\n",
    "        connectivityDict[key] = np.empty([nstructs,ndata])\n",
    "        connectivityDict[key].fill(np.nan)\n",
    "\n",
    "    # For each data set's set of unionizes, then for each individual structure,\n",
    "    # fill in the structure's connectivity vector.\n",
    "    for i,us in enumerate(unionizes):\n",
    "        # for each unionize\n",
    "        for j,u in enumerate(us):\n",
    "            sid = u['structure_id']\n",
    "            did = u['section_data_set_id']\n",
    "\n",
    "            struct = structHash[sid]\n",
    "            struct['volume'] = u['sum_pixels']\n",
    "            struct['coordinates'] = u['max_voxel_x'], u['max_voxel_y'], u['max_voxel_z']\n",
    "\n",
    "            if i ==0 and j == 0:\n",
    "              print u\n",
    "\n",
    "            if sidHash.has_key(sid) and didHash.has_key(did):\n",
    "                if u['hemisphere_id'] is 1:\n",
    "                    connectivityL[sidHash[sid]][didHash[did]]  = u['normalized_projection_volume']\n",
    "                elif u['hemisphere_id'] is 2:\n",
    "                    connectivityR[sidHash[sid]][didHash[did]] = u['normalized_projection_volume']\n",
    "                    for key in connectivityDict.keys():\n",
    "                        connectivityDict[key][sidHash[sid]][didHash[did]] = u[key]\n",
    "                elif u['hemisphere_id'] is 3:\n",
    "                  pass\n",
    "                  # this is just the average value of L+R\n",
    "            else:\n",
    "                print \"ERROR: structure {}/injection {} skipped.\".format(sid,did)\n",
    "\n",
    "    return connectivityL, connectivityR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read_v2_Data function**  \n",
    "*Author*: Nestor Timonidis  \n",
    "*Description*:  \n",
    "    This function calls a repository provided by Grange et al in [1]  \n",
    "    and parses the voxelized gridded data of allen brain atlas version 2,\n",
    "    that contain the in situ hybridization gene expression of 4104 genes  \n",
    "*Input*:  \n",
    "VoxelData -> the voxelized gridded gene expression data,  \n",
    "voxel_annots ->  the annotations regarding the id of each voxel,  \n",
    "gene_annots ->   the annotations regarding the name of each gene,  \n",
    "allenGeneIds ->  the annotations regarding the id of each gene,  \n",
    "dims ->  the number of dimensions that constitute the 3D brain grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: the parsing of the voxelized gridded data is being achieved   \n",
    "by the use of two files being stored in the Storage section: 'refAtlas.mat' and 'ExpEnergy.mat'.  \n",
    "This is due to the big amount of time that it takes to automatically download the files from their repository.  \n",
    "For archiving reasons though, the code in the following block demonstrates how the data were being automatically downloaded  \n",
    "and extracted for usage.  \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "urlretrieve('http://www.brainarchitecture.org/wp-content/uploads/2014/12/ABAtlasToolbox.zip','ABAToolbox.zip')   \n",
    "zip_ref = zipfile.ZipFile('ABAToolbox.zip','r' )  \n",
    "first_path = 'home/storage/AllenBrainAtlasToolboxVersion2/AllenBrainAtlasToolboxPrime/atlasData/refAtlas.mat'  \n",
    "second_path = 'home/storage/AllenBrainAtlasToolboxVersion2/AllenBrainAtlasToolboxPrime/atlasData/ExpEnergy.mat'  \n",
    "for zip_info in zip_ref.infolist():  \n",
    "    if cnt == 2:  \n",
    "        break  \n",
    "    if zip_info.filename[-1] == '/':  \n",
    "        continue  \n",
    "    if zip_info.filename ==  first_path or zip_info.filename ==  second_path:  \n",
    "        cnt = cnt + 1  \n",
    "        zip_info.filename = basename(zip_info.filename)  \n",
    "        zip_ref.extract(zip_info,target_path)  \n",
    "zip_ref.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Read_v2_Data(VoxelDict, Ref):   \n",
    "    \n",
    "    #clients = get_hbp_service_client()\n",
    "    #collab_path = get_collab_storage_path()\n",
    "    #clients.storage.download_file(collab_path + '/ExpEnergy.mat','/tmp/ExpEnergy.mat') \n",
    "    #clients.storage.download_file(collab_path + '/refAtlas.mat','/tmp/refAtlas.mat') \n",
    "    cnt = 0\n",
    "    \n",
    "    VoxelData = VoxelDict['D']\n",
    "    voxel_annots = Ref['Ref']['Coronal'][0][0]['Filters'][0][0]['idxArray'][0][0][0][1]\n",
    "    voxel_annots = np.asarray(voxel_annots,dtype = 'int32')\n",
    "    voxel_annots = [val[0] for idx,val in enumerate(voxel_annots)]\n",
    "\n",
    "    gene_filtered_ids = Ref['Ref']['Coronal'][0][0]['Genes'][0][0]['Filters'][0][0]['idxArray'][0][0][0][7]            \n",
    "    gene_filtered_ids = [val[0] for idx,val in enumerate(gene_filtered_ids)]\n",
    "\n",
    "    gene_annots = Ref['Ref']['Coronal'][0][0]['Genes'][0][0]['allenNames'][0][0][0][0]\n",
    "    gene_annots = [val[0][0] for idx,val in enumerate(gene_annots)]\n",
    "    gene_annots = [val for idx,val in enumerate(gene_annots) if (idx+1) in gene_filtered_ids]             \n",
    "    gene_annots = np.asarray(gene_annots,dtype = 'string')\n",
    "   \n",
    "    allenGeneIds = Ref['Ref']['Coronal'][0][0]['Genes'][0][0]['allenGeneIds'][0][0][0][0]\n",
    "    allenGeneIds = [val[0] for idx,val in enumerate(allenGeneIds)]\n",
    "    allenGeneIds = [val for idx,val in enumerate(allenGeneIds) if (idx+1) in gene_filtered_ids]\n",
    "    allenGeneIds = np.asarray(allenGeneIds,dtype = 'string')\n",
    "    \n",
    "    struct_infile = '../Cell_Density_Estimation/py_files/structures.csv'\n",
    "    dims = Ref['Ref']['Coronal'][0][0]['size'][0][0][0]  \n",
    "\n",
    "    return VoxelData,voxel_annots,gene_annots,allenGeneIds,dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CustomUnionization(InputMat, InjMat, InputMeta, resolution = 100):\n",
    "    \n",
    "    print('Custom Unionization script initialization!!')\n",
    "    \n",
    "    annotation = nrrd.read('../25 3 2019/annotation/ccf_2017/annotation_{}.nrrd'.format(resolution))\n",
    "    flat_annot = np.ndarray.flatten(annotation[0])\n",
    "    all_ids = np.unique(flat_annot)\n",
    "    all_ids = np.delete(all_ids,0)\n",
    "    with open('../25 3 2019/structures.csv','rb') as fp:\n",
    "            structure       = PL.readtable(fp) \n",
    "                        \n",
    "    struct_idx_dict = OrderedDict([(val,idx) for idx,val in enumerate(structure['id'])])\n",
    "\n",
    "    inj_len = np.shape(InputMat)[3]\n",
    "    str_len = len(struct_idx_dict.keys())\n",
    "    left_point = np.shape(annotation[0])[2]\n",
    "    middle_point = np.floor(left_point/2)\n",
    "    structural_conn = np.zeros((str_len*2,inj_len))\n",
    "    structural_conn2 = np.zeros((str_len*2,inj_len))\n",
    "    print structural_conn.shape\n",
    "    \n",
    "    for source in range(inj_len):\n",
    "        #injection_vol = len(np.nonzero(InjMat[:,:,:,source])[0])\n",
    "        injection_vol = np.mean(InjMat[:,:,:,source])\n",
    "        for annot,pos in struct_idx_dict.items():\n",
    "            if int(annot) in all_ids:     \n",
    "                print InputMeta[source]['structure-abbrev'], source, structure['acronym'][pos]\n",
    "                annot_mask = np.where(annotation[0] == int(annot))\n",
    "                split_right = annot_mask[2][annot_mask[2] < middle_point]\n",
    "                split_left = annot_mask[2][annot_mask[2] >= middle_point]\n",
    "                ipsi_subset = InputMat[annot_mask[0][annot_mask[2] < middle_point],\\\n",
    "                                       annot_mask[1][annot_mask[2] < middle_point],split_right,source]\n",
    "                contra_subset = InputMat[annot_mask[0][annot_mask[2] >= middle_point],\\\n",
    "                                         annot_mask[1][annot_mask[2] >= middle_point],split_left,source]\n",
    "                if len(split_right) > 0:\n",
    "                    structural_conn2[pos, source] = len(np.nonzero(ipsi_subset)[0])/(1.0*injection_vol)\n",
    "                    structural_conn[pos, source] = np.mean(ipsi_subset)/(1.0*injection_vol)\n",
    "                if len(split_left) > 0:\n",
    "                    structural_conn2[pos+str_len, source] = len(np.nonzero(contra_subset)[0])/(1.0*injection_vol)\n",
    "                    structural_conn[pos+str_len, source] = np.mean(contra_subset)/(1.0*injection_vol)\n",
    "\n",
    "            else:\n",
    "                structural_conn[pos, source] = np.nan\n",
    "                structural_conn[pos+str_len, source] = np.nan\n",
    "                \n",
    "    # save the complete matrix (both left and right inj):\n",
    "    first_quarter = structural_conn[:(structural_conn.shape[0] / 2),:]\n",
    "    second_quarter = structural_conn[(structural_conn.shape[0] / 2):, :]\n",
    "    sc_down = np.concatenate((second_quarter, first_quarter), axis=0)\n",
    "    structural_conn = np.concatenate((structural_conn, sc_down), axis=1)\n",
    "    structural_conn = structural_conn / (np.nanmax(structural_conn))  # normalize the matrix\n",
    "    \n",
    "    first_quarter = structural_conn2[:(structural_conn2.shape[0] / 2),:]\n",
    "    second_quarter = structural_conn2[(structural_conn2.shape[0] / 2):, :]\n",
    "    sc_down = np.concatenate((second_quarter, first_quarter), axis=0)\n",
    "    structural_conn2 = np.concatenate((structural_conn2, sc_down), axis=1)\n",
    "    structural_conn2 = structural_conn2 / (np.nanmax(structural_conn2))  # normalize the matrix\n",
    "    \n",
    "    struct_idx_dict_2 = OrderedDict([(pos,structure['acronym'][pos]) for annot,pos in struct_idx_dict.items()\\\n",
    "                                     if int(annot) in all_ids])\n",
    "    \n",
    "    return structural_conn, structural_conn2, struct_idx_dict_2                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UnionizePerSource(OldMat,Meta, level = 'fine'):\n",
    "\n",
    "    if level == 'full':\n",
    "        unique_sources = np.unique([inj['structure-abbrev'] for inj in Meta])\n",
    "    elif level == 'coarse':\n",
    "        with open('structures.csv','rb') as fp:\n",
    "            structure       = PL.readtable(fp) \n",
    "        tvmb_ids = pk.load(open('ids_for_tvmb.pkl','rb'))\n",
    "        unique_sources = [structure['acronym'][id2] for idx in tvmb_ids \n",
    "                          for id2 in range(len(structure['id'])) if structure['id'][id2] == idx]\n",
    "    \n",
    "     # Gathering clusters of injections to be averaged later on\n",
    "    inj_clusters = OrderedDict()\n",
    "    for idy,source in enumerate(unique_sources):\n",
    "        inj_cls  = [i for i,inj in enumerate(WTMeta) if inj['structure-abbrev'] == source]\n",
    "        if len(inj_cls) > 0: \n",
    "            if source not in inj_clusters: inj_clusters[source] = []\n",
    "            inj_clusters[source].extend(inj_cls) \n",
    "    \n",
    "    # More experiments...\n",
    "    black_sheeps = [idx for idx,val in unique_sources if val not in inj_clusters.keys()]\n",
    "    OldMat = np.delete(OldMat, black_sheeps,axis = 0)\n",
    "    #  *************************#\n",
    "    # For each cluster of injections, average their values and store it in the respective source space\n",
    "    NewMat = np.zeros((len(OldMat),len(unique_sources)))\n",
    "    idx = 0\n",
    "    for source in inj_clusters.keys():\n",
    "        NewMat[:,idx] = np.mean(OldMat[:,inj_clusters[source]], axis = 1)\n",
    "        idx+=1\n",
    "        \n",
    "    # Mirror projections at the source level, assuming that projections \n",
    "    # from the left hemisphere are the same as the right one    \n",
    "    right_target_hemi = NewMat[:(NewMat.shape[0] / 2),:]\n",
    "    left_target_hemi = NewMat[(NewMat.shape[0] / 2):,:]\n",
    "    sc_down = np.concatenate((left_target_hemi, right_target_hemi), axis = 0)\n",
    "    fullNewMat = np.concatenate((NewMat, sc_down), axis = 1)\n",
    "    fullNewMat = fullNewMat / (np.amax(fullNewMat))  # normalize the matrix\n",
    "    \n",
    "    return fullNewMat, inj_clusters    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates an xml file with information related to the Allen CCF v3 annotation volume\n",
    "# to be used by fsl\n",
    "import xml.etree.ElementTree as ET\n",
    "def CustomXmlMaker(annot_path = '../25 3 2019', resolution = 200, outfile = 'saves/allen_annotation_atlas.xml'):\n",
    "    mcc = MouseConnectivityCache(manifest_file='connectivity/mouse_connectivity_manifest.json')\n",
    "    mask_path =  '{}/annotation_{}.nrrd'.format(annot_path,resolution)\n",
    "    annot_200 = nrrd.read(mask_path)\n",
    "    unique_ids = np.unique(annot_200[0])\n",
    "    annot_shape = annot_200[0].shape\n",
    "    structure_tree = mcc.get_structure_tree()\n",
    "    ancestry_trg_dict = structure_tree.ancestors(unique_ids[1:len(unique_ids)]) \n",
    "    primary_list = ['CTX', 'OLF', 'HIP', 'RHP', 'STR', 'PAL', 'TH', 'HY', 'MB', 'P', 'MY', 'CB']\n",
    "\n",
    "    cnt = 0\n",
    "    tree = ET.parse('template_atlas.xml')\n",
    "    root = tree.getroot()\n",
    "    child=ET.Element(\"data\")\n",
    "    for x in range(annot_shape[0]):\n",
    "        for y in range(annot_shape[1]):\n",
    "            for z in range(annot_shape[2]):  \n",
    "                data = root.find('data')\n",
    "                kid = ET.SubElement(data, \"label\")\n",
    "                kid.attrib = {}\n",
    "                if annot_200[0][x,y,z] == 0: \n",
    "                    kid.text = 'background' \n",
    "                else:    \n",
    "                    kid.text = [val[1]['name'] if len(val) > 1 else val[0]['name'] for val in ancestry_trg_dict if val[0]['id'] == annot_200[0][x,y,z]][0]   \n",
    "                    print kid.text\n",
    "                kid.attrib['index'] = str(1) #str(annot_200[0][x,y,z])\n",
    "                kid.attrib['x'] = str(x)\n",
    "                kid.attrib['y'] = str(y)\n",
    "                kid.attrib['z'] = str(z)\n",
    "                #data.append(kid)\n",
    "\n",
    "\n",
    "    tree = ET.ElementTree(root)\n",
    "    #print ET.tostring(root)   \n",
    "    tree.write(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CreateExpressionMatrix(dataSets,structureIds,structHash,unionizes):\n",
    "    # Each structure will have an expression vector.  This vector will be as long\n",
    "    # as the number of requested structures.\n",
    "    nstructs = len(structureIds)\n",
    "    ndata = len(unionizes)\n",
    "\n",
    "    sidHash = dict([(id,i) for (i,id) in enumerate(structureIds)])\n",
    "    didHash = dict([(d['id'],i) for (i,d) in enumerate(dataSets)])\n",
    "\n",
    "    expression = np.empty([nstructs,ndata])\n",
    "    expression.fill(np.nan)\n",
    "\n",
    "    # For each data set's set of unionizes, then for each individual structure,\n",
    "    # fill in the structure's expression vector.\n",
    "    for i,us in enumerate(unionizes):\n",
    "        # for each unionize\n",
    "        for j,u in enumerate(us):\n",
    "            sid = u['structure_id']\n",
    "            did = u['section_data_set_id']\n",
    "\n",
    "            struct = structHash[sid]\n",
    "            struct['volume'] = u['sum_pixels']\n",
    "\n",
    "            if sidHash.has_key(sid) and didHash.has_key(did):\n",
    "                expression[sidHash[sid]][didHash[did]] = u['expression_energy']\n",
    "\n",
    "    return expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def allChildren(acr,acr2parent):\n",
    "   # Description: given a tree hierarchy and an entity,\n",
    "   # this function returns all the children\n",
    "   # of the entity\n",
    "\n",
    "   AC = []\n",
    "   for a,p in acr2parent.items():\n",
    "     if p == acr:\n",
    "       AC.append(a)\n",
    "       AC.extend(allChildren(a,acr2parent))\n",
    "   return AC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ReduceToLeafNodes(structure_acronyms,tree_file):\n",
    "    # Description: this function checks the givenReduceToLeafNodes structures\n",
    "    #              based on the tree hierarchy and returns the\n",
    "    #              leaf nodes\n",
    "\n",
    "    leaf_nodes = []\n",
    "    with open(tree_file) as fp:\n",
    "        acr2parent = json.load(fp)\n",
    "\n",
    "    for idx,acro in enumerate(structure_acronyms):\n",
    "        AC = allChildren(acro, acr2parent)\n",
    "        if len(AC) == 0: # structure is a leaf node\n",
    "           leaf_nodes.append((idx,acro))\n",
    "    return leaf_nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GetConUnionizes():\n",
    "\n",
    "    infile1 = 'expression_files/inj_unionizes.nrrd'\n",
    "    infile2 = 'expression_files/exp_density.nrrd'\n",
    "    mcc = MouseConnectivityCache(resolution = 100)\n",
    "    experiments = mcc.get_experiments(cre = True, dataframe=True)\n",
    "    uni_con = []\n",
    "    for idx,val in enumerate(experiments['id']):\n",
    "        print idx\n",
    "        tmp = mcc.get_experiment_structure_unionizes(experiment_id = val)\n",
    "        uni_con.append(tmp)\n",
    "    print uni_con.shape\n",
    "    fp = h5py.File('unionized_connectivity.hdf5','w')\n",
    "    fp.create_dataset('dataset1',data = uni_con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GetCreLines(infile = None):\n",
    "    infile1 = 'cre_inj_density.nrrd'\n",
    "    infile2 = 'cre_pro_density.nrrd'\n",
    "    mcc = MouseConnectivityCache(resolution = 100, cache = True, manifest_file = 'voxel_model_manifest.json')\n",
    "    mca = MouseConnectivityApi()\n",
    "    cre_experiments = mcc.get_experiments(cre = True, dataframe=True)\n",
    "    \n",
    "    MetaPerCre      = []\n",
    "    InjPerCre       = []\n",
    "    ProjPerCre      = []\n",
    "    creDict         = {}\n",
    "    with open('Supplementary Table 1.csv') as fp:\n",
    "         buff = csv.reader(fp)\n",
    "         for idx,row in enumerate(buff):\n",
    "             if len(row) > 1: # concatenate the two rows together - error caused by csv transition\n",
    "                row[0] =  row[0] + row[1]\n",
    "             remains = [x for x in filter(None,row[0].split(';'))]\n",
    "             if remains[0].isdigit() and remains[1] in infile:\n",
    "             #if len(remains) > 8 and idx > 2 and remains[1].isdigit() == False:\n",
    "                creDict[remains[1]] = []\n",
    "                creDict[remains[1]].append(remains[7])\n",
    "                creDict[remains[1]].append(remains[8])\n",
    "\n",
    "    # download the projection density volume for one of the experiments\n",
    "    for idx,val in enumerate(cre_experiments['id']):\n",
    "         \n",
    "        mcc.get_projection_density(val, infile2)\n",
    "        mcc.get_injection_fraction(val, infile1)\n",
    "        #mcc.get_injection_density(val, infile1)\n",
    "        tmp = cre_experiments['transgenic_line'][val]\n",
    "        if 'A93' in cre_experiments['transgenic_line'][val]:\n",
    "            tmp = 'A93-Tg1-Cre'\n",
    "        selCre = [key for key in creDict.keys() if tmp == key]\n",
    "        #if len(selCre) > 0:\n",
    "        if tmp in creDict.keys():\n",
    "            selCre = selCre[0]\n",
    "            MetaPerCre.append({})\n",
    "            rx = len(MetaPerCre)-1\n",
    "            MetaPerCre[rx]['injection-coordinates'] = \\\n",
    "            [cre_experiments['injection_x'][val],\n",
    "             cre_experiments['injection_y'][val],\n",
    "             cre_experiments['injection_z'][val],\n",
    "            ]\n",
    "            MetaPerCre[rx]['injection_volume'] = \\\n",
    "              cre_experiments['injection_volume'][val]\n",
    "            MetaPerCre[rx]['structure-abbrev'] =\\\n",
    "             cre_experiments['structure_abbrev'][val]\n",
    "            MetaPerCre[rx]['transgenic-line'] = tmp\n",
    "            MetaPerCre[rx]['id'] = cre_experiments['id'][val]\n",
    "            MetaPerCre[rx]['layer'] = creDict[selCre][0]\n",
    "            MetaPerCre[rx]['Cell Type'] = creDict[selCre][1]\n",
    "            # read it into memory\n",
    "            id_array, id_info = nrrd.read(infile1)\n",
    "            InjPerCre.append(id_array)\n",
    "            pd_array, pd_info = nrrd.read(infile2) \n",
    "            ProjPerCre.append(pd_array)\n",
    "            \n",
    "    InjPerCre = np.asarray(InjPerCre); ProjPerCre = np.asarray(ProjPerCre)\n",
    "    f2 = h5py.File('temporary_storage/InjPerCre.hdf5','w')\n",
    "    f2.create_dataset('dataset1',data = InjPerCre)\n",
    "    #f2 = h5py.File('new_saves/ProjPerCre.hdf5','w')\n",
    "    #f2.create_dataset('dataset1',data = ProjPerCre)\n",
    "    #pk.dump(MetaPerCre,open('new_saves/MetaPerCre.pkl','wb')) \n",
    "    return MetaPerCre, ProjPerCre\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ReadConnectivityData():\n",
    "    infile1 = 'expression_files/inj_density.nrrd'\n",
    "    infile2 = 'expression_files/proj_density.nrrd'\n",
    "    #mca = MouseConnectivityApi()\n",
    "    mcc = MouseConnectivityCache(resolution = 100, cache = True, \n",
    "                                 manifest_file = 'voxel_model_manifest.json')\n",
    "    experiments = mcc.get_experiments(cre = False, dataframe=True)\n",
    "    # get metadata for all non-Cre experiments\n",
    "    #experiments = mca.experiment_source_search(injection_structures = 'root', transgenic_lines = 0)\n",
    "    ProjPerExp = []\n",
    "    MetaPerInj = []\n",
    "    InjPerExp  = []\n",
    "    \n",
    "    # download the projection density volume for one of the experiments\n",
    "    for idx,val in enumerate(experiments['id']):\n",
    "        experiment_file = 'experiment_' + str(val) \n",
    "        if experiment_file in os.listdir('./'):\n",
    "            pd_array, pd_info = nrrd.read(experiment_file + '/projection_density_100.nrrd') \n",
    "        else:    \n",
    "            mcc.get_projection_density(val, infile2)\n",
    "            # read it into memory\n",
    "            pd_array, pd_info = nrrd.read(infile2)\n",
    "            mcc.get_injection_fraction(val, infile1)\n",
    "            # read it into memory\n",
    "            id_array, id_info = nrrd.read(infile1)\n",
    "        \n",
    "        MetaPerInj.append({})\n",
    "        for key,item in experiments.iteritems():\n",
    "            MetaPerInj[idx][key] = item.iloc[idx]\n",
    "        ProjPerExp.append(pd_array)\n",
    "        InjPerExp.append(id_array)\n",
    "   \n",
    "    InjPerExp = np.asarray(ProjPerExp,dtype = 'float32')\n",
    "    ProjPerExp = np.asarray(ProjPerExp,dtype = 'float32')\n",
    "    #f2 = h5py.File('ProjPerExp.hdf5','w')\n",
    "    #f2.create_dataset('dataset1',data = ProjPerExp)\n",
    "    f2 = h5py.File('temporary_storage/InjPerExp.hdf5','w')\n",
    "    f2.create_dataset('dataset1',data = InjPerExp)\n",
    "    #pk.dump(MetaPerInj,open('new_saves/MetaPerInj.pkl','wb'))\n",
    "    return MetaPerInj, ProjPerExp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def StructuralDistance(str_cords):\n",
    "    \n",
    "    middle_ground = len(str_cords)/2\n",
    "    DistMat_tmp = np.zeros((np.shape(str_cords)[0],middle_ground),np.float)\n",
    "    for row in range(middle_ground):\n",
    "        for col in range(middle_ground):\n",
    "            DistMat_tmp[row,col] = np.sqrt(np.sum((str_cords[row,:]-str_cords[col,:])**2))\n",
    "            DistMat_tmp[row+middle_ground,col] = np.sqrt(np.sum((str_cords[row+middle_ground,:]-str_cords[col,:])**2))\n",
    "    \n",
    "    right_target_hemi = DistMat_tmp[:middle_ground,:]\n",
    "    left_target_hemi = DistMat_tmp[middle_ground:,:]\n",
    "    DistMat_flip = np.concatenate((left_target_hemi, right_target_hemi), axis = 0)\n",
    "    DistMat = np.concatenate((DistMat_tmp, DistMat_flip), axis = 1)\n",
    "    \n",
    "    return DistMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConnectomeStorage(folder_name,data_name, weights, distances, labels, centres, hemispheres):\n",
    "    \n",
    "    labels = np.array(labels, dtype='|S58')\n",
    "    fp = h5py.File('{}/{}.h5'.format(folder_name,data_name),'w')\n",
    "    fp.create_dataset('hemispheres',data = hemispheres, dtype='|b1')\n",
    "    fp.create_dataset('weights',data = weights, dtype='f8')\n",
    "    fp.create_dataset('tract_lengths',data = distances, dtype='f8')\n",
    "    fp.create_dataset('region_labels',data = labels, dtype='|S58') \n",
    "    fp.create_dataset('centres',data = centres, dtype='|f8') \n",
    "    fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_reference(allen):\n",
    "    # first rotation in order to obtain: x1=x2, y1=z2, z1=y2\n",
    "    vol_trans = np.zeros((allen.shape[0], allen.shape[2], allen.shape[1]), dtype=float)\n",
    "    for x in range(allen.shape[0]):\n",
    "        vol_trans[x, :, :] = (allen[x, :, :][::-1]).transpose()\n",
    "\n",
    "    # second rotation in order to obtain: x1=z2, y1=x1, z1=y2\n",
    "    allen_rotate = np.zeros((allen.shape[2], allen.shape[0], allen.shape[1]), dtype=float)\n",
    "    for y in range(allen.shape[1]):\n",
    "        allen_rotate[:, :, y] = (vol_trans[:, :, y]).transpose()\n",
    "    return allen_rotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_centres(mcc, order, key_ord, NewMat,structure):\n",
    "    #centres = np.zeros((len(key_ord) * 2, 3), dtype=float)\n",
    "    Affinity = np.asarray([[0,0,0.001,-5.7125],\n",
    "                           [-0.001,0,0,5.3625],\n",
    "                           [0,-0.001,0,5.1625]])\n",
    "    names = []\n",
    "    row = -1\n",
    "    str_ids = [structure['id'][idx2] for val in NewMat[1].keys() \n",
    "                 for idx2,val2 in enumerate(structure['acronym']) if val == val2]\n",
    "    centres = np.zeros((len(str_ids) * 2, 3), dtype=float)\n",
    "    #for graph_ord_inj in key_ord:\n",
    "    for idx,node_id in enumerate(str_ids):\n",
    "        #node_id = order[graph_ord_inj][0]\n",
    "        coord = [0, 0, 0]\n",
    "        mask, _ = mcc.get_structure_mask(node_id)\n",
    "        mask = rotate_reference(mask)\n",
    "        mask_r = mask[:mask.shape[0] / 2, :, :]\n",
    "        xyz = np.where(mask_r)\n",
    "        if xyz[0].shape[0] > 0:  # Check if the area is in the annotation volume\n",
    "            coord[0] = np.mean(xyz[0])\n",
    "            coord[1] = np.mean(xyz[1])\n",
    "            coord[2] = np.mean(xyz[2])\n",
    "        row += 1\n",
    "        #centres[row, :] = coord\n",
    "        centres[row, :] = Affinity.dot([coord[0],coord[1],coord[2],1])\n",
    "        coord[0] = (mask.shape[0]) - coord[0]\n",
    "        #centres[row + len(key_ord), :] = coord\n",
    "        centres[row + len(key_ord), :] = Affinity.dot([coord[0],coord[1],coord[2],1])\n",
    "        #n = order[graph_ord_inj][1]\n",
    "        n = NewMat[1].keys()[idx]\n",
    "        right = 'Right '\n",
    "        right += n\n",
    "        right = str(right)\n",
    "        names.append(right)\n",
    "        sel_centre = centres[idx,:]\n",
    "        print sel_centre\n",
    "    for idx,node_id in enumerate(str_ids):\n",
    "        #n = order[graph_ord_inj][1]\n",
    "        n = NewMat[1].keys()[idx]\n",
    "        left = 'Left '\n",
    "        left += n\n",
    "        left = str(left)\n",
    "        names.append(left)\n",
    "       \n",
    "    return centres, names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ParseAtlasData():\n",
    "    \n",
    "    # *********** initial download of tracing experiments and brain structures ***********#\n",
    "    print 'Commencing cre-line mission'\n",
    "    # *********** Partition tracers based on their cre-line *****#\n",
    "    cre15      = ['Syt6-Cre_KI148', 'Ntsr1-Cre_GN220', 'Sim1-Cre_KJ18',\n",
    "                'Efr3a-Cre_NO108', 'Chrna2-Cre_OE25', 'A93-Tg1-Cre',\n",
    "                'Tlx3-Cre_PL56', 'Rbp4-Cre_KL100', 'Rorb-IRES2-Cre',\n",
    "                'Scnn1a-Tg3-Cre', 'Nr5a1-Cre', 'Sepw1-Cre_NP39',\n",
    "                'C57BL/6J', 'Emx1-IRES-Cre', 'Cux2-IRES-Cre']\n",
    "    #CreMeta, ProjPerCre = GetCreLines(infile = cre15)\n",
    "    print 'Cre-line parsing has been completed'\n",
    "    print 'Commencing rAAv mission'\n",
    "    #WTMeta, ProjPerExp = ReadConnectivityData()\n",
    "    WTMeta = pk.load(open('saves/MetaPerInj.pkl','rb'))\n",
    "    print 'rAAv parsing has been completed'\n",
    "    #structureIds,structHash = DownloadStructures() \n",
    "    #pk.dump(structureIds, open('saves/structureIds.pkl','wb'))\n",
    "    #pk.dump(structHash, open('saves/structHash.pkl','wb'))\n",
    "    structureIds      = pk.load(open('saves/structureIds.pkl','rb'))\n",
    "    structHash        = pk.load(open('saves/structHash.pkl','rb'))\n",
    "    with open('structures.csv', \"w\") as fp:\n",
    "        M = []\n",
    "        for sid in structureIds:\n",
    "            v = structHash[sid]\n",
    "            M.append([v['id'], v['acronym'], v['name'],\n",
    "                      v['parent_structure_id'], v['color_hex_triplet']])\n",
    "        w = csv.writer(fp, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        w.writerow(['id', 'acronym', 'name',\n",
    "                    'parent_structure_id', 'color_hex_triplet'])\n",
    "        for line in M:\n",
    "            w.writerow(line)\n",
    "\n",
    "    #pk.dump(structureIds,open('structureIds.pkl','wb'))\n",
    "    #pk.dump(structHash,open('structHash.pkl','wb'))\n",
    "    #*********** wild type-data unionization **************************************#\n",
    "    unionizes_wt_proj = [QueryAPI(UNIONIZE_CON_FMT % (API_PATH,d['id'])) for d in WTMeta]\n",
    "    #mcc = MouseConnectivityCache(resolution = 100, cache = True, manifest_file = 'voxel_model_manifest.json')\n",
    "    #unionizes_wt_proj = mcc.get_structure_unionizes([val['id'] for val in WTMeta], is_injection=False)\n",
    "    #pk.dump(unionizes_wt_proj, open('unionizes_wt_proj.pkl', 'wb'))\n",
    "    #unionizes_wt_proj = pk.load(open('unionizes_wt_proj.pkl', 'rb'))\n",
    "    #np.asarray(unionizes_wt_proj[['max_voxel_x','max_voxel_y','max_voxel_z']])\n",
    "    print 'unionizing of wild-type is complete'\n",
    "  \n",
    "    conL, conR = CreateConnectivityMatrix(WTMeta,\n",
    "                                           structureIds,structHash,\n",
    "                                           unionizes_wt_proj)\n",
    "    fp1 = h5py.File('conL.hdf5', 'w')\n",
    "    fp2 = h5py.File('conR.hdf5', 'w')\n",
    "    fp1.create_dataset('dataset1', data = conL)\n",
    "    fp2.create_dataset('dataset1', data = conR)\n",
    "    \n",
    "    # Cre-data unionization *************************************************#\n",
    "    #unionizes_cre_proj = [QueryAPI(UNIONIZE_CON_FMT % (API_PATH,d['id'])) for d in CreMeta]\n",
    "    #pk.dump(unionizes_cre_proj, open('unionizes_cre_proj.pkl', 'wb'))\n",
    "    print 'unionization is complete'\n",
    "    #cre_pr_L, cre_pr_R = CreateConnectivityMatrix(CreMeta, structureIds, structHash, unionizes_cre_proj)\n",
    "    #fp1 = h5py.File('cre_pr_L.hdf5', 'w')\n",
    "    #fp2 = h5py.File('cre_pr_R.hdf5', 'w')\n",
    "    #fp1.create_dataset('dataset1', data = cre_pr_L)\n",
    "    #fp2.create_dataset('dataset1', data = cre_pr_R)\n",
    "    #************************************************************************#\n",
    "\n",
    "\n",
    "    # Expression Data Parsing\n",
    "    ExpMeta = QueryAPI(DATA_EXP_SET_QUERY_URL)\n",
    "    #pk.dump(ExpMeta,open('GeneMeta.pkl','wb'))\n",
    "    unionizes_exp = [QueryAPI(UNIONIZE_EXP_FMT % (API_PATH,d['id'])) for d in ExpMeta]\n",
    "    gene_expression = CreateExpressionMatrix(ExpMeta,structureIds,structHash,unionizes_exp)\n",
    "    #fp1 = h5py.File('G_Exp.hdf5','w')\n",
    "    #fp1.create_dataset('dataset1',data = gene_expression)\n",
    "    \n",
    "    ''' \n",
    "    conR              = pk.load(open('saves/conR.pkl','rb'))\n",
    "    #conL              = pk.load(open('saves/conL.pkl','rb'))\n",
    "    CreMeta           = pk.load(open('MetaPerCre.pkl','rb'))\n",
    "    cre_pr_L          = h5py.File('cre_pr_L.hdf5', 'r')['dataset1']\n",
    "    cre_pr_R          = h5py.File('cre_pr_R.hdf5', 'r')['dataset1']\n",
    "    #unionizes_wt_proj = pk.load(open('unionizes_wt_proj.pkl', 'rb')) \n",
    "    '''\n",
    "\n",
    "    creCategories = [cre['transgenic-line'] for cre in CreMeta]\n",
    "    creCategories = list(set(creCategories))\n",
    "\n",
    "    Affinity = np.asarray([[0,0,0.001,-5.7125],\n",
    "                           [-0.001,0,0,5.3625],\n",
    "                           [0,-0.001,0,5.1625]])\n",
    "\n",
    "    InjCoo  = []\n",
    "    InjCoo2 = []\n",
    "    for idx,injection in enumerate(WTMeta):\n",
    "      coord = injection['injection-coordinates'];\n",
    "      coord = np.array([coord[0],coord[1],coord[2],1.0]);\n",
    "      InjCoo.append( Affinity.dot(coord) )\n",
    "    InjCoo = np.asarray(InjCoo,dtype = 'float32')      # Wild_type coordinates\n",
    "    for idx,injection in enumerate(CreMeta):\n",
    "      coord = injection['injection-coordinates'];\n",
    "      coord = np.array([coord[0],coord[1],coord[2],1.0]);\n",
    "      InjCoo2.append( Affinity.dot(coord) )\n",
    "    InjCoo2 = np.asarray(InjCoo2); InjCoo = np.asarray(InjCoo) \n",
    "    \n",
    "    CreLineDict = OrderedDict()\n",
    "    for category in cre15:\n",
    "        cre_members = np.asarray([idx for idx, val in enumerate(CreMeta)\\\n",
    "        if val['transgenic-line'] == category])\n",
    "        if len(cre_members) > 0:\n",
    "             \n",
    "            CreLineDict[category] = \\\n",
    "            {'ConMat' : cre_pr_R[:, cre_members],\\\n",
    "            'structure-abbrev' : [CreMeta[val]['structure-abbrev'] \n",
    "                                  for val in cre_members],\\\n",
    "            'layer'       : [CreMeta[val]['layer'] for val in cre_members],\\\n",
    "            'cell-type'   : [CreMeta[val]['Cell Type'] for val in cre_members],\\\n",
    "            'indices'     : cre_members,\\\n",
    "            'id'          : [CreMeta[val]['id'] for val in cre_members],\\\n",
    "            'Coordinates' : InjCoo2[cre_members,:],\\\n",
    "            'injection_volume': [CreMeta[val]['injection_volume']\n",
    "                                 for val in cre_members]}\n",
    "\n",
    "\n",
    "    CreLineDict['wild_type'] = {'ConMat' : conR, \\\n",
    "                                'structure-abbrev' : \\\n",
    "                                [val['structure-abbrev'] for val in WTMeta],\\\n",
    "                                'layer'    : ['inspecific' for idx in range(len(WTMeta))],\\\n",
    "                                'cell-type': ['inspecific' for idx in range(len(WTMeta))],\\\n",
    "                                'id'       : [val['id'] for val in WTMeta],\\\n",
    "                                'Coordinates' : InjCoo,\\\n",
    "                                'injection_volume': [val['injection-volume'] for val in WTMeta]}\n",
    "    \n",
    "    ConDict        = pk.dump(CreLineDict, open('scrambled_debug/CreLineDict.pkl','wb'))\n",
    "    \n",
    "    return CreLineDict"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "CreLineDict = ParseAtlasData()\n",
    "!rm -rf experiment_*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make an xml file with the Allen anatomical template"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Parse the file\n",
    "import xml.etree.ElementTree as ET\n",
    "mcc = MouseConnectivityCache(manifest_file='connectivity/mouse_connectivity_manifest.json')\n",
    "mask_path =  '../25 3 2019/annotation_200.nrrd'\n",
    "annot_200 = nrrd.read(mask_path)\n",
    "unique_ids = np.unique(annot_200[0])\n",
    "annot_shape = annot_200[0].shape\n",
    "structure_tree = mcc.get_structure_tree()\n",
    "ancestry_trg_dict = structure_tree.ancestors(unique_ids[1:len(unique_ids)]) \n",
    "primary_list = ['CTX', 'OLF', 'HIP', 'RHP', 'STR', 'PAL', 'TH', 'HY', 'MB', 'P', 'MY', 'CB']\n",
    "FL.CustomXmlMaker()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimental: Custom Unionization Script"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open('structures.csv','rb') as fp:\n",
    "        structure       = PL.readtable(fp) \n",
    "        struct_idx_dict = {val:idx for idx,val in enumerate(structure['id'])}\n",
    "        str_acronyms    = [val for val in structure['acronym']]\n",
    "hundred_path = '../25 3 2019/annotation_100.nrrd'\n",
    "annotation100 = nrrd.read(hundred_path)   \n",
    "WTMeta = pk.load(open('saves/MetaPerInj.pkl','rb'))\n",
    "WTProj = h5py.File('ProjPerExp.hdf5','r')['dataset1'][()]\n",
    "WTInj  = h5py.File('temporary_storage/InjPerExp.hdf5','r')['dataset1'][()]\n",
    "\n",
    "flat_100 = np.ndarray.flatten(annotation100[0])\n",
    "new_shape = np.shape(WTProj)[1]*np.shape(WTProj)[2]*np.shape(WTProj)[3]\n",
    "WTProj_res = np.zeros((new_shape, 498)); WTInj_res = np.zeros((new_shape, 498))\n",
    "UnionConMat = np.zeros((len(structure['id'])*2, len(WTProj_res[0])))\n",
    "for i in range(498):\n",
    "    WTProj_res[:,i] = np.reshape(WTProj[i,:,:,:], (new_shape))\n",
    "    WTInj_res[:,i] =  np.reshape(WTInj[i,:,:,:], (new_shape))\n",
    "    \n",
    "    \n",
    "UnionConMat = CustomUnionization(WTProj_res, WTInj_res, annotation100[0], WTMeta, level = 'full_inj')\n",
    "#pk.dump(UnionConMat, open('../Connectomic Repository/Custom_Unionization_wt.pkl','wb'))\n",
    "NewMat = UnionizePerSource(UnionConMat, WTMeta, level = 'full')\n",
    "#pk.dump(NewMat[0], open('../Connectomic Repository/Custom_Unionization_wt_source.pkl','wb'))\n",
    "plt.imshow(np.log(NewMat[0]))\n",
    "clear_output()\n",
    "\n",
    "pk.dump(UnionConMat, open('../Connectomic Repository/Custom_Unionization_wt.pkl','wb'))\n",
    "pk.dump(NewMat[0], open('../Connectomic Repository/Custom_Unionization_wt_source.pkl','wb'))\n",
    "pk.dump(NewMat[1], open('../Connectomic Repository/injection_clusters.pkl','wb'))\n",
    "'''UnionConMat = pk.load(open('../Connectomic Repository/Custom_Unionization_wt.pkl','rb'))\n",
    "NewMat = pk.load(open('../Connectomic Repository/Custom_Unionization_wt_source.pkl','rb'))'''\n",
    "\n",
    "sort_srcs = [structure['id'][idx2] for val in NewMat[1].keys() for idx2,val2 in enumerate(structure['acronym']) if val == val2]\n",
    "\n",
    "hits = [idx for val in sort_srcs for idx,val2 in enumerate(MesoPred.ConDict['wild_type']['Matches']) if val == val2] \n",
    "print MesoPred.ConDict['wild_type']['DistMat'][hits,:].shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Step - Pack the connectome in an hdf5 file in a format ready to be used by the TVMB pipeline"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fp = h5py.File('../Connectomic Repository/Custom_Unionization_full_inj.h5py','w')\n",
    "fp4.create_dataset('hemispheres',data = fp['hemispheres'][()][right_idx+left_idx], dtype='|b1')\n",
    "fp4.create_dataset('weights',data = merge_the_tvmb_pred, dtype='f8')\n",
    "fp4.create_dataset('tract_lengths',data = Dist_left_red, dtype='f8')\n",
    "fp4.create_dataset('region_labels',data = fp['region_labels'][()][right_idx+left_idx], dtype='|S58') \n",
    "fp4.create_dataset('centres',data = fp['centres'][()][right_idx+left_idx], dtype='|f8') \n",
    "fp4.close()'''"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Last minute additions for unionizing user input data (29-11-2019)\n",
    "target_coordinates = construct_centres(mcc, [], [], NewMat,structure)\n",
    "clear_output()\n",
    "tmp_mat = StructuralDistance(MesoPred.ConDict['wild_type']['Coordinates'])\n",
    "labels = NewMat[1].keys()\n",
    "labels_right = ['right ' + val for val in labels]\n",
    "labels_left = ['left ' + val for val in labels]\n",
    "labels = labels_right + labels_left\n",
    "hemispheres = [True if 'right' in val else False for val in labels ]\n",
    "ConnectomeStorage('for_TVMB','new_connectome', NewMat[0], tmp_mat, labels, target_coordinates[0],hemispheres)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from nilearn.image import resample_img\n",
    "affine_100  = 0.1*np.eye(4); affine_res[3,3] = 1.0\n",
    "WTInj_200 = np.zeros((annotation200[0].shape[0],annotation200[0].shape[1],annotation200[0].shape[2],WTInj.shape[0]))\n",
    "for source in range(WTInj.shape[0]):\n",
    "    tmp = nib.Nifti1Image(WTInj[source,:,:,:],affine_100)\n",
    "    WTInj_200[:,:,:,source] = resample_img(tmp,affine_res,interpolation = 'nearest').get_data()\n",
    "    \n",
    "WTInj_200_nii = nib.Nifti1Image(WTInj_200,affine_res)\n",
    "nib.save(WTInj_200_nii,'saves/WT_Injections_200.nii.gz')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
